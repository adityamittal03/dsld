
## Deciding on a set of confounders C  

One may have specific confounders in mind for a particular analysis, but
it is often unclear as to which to use, or for that matter, why not use
them all?  This section addresses those issues.

We will address that second question first: Why not use all our
variables, other than Y and S, as confounders?  We will see that if we
have a large number of variables, there is good reason select only a
subset of potential confounders.  

How might we do so?  We will first recommend some graphical and tabular
methods aimed at preliminary exploration toward this end, and then
present a **dsld* function that users may find useful for more
systematic selection of confounders.

### The problem 

The German credit dataset, included in the **fairml** package and thus
with **dsld**, consists of a total of 21 variables:

```{r}
data(german.credit)
names(german.credit)  
```

Excluding Y = credit risk and S = gender, that gives us 19 variables to
choose among for our confounders.  Which ones should we use?

Technically, almost any variable is a confounder.  The impact may quite
minuscule, but through a long chain of relations among many variables,
there will usually be at least some connection, though possibly very
faint.

Well, then, why not use them all?

* It may result in overfitting, resulting in large standard errors.

* It is unwieldy, difficult to interpret.  Many treatments of these
  issues speak of a desire for a "parsimonious" model.

* There is a concern regarding duplication.  It may be that, say, some
  pair of confounders suffices, and adding further confounders does
  nothing to further clarify discrimination effects.

Concerning this last point:  We do not merely have 19
choices for confounders; we must choose from the set of all possible
groups of confounders: singletons, pairs, triplets and so on.  There are
$2^{19}$ possibilities here, about half a million.

Our specific purpose here is to find a reasonable set of  confounders.
In essence, that means find variables C such that C is substantially
correlated with both Y and S.  One way to approach that is that do
separate predictions of Y and S, and see which features turn out to
predict both well.  

### Exploratory example:  engineering wages

One useful tool here is the function **dsldFrequencyByS()**, which aims
to analyze categorical (not numeric) columns by printing out a list of
frequencies. Since education level in the **svcensus* data is
categorical, we can call the function as follows:

```{r}
library(dsld)
data(svcensus)
dsldFrequencyByS(svcensus, "educ", "gender")
```

The similar frequency values between men and women suggests
not using education as a confounder.

On the other hand, we *do* see more interesting results if we look at 
*occupation* instead of *education*:

```{r}
library(dsld)
data(svcensus)
dsldFrequencyByS(svcensus, "occ", "gender")
```

Notice the difference between the proportion of males in occupation 106
[One might consider conducting formal statistical inference in this
comparison, but the relevance to the issue of our ultimate statistical
analysis of Y is unclear.  At any rate, here we are merely doing an
exploratory analysis anyway.]{.column-margin} vs females in that same
occupation--a difference of approximately 11%.  The difference in
frequencies here is much greater than with the 'educ' example, which
suggests using occupation as a confounder.  Since different occupations
correlate with different wages, it is possible that this difference in
occupations that men have compared with what women have could be
affecting the relationship between gender and wage. 

### Exploratory example: Law school admissions data

Suppose we are investigating the relationship between LSAT exams scores
and race, and wish to graphically explore the effects of the C, family
income, on the relationship between Y = LSAT score, and S = race.

We have to look at both the relationship between race and LSAT scores
and the relationship between race and family income. We may also want to
look at the effects of GPA as well. A quick way to view multiple
variables (and their relation to the sensitive variable) at once is to
plot them in a 3D scatter plot:

<!-- Placement pending -->
```{r, eval=FALSE}
data(law.school.admissions)
dsldScatterPlot3D(law.school.admissions, c('lsat','fam_inc','ugpa'), 'race1',
                  pointSize = 4)
```

![](graphTab/lawscatterplot3d2.png){width="90%"}

Looking at the fam_inc axis, the lowest quintile of family income is
[This is a **plotly** interactive graph.  To fully understand this
function, the reader should execute the function outside of Quarto,
trying features such as move, rotate, show annotation etc.]{.column-margin}
mostly populated by Black and Hispanic students, while the upper two
levels are almost entirely made up of white students. Interestingly on
the lsat axis, most of those with a lower score happen to be non-white,
across all income levels. On the other hand, the ugpa axis has a similar
trend to that of the lsat axis, albeit to a much weaker extent.

Once again, this analysis is merely exploratory, but the graph lends
some credence to claims that family income may confound the relationship
between race and LSAT score.  Note however, that these graphs do not do much to 
answer the question of whether, in this case, that relationship is substantial.
The formal analysis we did earlier indicates that it is not.

To further support our claim that race may be affecting LSAT results, we
can look at the density plot of LSAT scores against different races.

```{r, eval=FALSE}
data(law.school.admissions)
dsldDensityByS(law.school.admissions, cName = "lsat", sName = "race1")
```

![](graphTab/lawdensitylsat.png){width="80%"}

Each curve represents the distribution of LSAT scores for each race, with the
peak of each curve representing the most common scores. From this, its easy
to see the possibility of racial bias in the LSAT. But again, this bias may be
confounded.

Recall that the scatter plot also indicated a potential trend between family 
income and race. We can zoom in on this relationship by generating another 
density plot:

```{r, eval=FALSE}
data(law.school.admissions)
dsldDensityByS(law.school.admissions, cName = "fam_inc", sName = "race1")
```

![](graphTab/lawdensityfam.png){width="80%"}

White students have larger peaks at income levels 3, 4, and 5, indicating that
a larger proportion of white students are in the higher income brackets than 
non-white students. Conversely, a larger proportion of black and Hispanic
students occupy the lower income bracket levels.

[If we wanted to investigate other potential confounders, we can call 
**dsldConfounders()** with 'race1' as our sensitive variable.]{.column-margin}

<!-- remove this paragraph? -->
This suggests that family income could be confounding the potential 
relationship between race and LSAT scores.

::: {.callout-note}
### Everything is a Confounder

Everything is a confounder to *some* degree, so it is up to the analyst
to decide when a feature is confounding enough to be considered in a
model.

:::

### Variable selection methodology

Many, many approaches to this *variable selection* or *feature
selection* [A nice overview of such methodology, and implementations in
R and Python, are given in [Parr *et
al*](https://explained.ai/rf-importance).]{.column-margin} problem have
been proposed over the years.  The **qeML** package includes five of
them (see the documentation), and there are myriad others.

One widely-used method is *permutation*, which we will use here.  To
gauge the importance of a variable V in the context of using some ML
algorithm, we do two runs through the data, first with the original dataset,
and then with the V column changed through a random shuffling of the
values in that column.  In that second run, our predictive accuracy
should be compromised, and the importance measure is taken to be the
proportional increase in error rate.  The larger the increase, the
more important we deem the variable.

### The dsldCHunting() function

This function serves as an aid to selecting confounders, using the 
following approach.  Denote our dataset by **d**.

* Two columnwise subsets of **d** are formed, **dNoS** and **doNoY**,
  deleting the respective column.

* We fit a random forests model to **dNoS**, predicting Y, resulting in
  variable importance measure **vY**.

* We fit a random forests model to **dNoY**, predicting S, resulting in
  variable importance measure **vS**.

* We sort each of the two 'v' measures, from largest to smallest.

* For each i, we take the intersection of the top-i 'v' measures in each set.

By taking the intersection, we are focusing on variables that are
correlated with both Y and S, as desired.

Here is an example, using the mortgage data.

```{r}
data(mortgageSE)
dsldCHunting(mortgageSE,'deny','black')
```

The importance measures are printed out (the ranking is what matters),
followed by the "top-i" sets in common.

As usual, there are no magic formulas to use here.  The analyst is given
a choice.  One might use just two confounders, 'p_irat' and 'loan_val';
for a set of three, the software suggests 'p_irat', 'loan_val' and 'hse_inc',
and so on.


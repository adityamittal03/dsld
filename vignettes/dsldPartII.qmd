
# Part II:  Fairness in Machine Learning

## Motivation
<!-- Don't know how accurate this is, please feel free to make edits -->
In modern applications of machine learning as a predictive modeling tool, it 
would be irresponsible to produce a model that biases one sensitive group over 
another. As such, it becomes imperative to find methods of reducing any such 
biases that a model might hold when it comes to modeling and predicting on 
sensitive datasets.

Of course, nothing comes for free: the inherent tradeoff of increasing fairness 
is reduced utility (predictive power over the dataset). Thus, a means of 
balancing this tradeoff between fairness and utility becomes essential in any 
future implementations of machine learning.

The end result of this modified modeling process is a tool that allows users to:
    1. Compare a fair model against one with no fairness implemented; the 
       effective result is a means of uncovering if biases exist within the 
       dataset itself, but also how much fairness is gained for utility lost.
    2. Reduce the impact of any inherent discrimination within the data (of 
       course only to the level that is specified).


## Example
<!-- introduce running example + context... this example should most likely 
come from COMPAS -->
Let's take a look back at the COMPAS dataset from before to best compare the 
range of approaches to measuring and ensuring fairness in machine learning 
model deployment. [more background here...]


## Approaches Deployed in DSLD
<!-- go over all the approaches we use in the fairML & EDFair wrappers -->
<!-- compare their performance against each other across all metrics 
previously described -->
While there exists a plethora of methods for ensuring a fair machine learning 
model, the primary approaches utilized in the **dsld** package rely on fairness 
constraints, each with their own strengths and weaknesses. As such, it will be 
up to users to decide first which *metric* of fairness makes the most sense 
within their domain, and then based on the performances we record below, choose 
an appropriate model that best balances that measurement of fairness with the 
utility they would like. Again, it should be stressed that how you measure 
fairness and utility will determine the approach taken in balancing the two 
with regards to the model itself.

<!-- accuracy of this information may need to be double-checked -->
So, what are fairness constraints? In simple terms, we can treat Machine 
Learning as an optimization problem. We want to find a set of weights that 
somewhow minimizes the amount of error, or cost as it's commonly referred as, 
that the model produces during testing. This cost function that's used to gauge 
how well/poorly the model is performing provides the feedback necessary to 
make adjustments to the weights as needed.

Fairness constraints come into play by enforcing an additional parameter to 
consider with this cost function during model training. Instead of simply 
saying "minimize the RMSE (Root Mean Square Error)" or whatever utility metric 
chosen (MAPE, MSPE, etc.), this class of fair ML models *also* includes a 
fairness metric to calculate cost. In effect, this produces a final set of 
weights that optimizes not for the best performance [as with most base ML 
models], but for the best **balance** of performance between utility and a 
given definition of fairness.

More simply put, an additional fairness parameter is introduced into the 
optimization problem, forcing whichever optimization algorithm is picked to 
weigh a tradeoff between utility **and** fairness when calculating coefficients 
or weights. This has the effect of balancing the model to be both fair and 
useful to users, but there is a nuance underneath this.

As we will explain in more detail further in this book, the definition of fair 
and fairness with respect to ML algorithms will vary from context to context 
and research paper to research paper. While we provide a plethora of different 
metrics that can be used to gauge fairness, there is no **single correct** 
**metric**; each will have their own use-case. It's up to the reader to decide 
which constraint/metric will optimize the model for their needs.

<!-- following part may be too technical, might move to margin -->
<!-- there is also some redundancy, some parts will have to be merged with above -->
The final set of weights produced is not only not always deterministic (random 
initialization w/ gradient descent means global optimal is not always converged 
on), but is also variable across approaches. The reason for this is each 
approach considers a different definition (and therefore metric) of fairness. 
This will be explained in depth later in this section, but it's important to 
note that depending on how **you** define fairness with the context of your 
data, the best approach will vary.


### Explicitly Deweighted Features
<!-- this will be for the Matloff paper -->
description...


### Ridge Penalty
<!-- this will be for the Scutari paper -->
description...


### Nonconvex Optimization with Fairness Constraints
<!-- this will be for the Komiyama paper -->
description...


### Other Constraints (paper three, we'll update title later)
<!-- this will be for the Zafar paper -->
description...


## Measurements of Fairness & Utility
Different algorithms and approaches attempt to reduce bias as indicated through 
different metrics. In the following subsection, our aim is to explain what each 
of these metrics tell users and which algorithms perform best with respect to 
each one.


### Fairness Metrics
<!-- compare all the fairness metrics against one another... -->
reiterate points about measuring fairness...


#### S-Correlation
<!-- correlation between S-feature and output -->
In order to ensure an ML model isn't discriminating, it may be a meaningful 
measure of fairness to see how much a sensitive feature like race, gender, or 
age relates to the predictions the model is making. A way to measure this sort 
of association is called S-correlation. Measuring how strongly S (sensitive 
feature) and Y (response variable) are correlated allows us to understand the 
how much the model has trained to discriminate. An added benefit of 
S-Correlation is a warning that the data itself may be biased if there exists 
a strong correlation between S and Y.

The function dsldTakeALookAround calculates the S-Correlation through a
non-parametric test, known as the Kendall rank correlation. The strength of 
associativity is measured by the pattern of accordance/discordance 
(i.e: agreement/disagreement) of pairs of observations. A benefit of using
Kendall's rank correlation is that it can be used with ordinal or continuous
data. It is also viable with smaller sets of data compared to other methods.
For more information on how Kendall rank correlation is calculated, click
[here](https://arxiv.org/abs/2206.04019).


#### Demographic Parity
Demographic parity is a fairness metric that measures if a classifier model's 
predictions are dependent on a sensitive attribute. Demographic parity 
essentially compares if the same proportion of each sub-group within the 
sensitive feature is classified at equal rates for each of the possible 
outcomes.

For example, let's consider the COMPAS dataset again. If the proportion of 
African-Americans who are predicted to recidivate is different than the 
proportion of Asian-Americans, then we can consider this model to fail to be 
demographically par. Of course, exactness is something to be decided by the 
user--what constitutes a signficant difference will vary on the data, context, 
etc.

<!-- the followng paragraphs' accuracy will need to be double-checked -->
The measure may seem a bit restrictive or aggressive at first, but keep in mind 
its main use-cases. Typically, demographic parity performs well as a measure 
for fairness in contexts surrounding historical biases skewing the dataset 
itself. In order to prevent a self-reinforcing cycle (such as in the case of 
COMPAS predicting African-Americans to recidivate more --> kept in jail longer 
--> have a more difficult time adjusting to society post-jail --> more likely 
to commit crime), we'll try to minimize the bias of the model used to predict 
on that dataset as a compensatory measure, even if it comes at the risk of 
losing even more utility.

That last point is crucial: the restrictiveness of demographic parity as a 
fairness metric will mean a lower-reward tradeoff with utility--more will need 
to be sacrificed in performance to achieve fairness. Of course, if this metric 
is used in the first place, the seriousness of the context the model will be 
deployed in motivates the use of something as restrictive as demographic 
parity.


### Utility Metrics
<!-- compare all the utility metrics against one another... -->
Utility metrics are used to inform the user of how accurate and/or reliable 
a model is with regards to its performance. This (usually quantitative) 
information can be used to further modify/optimize a model for its usecase, or 
to help the user make a choice in comparing different models for selection.

We can choose what utility metric to use based on the problem we are trying to 
solve. [Reminder: classification settings are prediction applications where Y is 
categorical, while regression settings are prediction applications where Y 
is continuous and/or ordinal]{.column-margin}
For classification settings, we may want to determine how well a model can 
classify instances into different classes. For regression settings, we may want 
to instead look at how different our predicted values are to the actual values.

Usually, a utility metric has its intended usecase in one of the above settings, 
not necessarily both.

#### Test Accuracies (Misclassification Error)
<!-- accuracy results on test sets -->
For classification settings, one simple method to measure the utility of a model 
is to measure how well it can predict something via test accuracy. Normally, 
test accuracy can be defined as follows:
$$
Accuracy = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}
$$

However, when using the **dsld** package for classification settings, the 
**TestAcc** attribute refers to the *error rate*, or how *inaccurate* our 
prediction is.
[For example, if our Y was gender (from **svcensus**) and our model 
produced a **TestAcc** 
between 0.6 and 0.7, our prediction was incorrect around 60-70% of the 
time.]{.column-margin} 
Accuracy and error rate are complements of each other, so 
$Error Rate = 1 - Accuracy$.

For a two-class (binary) classification scenario (i.e. negative and positive, as 
in diagnoses), we can make use of more descriptive terms:

* True Positives (TP): Number of instances from the positive class predicted 
correctly (i.e. as positive)

* True Negatives (TN): Number of instances from the negative class predicted 
correctly (i.e. as negative)

* False Positives (FP): Type I Error - Number of instances from negative class 
predicted incorrectly (i.e. as positive)

* False Negatives (FN): Type II Error - Number of instances from positive class 
predicted incorrectly (i.e. as negative)

Thus, we can redefine our accuracy and error rate formulas (for binary 
classification) as follows:
$$
Accuracy = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$
$$
Error Rate = \frac{\text{FP}+\text{FN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$

The advantage of test accuracy as a utility metric is its simplicity and 
intuitiveness compared to other metrics. However, test accuracy can be limited as 
a utility metric when used in scenarios involving imbalanced classes (i.e. 
substantially more instances of one class over another). In this scenario, a high 
accuracy score may be misleading if it only signifies that the majority class had 
a high prediction accuracy, while the minority class had a low accuracy. Depending 
on the scenario, a low minority class prediction accuracy could indicate a poor 
model--test accuracy alone would not inform the user of this potential failure. 
For this reason, test accuracy is less straightforward with scenarios involving 
several classes, where imbalance is more likely.

Another issue lies with decision boundaries/thresholds, which are used to assign 
instances to a particular class based on its value (i.e. if the threshold is 0.5, 
a value of 0.6 is assigned to Y = 1, while 0.4 is assigned to Y = 0)--some **dsld** 
functions allow for the user to enter a threshold of their choosing. In cases where 
instances fall close to this threshold (i.e. 0.5001), these instances could be 
misclassified, thus affecting the Test Accuracy--more on this in the below section 
on ROC curves.

If our problem is a regression setting as opposed to a classification setting, it 
may fare better to use a different metric (see the below section on MSPE & MAPE).

<!-- Commenting out ROC curves until we can find away to either reduce it to a margin 
note or remove it entirely -->
<!--
#### ROC Curves
[![ROC Curve](partII/rocCurve.png)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic){width=50%}
An ROC curve (receiver operating characteristic curve) is a graph that plots
the model's behavior at various classification thresholds. The graph aids in 
visually identifying optimal thresholds by avoiding confusion matrices.

The graph plots the true positive rate (sensitivity) on the y-axis and the 
false positive rate (selectivity) on the x axis. For each threshold, its true 
positive rate and false positive rate are calculated.

A model that perfectly classifies each sample is plotted at (0, 1). A point at
(1,1), (.75, .75), (.5, .5) etc. means that the proportion of correctly 
classified samples is equal to the the proportion of incorrectly classified
samples. A point at (1, 0) incorrectly classifies each sample.

Say, we are trying to classify a group of people as infected or not infected
to prevent an outbreak. So, initially we set our threshold to be "Lets consider
everyone to be infected." Then our threshold correctly identified a person who 
is infected as infected. But it also incorrectly calculated all non-infected 
peopl as infected. So, this threshold is plotted at (1, 1). 

If we change our threshold to "a person with a high body temperature is 
considered infected" then our point will move on the graph. This may correctly 
identify most people who are infected, but it may also misidentify a few people
who are not infected as infected, and vice-versa. So we can arbitrarily say 
that this threshold is plotted at (.1, .8).

We can keep changing our threshold level and we will get different points on
the graph. It is up to the user to determine what true positive rate he wants 
for the model and make a risk assessment on who is free to socialize and who
is required to quarantine. The ROC curve is a diagnostic tool to help the user
choose which threshold he wants to use.
-->

#### MSPE & MAPE
<!-- Mean Squared Prediction Error & Mean Absolute Prediction Error -->
The Mean Squared Prediction Error (MSPE) and Mean Absolute Prediction Error 
(MAPE) are both metrics used for quantifying the accuracy of a predictive 
model. The two parallel each other in many ways as they are both an average of 
the difference in predicted value and the actual value, except one (as the 
names suggest) squares the difference and the other takes its absolute value.

The Mean Squared Prediction Error can be defined as:
$MSPE(L) = \frac{1}{n} * \sum{[(L(X_{i}) - y(X_{i}))^{2}]}$, 
where $L$ is a predictive model and $y(X_{i})$ is the actual value at the 
specified point in space $X_{i}$.

The Mean Absolute Prediction Error can be defined as:
$MSPE(L) = \frac{1}{n} * \sum{|L(X_{i}) - y(X_{i})|}$, 
where $L$ is a predictive model and $y(X_{i})$ is the actual value at the 
specified point in space $X_{i}$.

They both measure roughly the same thing (deviation of the prediction from the 
actual value), but their behavior varies depending on the type of data being 
dealt with. For one, the squared error terms means the MSPE is more sensitive 
to outliers. In other words, outliers may artificially inflate the MSPE, 
especially for linear models that tend to over-generalize behavior as a simple 
trend line. MAPE, therefore, performs a lot more robustly when gauging general 
performance, but fails in another sense. 
<!-- This following part is what my research has indicated, and it makes sense, 
but maybe it shouldn't be included --> Mathematically, the absolute value function 
isn't differentiable for all x (it's undefined at the origin). Therefore, given its 
differentiability, the MSPE is preferred for optimization problems in general as 
extrema can be found without iterative methods.

## Testing

We are (purely hypothetically) investigating racial bias in the COMPAS data with 
regards to recidivism. We are trying to predict two_year_recid (which is a 
classification setting), with race as our sensitive variable. Since this is a 
classification setting, the only utility metric we can use (out of the above listings) 
for this scenario is Test Accuracy (misclassification error). For both fairness metrics 
(S Correlation and Demographic Parity), we will try to find the best balance between 
fairness and utility, using the approaches implemented in various models from this 
package.

### Testing: S Correlation

Suppose that out_custody and c_jail_out are the proxies to our sensitive variable 
(race).[Based on results from **dsldCHunting**]{.column-margin}

We will start by looking at the Random Forests model using **dsldQeFairRF**, which 
can output S Correlation and Misclassification Error without extra calculation from 
the user. This function uses the *explicitly deweighted features* strategy, so by 
adjusting the deweight values for proxies, we should be able to adjust fairness and 
utility.

We can use the **replicMeans** (included in **qeML**) to run several trials of our model 
to increase the reliability of our results:

```{r}
data(compas)
numTrials = 20
deweightPars01 = list(out_custody=0.1, c_jail_out=0.1)
deweightPars1 = list(out_custody=1, c_jail_out=1)
```
```{r, eval=FALSE}
# S Correlation
sCorrTest <- function(deweightList) {
    model <- dsldQeFairRF(compas, 'two_year_recid', deweightPars = deweightList,
                        sName = 'race')
    
    c(model$corrsens, model$testAcc)
}

sRes <- regtools::replicMeans(numTrials,"{c(sCorrTest(deweightPars01),
                                    sCorrTest(deweightPars1))}")
# logic for print statement
if (sRes[1] > sRes[8]) {
    sFairnessDir <- "increased"
} else {
    sFairnessDir <- "decreased"
}
if (sRes[7] > sRes[14]) {
    sUtilityDir <- "increased"
} else {
    sUtilityDir <- "decreased"
}

print(sRes[1:6])
print(paste("Deweight0.1 TestAcc:",sRes[7]))
print(sRes[8:13])
print(paste("Deweight1 TestAcc:",sRes[14]))
# summary
print(paste("Based on S Correlation, as Deweight increased, fairness", 
            sFairnessDir, "and utility", sUtilityDir))
```

This example demonstrates the tradeoff between utility and fairness: as we increase the 
deweight values, our utility also increases (since the misclassification error decreases).
On the other hand, S Correlation increases, which indicates that fairness is *decreasing*
since we are trying to increase fairness by reducing the impact of S proxies.

### Testing: Demographic Parity

<!-- Results inconclusive -->
```{r, eval=FALSE}
# Demographic Parity
demoTest <- function(deweightList, threshold = .5) {
    data <- compas
    train <- data[1:4500,]
    test <- data[4501:nrow(data),]
    model <- dsld::dsldQeFairRF(data = train, yName = 'two_year_recid', 
                                sName = 'race', deweightPars = deweightList)
    
    # model's probability from 0 to 1 that the person recidivated
    prediction <- predict(model,test)$probs
    
    # if that probability is above threshold, we call it a yes
    yes <- prediction[,1] > threshold
    
    # output of those who recidivated and their race
    out <- cbind(yes, test['race'])
    
    # create separate vectors for each group in 'race'
    AA <- out[out['race'] == 'African-American',]
    C <- out[out['race'] == 'Caucasian',]
    AS <- out[out['race'] == 'Asian',]
    H <- out[out['race'] == 'Hispanic',]
    NAT <- out[out['race'] == 'Native American',]
    O <- out[out['race'] == 'Other',]
    
    # for each race, return the proportion of which the model classified as will
    # recidivate, along with model's test accuracy
    c(mean(AA[,1]), mean(C[,1]), mean(AS[,1]), mean(H[,1]), mean(NAT[,1]), 
      mean(O[,1]), model$testAcc)
}

# calculate the mean of 20 trials of no deweighting and deweighting 
# out_custody and c_jail_out at 10%
demoRes <- regtools::replicMeans(numTrials,"{c(demoTest(deweightPars01),
                                 demoTest(deweightPars1))}")

# determine sample variance in recividism proportions across races
var01 <- var(demoRes[1:6]) # deweight = 0.1
var1 <- var(demoRes[8:13]) # deweight = 1

# logic for print statement
if (var01 > var1) {
    dFairnessDir <- "increased"
} else {
    dFairnessDir <- "decreased"
}
if (demoRes[7] > demoRes[14]) {
    dUtilityDir <- "increased"
} else {
    dUtilityDir <- "decreased"
}

# print the full vector for reference
print(demoRes)

# print variance and test accuracies: fairness and utility scores
print(paste("Deweight0.1 Var:", var01, "TestAcc:", demoRes[7]))
print(paste("Deweight1 Var:", var1, "TestAcc:", demoRes[14]))

# summary
print(paste("Based on Demo Parity, as Deweight increased, fairness", 
            dFairnessDir, "and utility", dUtilityDir))
```

# Part II:  Fairness in Machine Learning

## Motivation & Background
<!-- Don't know how accurate this is, please feel free to make edits -->
In modern applications of machine learning as a predictive modeling tool, it 
would be irresponsible to produce a model that biases one sensitive group over 
another. As such, it becomes imperative to find methods of reducing any such 
biases that a model might hold when it comes to modeling and predicting on 
sensitive datasets.

Of course, nothing comes for free: the inherent tradeoff of increasing fairness 
is reduced utility (predictive power over the dataset). Thus, a means of 
balancing this tradeoff between fairness and utility becomes essential in any 
future implementations of machine learning.

::: {.callout-note}
#### Fairness-Utility Tradeoff
There exists a tradeoff between the predictive power a model possesses and the 
fairness of its predictions.

This "law" of sorts is becomes intuitive when considering what it means to be 
"fair". In order to ensure decisions aren't being made with respect to a 
sensitive variable, we must reduce our consideration of or neglect to consider 
entirely the sensitive feautre. In doing so, we lose the information it 
provides, but we boost fairness.

Due to this tradeoff, considering what balance of fairness-utility best suites 
the context with which the model operates in becomes essential. You may hear 
phrases along the lines of "business necessity clause", which simply implies 
that as fair a model may be, for practical application a model must be accurate 
to at least a minimum threshold. Otherwise, it isn't viable for deployment.
:::

The end result of this modified modeling process is a tool that allows users to:

1. Compare a fair model against one with no fairness implemented; the 
   effective result is a means of uncovering if biases exist within the 
   dataset itself, but also how much fairness is gained for utility lost.
2. Reduce the impact of any inherent discrimination within the data (of 
   course, only to the level that is specified).

There are two main parts to fair machine learning:

1. **Measuring fairness**, a deeper question that it appears at first glance, 
as we will later discuss
2. **Reducing unfairness**, a section dealing more with balancing fairness with 
utility for practical uses in the real world; the approaches we'll be discussing 
for reducing unfairness will typically be on the algorithm-side

In the latter section, we will take a look back at the **COMPAS** dataset to 
understand how well some of the approaches reduce unfairness in real world examples.


## Approaches Deployed in DSLD
While there exists a plethora of methods for ensuring a fair machine learning 
model, the primary approaches utilized in the **dsld** package rely on fairness 
constraints; each metric has its strengths and weaknesses. As such, it will be 
up to users to decide first which *metric* of fairness makes the most sense 
within their domain, and then based on the performances we record below, choose 
an appropriate model that best balances that measurement of fairness with the 
utility they would like. Again, it should be stressed that how you measure 
fairness and utility will determine the approach taken in balancing the two 
with regards to the model itself.

So, what are fairness constraints? In simple terms, we can treat Machine 
Learning as an optimization problem. We want to find a set of weights that 
somewhow minimizes the amount of error, or cost as it's commonly referred to as, 
that the model produces during testing. This cost function that's used to gauge 
how well/poorly the model is performing provides the feedback necessary to 
make adjustments to the weights as needed.

Fairness constraints come into play by enforcing an additional parameter to 
consider with this cost function during model training. Instead of simply 
saying "minimize the RMSE (Root Mean Square Error)" or whatever utility metric 
chosen (MAPE, MSPE, etc.), this class of fair ML models *also* includes a 
fairness metric to calculate cost. In effect, this produces a final set of 
weights that optimizes not for the best performance [as with most base ML 
models], but for the best **balance** of performance between utility and a 
given definition of fairness.

More simply put, an additional fairness parameter is introduced into the 
optimization problem, forcing whichever optimization algorithm is picked to 
weigh a tradeoff between utility **and** fairness when calculating coefficients 
or weights. This has the effect of balancing the model to be both fair and 
useful to users, but there is a nuance underneath this.

As we will explain in more detail further in this book, the definition of fair 
and fairness with respect to ML algorithms will vary from context to context 
and research paper to research paper. While we provide a plethora of different 
metrics that can be used to gauge fairness, there is no **single correct** 
**metric**; each will have their own use-case. It's up to the reader to decide 
which constraint/metric will optimize the model for their needs.


### Explicitly Deweighted Features
<!-- TODO: this will be for the Matloff paper -->


### Ridge Penalty
<!-- TODO: this will be for the Scutari paper -->


### Nonconvex Optimization with Fairness Constraints
<!-- TODO: this will be for the Komiyama paper -->


### Flexible Fairness Classification
As introduced by Zafar et al. in 2019, this approach to fair machine learning 
defines a framework that **(1)** accomodates multiple definitions of fairness 
simultaneously, **(2)** reduces fairness with respect to a variable amount of 
sensitive features, and **(3)** is applicable to multiple ML algorithms.

The authors of this approach specifically focused on the following key fairness 
metrics in development:

1. **Disparate Treatment**: if a change in a sensitive feature's value results 
in a substantial change in the predicted outcome. For example, if changing the 
gender results in someone being predicted as a violent crime risk then the 
algorithm would be guilty of disparate treatment.
2. **Disparate Impact**: if the overall result with respect to different 
sensitive sub-groups is different. We also reference this metric as *demographic 
parity*. For example, if Race was the sensitive feature and the white subgroup 
would be given preferential treatment with respect to credit scores (i.e. higher 
scores) than other subgroups, then the algorithm would be guilty of disparate 
impact (in violation of demographic parity).
3. **Disparate Mistreatment**: if the error rate in predictions is substantially 
different with respect to different sensitive sub-groups. For example, if a 
model predicts with higher accuracy if asian applicants will be accepted for a 
loan or not, then we can conclude the model is guilty of disparate mistreatment.

The contraint employed by Zafar et al. in their approach relies on binding the 
covariance of the sensitive features to the predicted outcomes. In other words, 
they have designed a model that constrains its behavior based on if the 
predictions it makes is mimicking the behavior of the sensitive feature. By 
doing so, they can effectively balance the amount of information the sensitive 
attribute provides to the model by limiting the impact it *can* have on the 
model's predictions.

It's worth noting here that this approach has also enabled Zafar et al. to 
optimize with respect to a minimum accuracy threshold, essentially allowing for 
maximal fairness while meeting the minimum benchmark requirements. This is 
especially useful for implementations of fair machine learning in the context 
of business.

::: {.callout-note}
While we are yet to apply this approach in the dsld package, users are 
encouraged to explore the fairML package if they deem this framework best 
suites their needs.

These are implemented under the functions `zlm`, `zlm.orig`, `zlrm`, and 
`zlrm.orig`. This is for both linear & logistic regression, although the paper 
defines approaches for SVMs as well.
:::


## Measurements of Fairness & Utility
Different algorithms and approaches attempt to reduce bias as indicated through 
different metrics. In the following subsection, our aim is to explain what each 
of these metrics tell users and which algorithms perform best with respect to 
each one.

::: {.callout-note}
### True-False Positives and Negatives {#classification-terms}
Over the following sections, you may see terms like True-Positive rate 
(TPR), False-Negative rate (FPR), etc. These terms will primarily come in 
the context of binary classifiers as a means of measuring performance or even 
fairness. The definitions are as follows:

* **True** means that the actual result and the predicted result are the same
* **False** means that the actual result is the opposite of the predicted result
* **Positive** refers to the act of a positive prediction, a `1`; this is 
   simply assigning one of two outcomes to the positive class on an arbitrary 
   basis
* **Negative** refers to the act of a negative prediction, a `0`
* **True Positives (TP)**: Number of instances from the positive class predicted 
   correctly (i.e. as positive)
* **True Negatives (TN)**: Number of instances from the negative class predicted 
   correctly (i.e. as negative)
* **False Positives (FP)**: Type I Error - Number of instances from negative class 
   predicted incorrectly (i.e. as positive)
* **False Negatives (FN)**: Type II Error - Number of instances from positive class 
   predicted incorrectly (i.e. as negative)

The rate appended to the end means a ratio with regards to all positives or 
negatives.

Aliases for the common rates (defined as the ratio of TP (true positives) over 
P (all positives), FP over N (all negatives), etc.) are as follows:

* **TPR** is also *sensitivity*, *recall*, or *hit rate*
* **FPR** is also *fall-out* or *false-alarm ratio*
* **TNR** is also *specificity*
* **FNR** is also *miss rate*
:::


### Fairness Metrics
Fairness metrics are used to inform the user of the potential disparity in the 
consistency of a model's predictions across different groups (e.g. demographic 
groups). These groups are usually related to the sensitive variable S. Recall 
from the introduction to this book that to minimize fairness, we must reduce 
the impact of race from our predictions (as well as reduce the impact of any proxy 
variables).

Fairness metrics can detect potential biases in model predictions based on the 
aforementioned sensitive variable(s). Many fairness metrics can be related back to 
the aforementioned True-False Positives and Negatives, so it is important for the 
reader to understand those concepts to understand how these different metrics work


#### S-Correlation
<!-- correlation between S-feature and output -->
In order to ensure an ML model isn't discriminating, it may be a meaningful 
measure of fairness to see how much a sensitive feature like race, gender, or 
age relates to the predictions the model is making. A way to measure this sort 
of association is called S-correlation. Measuring how strongly S (sensitive 
feature) and Y (response variable) are correlated allows us to understand the 
how much the model has trained to discriminate. An added benefit of 
S-Correlation is a warning that the data itself may be biased if there exists 
a strong correlation between S and Y.

The function dsldTakeALookAround calculates the S-Correlation through a
non-parametric test, known as the Kendall rank correlation. The strength of 
associativity is measured by the pattern of accordance/discordance 
(i.e: agreement/disagreement) of pairs of observations. A benefit of using
Kendall's rank correlation is that it can be used with ordinal or continuous
data. It is also viable with smaller sets of data compared to other methods.
For more information on how Kendall rank correlation is calculated, click
[here](https://arxiv.org/abs/2206.04019).


#### Demographic Parity
<!-- TODO: citation -->
Demographic parity is a fairness metric that measures if a classifier model's 
predictions are dependent on a sensitive attribute. Demographic parity 
essentially compares if the same proportion of each sub-group within the 
sensitive feature is classified at equal rates for each of the possible 
outcomes. [[Based on this source](https://arxiv.org/pdf/2205.13619.pdf)]{.column-margin}
Perfect demographic parity requires that every sensitive sub-group 
has an equal chance of being classified as Positive.

For example, let's consider the COMPAS dataset again. If the proportion of 
African-Americans who are predicted to recidivate is different than the 
proportion of Asian-Americans, then we can consider this model to fail to be 
demographically par. Of course, exactness is something to be decided by the 
user--what constitutes a signficant difference will vary on the data, context, 
etc.

<!-- the followng paragraphs' accuracy will need to be double-checked -->
The measure may seem a bit restrictive or aggressive at first, but keep in mind 
its main use-cases. Typically, demographic parity performs well as a measure 
for fairness in contexts surrounding historical biases skewing the dataset 
itself. In order to prevent a self-reinforcing cycle (such as in the case of 
COMPAS predicting African-Americans to recidivate more --> kept in jail longer 
--> have a more difficult time adjusting to society post-jail --> more likely 
to commit crime), we'll try to minimize the bias of the model used to predict 
on that dataset as a compensatory measure, even if it comes at the risk of 
losing even more utility.

That last point is crucial: the restrictiveness of demographic parity as a 
fairness metric will mean a lower-reward tradeoff with utility--more will need 
to be sacrificed in performance to achieve fairness. Of course, if this metric 
is used in the first place, the seriousness of the context the model will be 
deployed in motivates the use of something as restrictive as demographic 
parity.

<!--
#### Equal Opportunity

TODO, but ONLY IF we figure out the testing for this 
-->

#### Equalized Odds

Equalized Odds requires that the different senstive sub-groups have equal 
*sensitivity* and *fall-out* (aka **TPR** and **FPR**). The strictness of this 
requirement (even compared to demographic parity) can make this metric more 
difficult to implement and achieve,[[Based on this source](https://ocw.mit.edu/courses/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/pages/module-three-framework/fairness-criteria/)]{.column-margin}
but if Equalized Odds is achieved, this is a very strong indicator 
of high fairness. For example, if a model that predicted COMPAS *two_year_recid* 
was perfectly fair with regard to race, then the chance of an African-American 
recidivating would be the same as a non African-American *not* recidivating.


### Utility Metrics

Utility metrics are used to inform the user of how accurate and/or reliable 
a model is with regards to its performance. This (usually quantitative) 
information can be used to further modify/optimize a model for its usecase, or 
to help the user make a choice in comparing different models for selection.

We can choose what utility metric to use based on the problem we are trying to 
solve. [Reminder: classification settings are prediction applications where Y is 
categorical, while regression settings are prediction applications where Y 
is continuous and/or ordinal]{.column-margin}
For classification settings, we may want to determine how well a model can 
classify instances into different classes. For regression settings, we may want 
to instead look at how different our predicted values are to the actual values.

Usually, a utility metric has its intended usecase in one of the above settings, 
not necessarily both.

#### Test Accuracies (Misclassification Error)

For classification settings, one simple method to measure the utility of a model 
is to measure how well it can predict something via test accuracy. Normally, 
test accuracy can be defined as follows:

$$
Accuracy = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}
$$

However, when using the **dsld** package for classification settings, the 
**TestAcc** attribute refers to the *error rate*, or how *inaccurate* our 
prediction is.
[For example, if our Y was gender (from **svcensus**) and our model 
produced a **TestAcc** 
between 0.6 and 0.7, our prediction was incorrect around 60-70% of the 
time.]{.column-margin} 
Accuracy and error rate are complements of each other, so 
$Error Rate = 1 - Accuracy$.

For a two-class (binary) classification scenario (i.e. negative and positive, as 
in diagnoses), we can make use of True-False Positives and Negatives to redefine 
our accuracy and error rate formulas as follows:
[See @classification-terms for definitions]{.column-margin}

$$
Accuracy = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$
$$
Error Rate = \frac{\text{FP}+\text{FN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$

The advantage of test accuracy as a utility metric is its simplicity and 
intuitiveness compared to other metrics. However, test accuracy can be limited as 
a utility metric when used in scenarios involving imbalanced classes (i.e. 
substantially more instances of one class over another). In this scenario, a high 
accuracy score may be misleading if it only signifies that the majority class had 
a high prediction accuracy, while the minority class had a low accuracy. Depending 
on the scenario, a low minority class prediction accuracy could indicate a poor 
model--test accuracy alone would not inform the user of this potential failure. 
For this reason, test accuracy is less straightforward with scenarios involving 
several classes, where imbalance is more likely.

Another issue lies with decision boundaries/thresholds, which are used to assign 
instances to a particular class based on its value (i.e. if the threshold is 0.5, 
a value of 0.6 is assigned to Y = 1, while 0.4 is assigned to Y = 0)--some **dsld** 
functions allow for the user to enter a threshold of their choosing. In cases where 
instances fall close to this threshold (i.e. 0.5001), these instances could be 
misclassified, thus affecting the Test Accuracy--more on this in the below section 
on ROC curves.

If our problem is a regression setting as opposed to a classification setting, it 
may fare better to use a different metric (see the below section on MSPE & MAPE).

<!-- Commenting out ROC curves until we can find away to either reduce it to a margin 
note or remove it entirely -->
<!--
#### ROC Curves
[![ROC Curve](partII/rocCurve.png)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic){width=50%}
An ROC curve (receiver operating characteristic curve) is a graph that plots
the model's behavior at various classification thresholds. The graph aids in 
visually identifying optimal thresholds by avoiding confusion matrices.

The graph plots the true positive rate (sensitivity) on the y-axis and the 
false positive rate (selectivity) on the x axis. For each threshold, its true 
positive rate and false positive rate are calculated.

A model that perfectly classifies each sample is plotted at (0, 1). A point at
(1,1), (.75, .75), (.5, .5) etc. means that the proportion of correctly 
classified samples is equal to the the proportion of incorrectly classified
samples. A point at (1, 0) incorrectly classifies each sample.

Say, we are trying to classify a group of people as infected or not infected
to prevent an outbreak. So, initially we set our threshold to be "Lets consider
everyone to be infected." Then our threshold correctly identified a person who 
is infected as infected. But it also incorrectly calculated all non-infected 
peopl as infected. So, this threshold is plotted at (1, 1). 

If we change our threshold to "a person with a high body temperature is 
considered infected" then our point will move on the graph. This may correctly 
identify most people who are infected, but it may also misidentify a few people
who are not infected as infected, and vice-versa. So we can arbitrarily say 
that this threshold is plotted at (.1, .8).

We can keep changing our threshold level and we will get different points on
the graph. It is up to the user to determine what true positive rate he wants 
for the model and make a risk assessment on who is free to socialize and who
is required to quarantine. The ROC curve is a diagnostic tool to help the user
choose which threshold he wants to use.
-->


#### F1-Score

One utility metric trending in recent times, especially popular with the ML 
research world, is F1-Score. The F1-Score is defined as the harmonic mean of 
precision and recall. In more simple terms, the F1-Score essentially balances 
the performance of a classifier model with respect to precision--a ratio of how 
many of the positive predictions are correct $\frac{TP}{TP + FP}$--and recall--
aka TPR (true positive rate), aka the ratio of correctly predicted positive 
outcomes, given by $\frac{TP}{TP + FN}$.

The full formula for F1-Score is given therefore by 
$F_1 = \frac{2}{\text{precision}^{-1} + \text{recall}^{-1}}$

To unpack this, let's first consider harmonic mean. Harmonic mean is simply 
another form of averaging a set of numbers. While people are most familiar with 
the arithmetic mean, $\frac{\sum_{i=0}^{n}x_i}{n}$, the harmonic mean offers 
some behaviors that make it more attractive in machine learning.

::: {.callout-note}
##### Harmonic Means
For anyone curious, the formula for harmonic mean is 
$\frac{n}{\sum_{i=0}^{n}\frac{1}{x_i}}$. It essentially takes reciprocal of the 
arithmetic mean of the reciprocals of the set. This is not relevant, however, 
to understanding F1 and its uses, so don't worry too much about the formula if 
you don't understand it.
:::

1. It's weighting on the smaller of the two (precision or recall) effectively 
stops researchers from adjusting hyperparameters to boost performance of either 
precision or recall without accounting for the other.

2. Mathematically, the F1-Score will always be less than the arithmetic 
mean except for when precision and recall are requivalent

3. The inherent tradeoff between precision and recall also means models will 
be forced to make an intelligent decisions. Take, for example, a dataset 
with binary outcomes where one of the possible outcomes far outnumbers the 
other. A model that blindly predicts this outcome would perform relatively 
well under test accuracy, but against F1-Score this model would (rightly so) 
be rejected.

F1-Score essentially restrains models from biasing one side of this precision-
recall tradeoff, ensuring a more performant model in the face of deployment.


#### MSPE & MAPE
<!-- Mean Squared Prediction Error & Mean Absolute Prediction Error -->
The Mean Squared Prediction Error (MSPE) and Mean Absolute Prediction Error 
(MAPE) are both metrics used for quantifying the accuracy of a predictive 
model. The two parallel each other in many ways as they are both an average of 
the difference in predicted value and the actual value, except one (as the 
names suggest) squares the difference and the other takes its absolute value.

The Mean Squared Prediction Error can be defined as:
$MSPE(L) = \frac{1}{n} * \sum{[(L(X_{i}) - y(X_{i}))^{2}]}$, 
where $L$ is a predictive model and $y(X_{i})$ is the actual value at the 
specified point in space $X_{i}$.

The Mean Absolute Prediction Error can be defined as:
$MSPE(L) = \frac{1}{n} * \sum{|L(X_{i}) - y(X_{i})|}$, 
where $L$ is a predictive model and $y(X_{i})$ is the actual value at the 
specified point in space $X_{i}$.

They both measure roughly the same thing (deviation of the prediction from the 
actual value), but their behavior varies depending on the type of data being 
dealt with. For one, the squared error terms means the MSPE is more sensitive 
to outliers. In other words, outliers may artificially inflate the MSPE, 
especially for linear models that tend to over-generalize behavior as a simple 
trend line. MAPE, therefore, performs a lot more robustly when gauging general 
performance, but fails in another sense. 
<!-- This following part is what my research has indicated, and it makes sense, 
but maybe it shouldn't be included --> Mathematically, the absolute value function 
isn't differentiable for all x (it's undefined at the origin). Therefore, given its 
differentiability, the MSPE is preferred for optimization problems in general as 
extrema can be found without iterative methods.

## Testing
<!-- TODO: Rewrite analyses to account for randomness
Might need to switch eval to FALSE and paste in results instead
May need to switch Utility metrics if possible -->

We are (purely hypothetically) investigating racial bias in the COMPAS data with 
regards to recidivism. We are trying to predict two_year_recid (which is a 
classification setting), with race as our sensitive variable. Since this is a 
classification setting, the only utility metric we can use (out of the above listings) 
for this scenario is Test Accuracy (misclassification error). For each fairness metric, 
we will try to find the best balance between fairness and utility using an approach 
implemented in the models from this package.

We will consider decile_score and priors_count to be the proxies to our sensitive 
variable (race).[Based on results from **dsldConfounders**]{.column-margin}

We will look at the Random Forests model using **dsldQeFairRF**. This 
function uses the *explicitly deweighted features* strategy, so by 
adjusting the deweight values for proxies, we should be able to adjust fairness and 
utility.

That approach used in this model to minimize fairness is *Explicitly Deweighted Features*, 
so we will present the two extreme cases (deweighting and not deweighting) and determine 
the fairness and utility for each based on the selected metrics. The trends in fairness 
vs utility can help inform the user's decision of where the best balance is (at the end of 
the day, there is no definitive balance between fairness and utility).

### Testing: S Correlation

**DSLD** has a **dsldFairTest** function that allows us to invoke a test function
on different models. For **dsldQeFairRF**, we can simply call the *corrsens* attribute
to get the S Correlations of the model with and without the deweighted features.

```{r, eval = FALSE}
# set up base variables
data(compas)
data <- compas
yName <- 'two_year_recid'
sName <- 'race'

############## S Correlation ##############
# The dsldQeFairRF model has a corrsens attribute that contains the 
# s correlations of all the levels
scorr <- function (model) model$corrsens

# Deweights = 1
dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, scorr, nReps = 10)
# Results
                          Metric Misclass Error
race.African-American 0.29496767      0.2166259
race.Asian            0.15994065            NaN
race.Caucasian        0.19855852            NaN
race.Hispanic         0.29190186            NaN
race.Native American  0.02544242            NaN
race.Other            0.21154683            NaN

##############################

# Deweights = 0
dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, scorr, nReps = 10,
                   deweightPars = list(decile_score=0, priors_count=0))
# Results
                          Metric Misclass Error
race.African-American 0.19991603      0.2288509
race.Asian            0.12780363            NaN
race.Caucasian        0.13936898            NaN
race.Hispanic         0.19217361            NaN
race.Native American  0.00970379            NaN
race.Other            0.10438146            NaN
```

This example demonstrates the tradeoff between utility and fairness: as the 
deweight values transition from 1 to 0, our utility also **decreases** (since the 
misclassification error increases). On the other hand, S Correlation decreases, 
which indicates that fairness is **increasing** since we are trying to increase 
fairness by reducing the impact of S proxies.

::: {.callout-note}
##### Randomness in Misclassification Error
One issue that we will encounter later is the potential unreliability of our 
Utility metric due to high variation in the Misclassification Error results. 
Recall that for each trial run, we produce a slightly different fairness and 
utility score, even with the exact same arguments. We try to alleviate the 
impact of this error by increasing the number of repititions/trial runs, but 
the randomness of Misclassification Error in particular cannot be fully 
eliminated. This may result in an indication of unexpected trends 
that may not be entirely accurate. It also means that we cannot fully rely on 
misclassification error alone to define utility, given the randomness.
:::

### Testing: Demographic Parity

The **dsldFairTest** function really shines with the **fairness** package, which
has several functions for several fairness metrics, including demographic
parity. Its important to note that the demographic parity we described earlier is 
labeled *prop_parity* under the fairness package.

```{r, eval = FALSE}
# Deweights = 1
dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::prop_parity, 
             nReps = 10, deweightPars = list(decile_score=1, priors_count=1))
# Results
                 Proportion Proportional Parity Group size Misclass Error
African-American  0.5091107           1.0000000      886.8      0.2124694
Asian             0.2150000           0.4274612        9.1            NaN
Caucasian         0.3479712           0.6841210      613.1            NaN
Hispanic          0.3167917           0.6238221      151.1            NaN
Native American   0.3250000           0.6484604        3.6            NaN
Other             0.3188334           0.6274617       92.3            NaN

##############################

# Deweights = 0
dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::prop_parity, 
             nReps = 10, deweightPars = list(decile_score=0, priors_count=0))
# Results
                 Proportion Proportional Parity Group size Misclass Error
African-American  0.5008996           1.0000000      880.4      0.2222494
Asian             0.3391667           0.6777102        6.6            NaN
Caucasian         0.3641984           0.7271936      612.9            NaN
Hispanic          0.3324559           0.6631030      156.5            NaN
Native American   0.2108333           0.4259763        4.3            NaN
Other             0.2908529           0.5809273       95.3            NaN
```

We see a similar set of results to our S Correlation testing: in the transition from 
deweight values = 1 to deweight values = 0, we see that the utility is **decreasing** 
since the misclassification error is decreasing. For fairness, the closer our results 
are to 1, the level of achievement of demographic parity increases (i.e. more fair).
For our demographic parity results, the distance from 1 is smaller at the deweight 
value of 0, so our fairness is **increasing** according to demographic parity.

<!-- TODO: explain why AA demo par is 1 i.e. relation to AA -->

::: {.callout-note}
##### Randomness in Misclassification Error
You might have noticed that the results for Native-American and Asian groups seem 
to contradict the overall trends we are analyzing. This could be due to the small 
sample sizes in this dataset for Native-American and Asian sub-groups (14 and 28 
respectively, compared to 2941 for African-American). Due to the small sample size, 
we will be ignoring the results for these sub-groups in our analyses.
:::

### Testing: Equalized Odds

Similar to our approach for measuring demographic parity, we will make use of the 
*equal_odds* label with the **fairness** package to determine the equalized odds.

```{r, eval = FALSE}
# Deweights = 1
dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::equal_odds, 
             nReps = 10, deweightPars = list(decile_score=1, priors_count=1))
# Results
                 Sensitivity Equalized odds Group size Misclass Error
African-American   0.7752200      1.0000000      879.7      0.2266504
Asian              0.9166667      1.1818755        8.2            NaN
Caucasian          0.6887573      0.8888523      613.4            NaN
Hispanic           0.6663525      0.8606362      156.2            NaN
Native American    0.9000000      1.1604017        4.5            NaN
Other              0.6970377      0.9007743       94.0            NaN

##############################

# Deweights = 0
dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::equal_odds, 
             nReps = 10, deweightPars = list(decile_score=0, priors_count=0))
# Results
                 Sensitivity Equalized odds Group size Misclass Error
African-American   0.7387521      1.0000000      877.4      0.2117359
Asian              0.8250000      1.1125278        8.5            NaN
Caucasian          0.6998496      0.9478001      614.7            NaN
Hispanic           0.6632300      0.8986169      155.1            NaN
Native American    0.3900000      0.5357268        4.3            NaN
Other              0.6966142      0.9421963       96.0            NaN
```

Similar to demographic parity, the closer our equal odds results are to 1, the more
fair our model is. We see a similar trend as before, where the fairness is 
**increased** at a deweight value of 0. However, our utility *also* seems to be 
**increasing** based on the decreasing misclassification error, which is unexpected 
and likely due to random error.

<!-- TODO: Conclusion showing the effectiveness of deweighting
Maybe summarizing the expected fairness/utility at deweight 1 vs deweight 0
-->



<!-- TODO: If ALL of the above todo's have been addressed, we can expand the testing/conceptual 
sections with any of the following options, in a rough top-down order of priority:
- More fairness metrics
- More models (besides qeFairRF)*
- * ^ This will likely require a new COMPAS example with a continuous Y name as opposed to
      the binary two_year_recid
- More utility metrics
-->
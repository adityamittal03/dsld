
# Part II:  Fairness in Machine Learning

## Motivation
<!-- Don't know how accurate this is, please feel free to make edits -->
In modern applications of machine learning as a predictive modeling tool, it 
would be irresponsible to produce a model that biases one sensitive group over 
another. As such, it becomes imperative to find methods of reducing any such 
biases that a model might hold when it comes to modeling and predicting on 
sensitive datasets.

Of course, nothing comes for free: the inherent tradeoff of increasing fairness 
is reduced utility (predictive power over the dataset). Thus, a means of 
balancing this tradeoff between fairness and utility becomes essential in any 
future implementations of machine learning.

The end result of this modified modeling process is a tool that allows users to:
    1. Compare a fair model against one with no fairness implemented; the 
       effective result is a means of uncovering if biases exist within the 
       dataset itself, but also how much fairness is gained for utility lost.
    2. Reduce the impact of any inherent discrimination within the data (of 
       course only to the level that is specified).


## Example
<!-- introduce running example + context... this example should most likely 
come from COMPAS -->
Let's take a look back at the COMPAS dataset from before to best compare the 
range of approaches to measuring and ensuring fairness in machine learning 
model deployment. [more background here...]


## Approaches Deployed in DSLD
While there exists a plethora of methods for ensuring a fair machine learning 
model, the primary approaches utilized in the **dsld** package rely on fairness 
constraints; each metric has its strengths and weaknesses. As such, it will be 
up to users to decide first which *metric* of fairness makes the most sense 
within their domain, and then based on the performances we record below, choose 
an appropriate model that best balances that measurement of fairness with the 
utility they would like. Again, it should be stressed that how you measure 
fairness and utility will determine the approach taken in balancing the two 
with regards to the model itself.

So, what are fairness constraints? In simple terms, we can treat Machine 
Learning as an optimization problem. We want to find a set of weights that 
somewhow minimizes the amount of error, or cost as it's commonly referred to as, 
that the model produces during testing. This cost function that's used to gauge 
how well/poorly the model is performing provides the feedback necessary to 
make adjustments to the weights as needed.

Fairness constraints come into play by enforcing an additional parameter to 
consider with this cost function during model training. Instead of simply 
saying "minimize the RMSE (Root Mean Square Error)" or whatever utility metric 
chosen (MAPE, MSPE, etc.), this class of fair ML models *also* includes a 
fairness metric to calculate cost. In effect, this produces a final set of 
weights that optimizes not for the best performance [as with most base ML 
models], but for the best **balance** of performance between utility and a 
given definition of fairness.

More simply put, an additional fairness parameter is introduced into the 
optimization problem, forcing whichever optimization algorithm is picked to 
weigh a tradeoff between utility **and** fairness when calculating coefficients 
or weights. This has the effect of balancing the model to be both fair and 
useful to users, but there is a nuance underneath this.

As we will explain in more detail further in this book, the definition of fair 
and fairness with respect to ML algorithms will vary from context to context 
and research paper to research paper. While we provide a plethora of different 
metrics that can be used to gauge fairness, there is no **single correct** 
**metric**; each will have their own use-case. It's up to the reader to decide 
which constraint/metric will optimize the model for their needs.

::: {.callout-note}
#### Side-Note: Determinism
The following paragraph reads a bit more technical, though it is *not* 
necessary for users to understand in order to deploy a fair ML model.

The final set of weights produced by some of these algorithims is not always 
deterministic. In other words, repeated attempts with the same model and 
parameters may lead to varying degrees of success depending on the type of 
optimization being used. Iterative optimization methods (such as gradient 
descent) will be the main culprits with this non-determinism. Some closed-form 
algorithms (i.e. Ridge Regression) will not be affected by this.
:::


### Explicitly Deweighted Features
<!-- TODO: this will be for the Matloff paper -->


### Ridge Penalty
<!-- TODO: this will be for the Scutari paper -->


### Nonconvex Optimization with Fairness Constraints
<!-- TODO: this will be for the Komiyama paper -->


### Other Constraints (paper three, we'll update title later)
<!-- TODO: this will be for the Zafar paper -->


## Measurements of Fairness & Utility
Different algorithms and approaches attempt to reduce bias as indicated through 
different metrics. In the following subsection, our aim is to explain what each 
of these metrics tell users and which algorithms perform best with respect to 
each one.

::: {.callout-note}
### True-False Positives and Negatives {#classification-terms}
Over the following sections, you may see the terms like True-Positive rate 
(TPR) or False-Negative rate (FPR), etc. These terms will primarily come in 
the context of binary classifiers as a means of measuring performance or even 
fairness. The definitions are as follows:

* **True** means that the actual result and the predicted result are the same
* **False** means that the actual result is the opposite of the predicted result
* **Positive** refers to the act of a positive prediction, a `1`; this is 
   simply assigning one of two outcomes to the positive class on an arbitrary 
   basis
* **Negative** refers to the act of a negative prediction, a `0`
* **True Positives (TP)**: Number of instances from the positive class predicted 
   correctly (i.e. as positive)
* **True Negatives (TN)**: Number of instances from the negative class predicted 
   correctly (i.e. as negative)
* **False Positives (FP)**: Type I Error - Number of instances from negative class 
   predicted incorrectly (i.e. as positive)
* **False Negatives (FN)**: Type II Error - Number of instances from positive class 
   predicted incorrectly (i.e. as negative)

The rate appended to the end means a ratio with regards to all positives or 
negatives.

Aliases for the common rates (defined as the ratio of TP (true positives) over 
P (all positives), FP over N (all negatives), etc.) are as follows:

* **TPR** is also *sensitivity*, *recall*, or *hit rate*
* **FPR** is also *fall-out* or *false-alarm ratio*
* **TNR** is also *specificity*
* **FNR** is also *miss rate*
:::


### Fairness Metrics
<!-- compare all the fairness metrics against one another... -->
<!-- TODO: reiterate points about measuring fairness... -->


#### S-Correlation
<!-- correlation between S-feature and output -->
In order to ensure an ML model isn't discriminating, it may be a meaningful 
measure of fairness to see how much a sensitive feature like race, gender, or 
age relates to the predictions the model is making. A way to measure this sort 
of association is called S-correlation. Measuring how strongly S (sensitive 
feature) and Y (response variable) are correlated allows us to understand the 
how much the model has trained to discriminate. An added benefit of 
S-Correlation is a warning that the data itself may be biased if there exists 
a strong correlation between S and Y.

The function dsldTakeALookAround calculates the S-Correlation through a
non-parametric test, known as the Kendall rank correlation. The strength of 
associativity is measured by the pattern of accordance/discordance 
(i.e: agreement/disagreement) of pairs of observations. A benefit of using
Kendall's rank correlation is that it can be used with ordinal or continuous
data. It is also viable with smaller sets of data compared to other methods.
For more information on how Kendall rank correlation is calculated, click
[here](https://arxiv.org/abs/2206.04019).


#### Demographic Parity
Demographic parity is a fairness metric that measures if a classifier model's 
predictions are dependent on a sensitive attribute. Demographic parity 
essentially compares if the same proportion of each sub-group within the 
sensitive feature is classified at equal rates for each of the possible 
outcomes.

For example, let's consider the COMPAS dataset again. If the proportion of 
African-Americans who are predicted to recidivate is different than the 
proportion of Asian-Americans, then we can consider this model to fail to be 
demographically par. Of course, exactness is something to be decided by the 
user--what constitutes a signficant difference will vary on the data, context, 
etc.

<!-- the followng paragraphs' accuracy will need to be double-checked -->
The measure may seem a bit restrictive or aggressive at first, but keep in mind 
its main use-cases. Typically, demographic parity performs well as a measure 
for fairness in contexts surrounding historical biases skewing the dataset 
itself. In order to prevent a self-reinforcing cycle (such as in the case of 
COMPAS predicting African-Americans to recidivate more --> kept in jail longer 
--> have a more difficult time adjusting to society post-jail --> more likely 
to commit crime), we'll try to minimize the bias of the model used to predict 
on that dataset as a compensatory measure, even if it comes at the risk of 
losing even more utility.

That last point is crucial: the restrictiveness of demographic parity as a 
fairness metric will mean a lower-reward tradeoff with utility--more will need 
to be sacrificed in performance to achieve fairness. Of course, if this metric 
is used in the first place, the seriousness of the context the model will be 
deployed in motivates the use of something as restrictive as demographic 
parity.

<!--
#### Equal Opportunity

TODO, but ONLY IF we figure out the testing for this 
-->

#### Equalized Odds
<!-- TODO: equalized odds definition, meaning, intuition here -->


### Utility Metrics

Utility metrics are used to inform the user of how accurate and/or reliable 
a model is with regards to its performance. This (usually quantitative) 
information can be used to further modify/optimize a model for its usecase, or 
to help the user make a choice in comparing different models for selection.

We can choose what utility metric to use based on the problem we are trying to 
solve. [Reminder: classification settings are prediction applications where Y is 
categorical, while regression settings are prediction applications where Y 
is continuous and/or ordinal]{.column-margin}
For classification settings, we may want to determine how well a model can 
classify instances into different classes. For regression settings, we may want 
to instead look at how different our predicted values are to the actual values.

Usually, a utility metric has its intended usecase in one of the above settings, 
not necessarily both.

#### Test Accuracies (Misclassification Error)

For classification settings, one simple method to measure the utility of a model 
is to measure how well it can predict something via test accuracy. Normally, 
test accuracy can be defined as follows:

$$
Accuracy = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}
$$

However, when using the **dsld** package for classification settings, the 
**TestAcc** attribute refers to the *error rate*, or how *inaccurate* our 
prediction is.
[For example, if our Y was gender (from **svcensus**) and our model 
produced a **TestAcc** 
between 0.6 and 0.7, our prediction was incorrect around 60-70% of the 
time.]{.column-margin} 
Accuracy and error rate are complements of each other, so 
$Error Rate = 1 - Accuracy$.

For a two-class (binary) classification scenario (i.e. negative and positive, as 
in diagnoses), we can make use of True-False Positives and Negatives to redefine 
our accuracy and error rate formulas as follows:
[See @classification-terms for definitions]{.column-margin}

$$
Accuracy = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$
$$
Error Rate = \frac{\text{FP}+\text{FN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$

The advantage of test accuracy as a utility metric is its simplicity and 
intuitiveness compared to other metrics. However, test accuracy can be limited as 
a utility metric when used in scenarios involving imbalanced classes (i.e. 
substantially more instances of one class over another). In this scenario, a high 
accuracy score may be misleading if it only signifies that the majority class had 
a high prediction accuracy, while the minority class had a low accuracy. Depending 
on the scenario, a low minority class prediction accuracy could indicate a poor 
model--test accuracy alone would not inform the user of this potential failure. 
For this reason, test accuracy is less straightforward with scenarios involving 
several classes, where imbalance is more likely.

Another issue lies with decision boundaries/thresholds, which are used to assign 
instances to a particular class based on its value (i.e. if the threshold is 0.5, 
a value of 0.6 is assigned to Y = 1, while 0.4 is assigned to Y = 0)--some **dsld** 
functions allow for the user to enter a threshold of their choosing. In cases where 
instances fall close to this threshold (i.e. 0.5001), these instances could be 
misclassified, thus affecting the Test Accuracy--more on this in the below section 
on ROC curves.

If our problem is a regression setting as opposed to a classification setting, it 
may fare better to use a different metric (see the below section on MSPE & MAPE).

<!-- Commenting out ROC curves until we can find away to either reduce it to a margin 
note or remove it entirely -->
<!--
#### ROC Curves
[![ROC Curve](partII/rocCurve.png)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic){width=50%}
An ROC curve (receiver operating characteristic curve) is a graph that plots
the model's behavior at various classification thresholds. The graph aids in 
visually identifying optimal thresholds by avoiding confusion matrices.

The graph plots the true positive rate (sensitivity) on the y-axis and the 
false positive rate (selectivity) on the x axis. For each threshold, its true 
positive rate and false positive rate are calculated.

A model that perfectly classifies each sample is plotted at (0, 1). A point at
(1,1), (.75, .75), (.5, .5) etc. means that the proportion of correctly 
classified samples is equal to the the proportion of incorrectly classified
samples. A point at (1, 0) incorrectly classifies each sample.

Say, we are trying to classify a group of people as infected or not infected
to prevent an outbreak. So, initially we set our threshold to be "Lets consider
everyone to be infected." Then our threshold correctly identified a person who 
is infected as infected. But it also incorrectly calculated all non-infected 
peopl as infected. So, this threshold is plotted at (1, 1). 

If we change our threshold to "a person with a high body temperature is 
considered infected" then our point will move on the graph. This may correctly 
identify most people who are infected, but it may also misidentify a few people
who are not infected as infected, and vice-versa. So we can arbitrarily say 
that this threshold is plotted at (.1, .8).

We can keep changing our threshold level and we will get different points on
the graph. It is up to the user to determine what true positive rate he wants 
for the model and make a risk assessment on who is free to socialize and who
is required to quarantine. The ROC curve is a diagnostic tool to help the user
choose which threshold he wants to use.
-->


#### F1-Score

One utility metric trending in recent times, especially popular with the ML 
research world, is F1-Score. The F1-Score is defined as the harmonic mean of 
precision and recall. In more simple terms, the F1-Score essentially balances 
the performance of a classifier model with respect to precision--a ratio of how 
many of the positive predictions are correct $\frac{TP}{TP + FP}$--and recall--
aka TPR (true positive rate), aka the ratio of correctly predicted positive 
outcomes, given by $\frac{TP}{TP + FN}$.

The full formula for F1-Score is given therefore by 
$F_1 = \frac{2}{\text{precision}^{-1} + \text{recall}^{-1}}$

To unpack this, let's first consider harmonic mean. Harmonic mean is simply 
another form of averaging a set of numbers. While people are most familiar with 
the arithmetic mean, $\frac{\sum_{i=0}^{n}x_i}{n}$, the harmonic mean offers 
some behaviors that make it more attractive in machine learning.

::: {.callout-note}
##### Harmonic Means
For anyone curious, the formula for harmonic mean is 
$\frac{n}{\sum_{i=0}^{n}\frac{1}{x_i}}$. It essentially takes reciprocal of the 
arithmetic mean of the reciprocals of the set. This is not relevant, however, 
to understanding F1 and its uses, so don't worry too much about the formula if 
you don't understand it.
:::

1. It's weighting on the smaller of the two (precision or recall) effectively 
stops researchers from adjusting hyperparameters to boost performance of either 
precision or recall without accounting for the other.

2. Mathematically, the F1-Score will always be less than the arithmetic 
mean except for when precision and recall are requivalent

3. The inherent tradeoff between precision and recall also means models will 
be forced to make an intelligent decisions. Take, for example, a dataset 
with binary outcomes where one of the possible outcomes far outnumbers the 
other. A model that blindly predicts this outcome would perform relatively 
well under test accuracy, but against F1-Score this model would (rightly so) 
be rejected.

F1-Score essentially restrains models from biasing one side of this precision-
recall tradeoff, ensuring a more performant model in the face of deployment.


#### MSPE & MAPE
<!-- Mean Squared Prediction Error & Mean Absolute Prediction Error -->
The Mean Squared Prediction Error (MSPE) and Mean Absolute Prediction Error 
(MAPE) are both metrics used for quantifying the accuracy of a predictive 
model. The two parallel each other in many ways as they are both an average of 
the difference in predicted value and the actual value, except one (as the 
names suggest) squares the difference and the other takes its absolute value.

The Mean Squared Prediction Error can be defined as:
$MSPE(L) = \frac{1}{n} * \sum{[(L(X_{i}) - y(X_{i}))^{2}]}$, 
where $L$ is a predictive model and $y(X_{i})$ is the actual value at the 
specified point in space $X_{i}$.

The Mean Absolute Prediction Error can be defined as:
$MSPE(L) = \frac{1}{n} * \sum{|L(X_{i}) - y(X_{i})|}$, 
where $L$ is a predictive model and $y(X_{i})$ is the actual value at the 
specified point in space $X_{i}$.

They both measure roughly the same thing (deviation of the prediction from the 
actual value), but their behavior varies depending on the type of data being 
dealt with. For one, the squared error terms means the MSPE is more sensitive 
to outliers. In other words, outliers may artificially inflate the MSPE, 
especially for linear models that tend to over-generalize behavior as a simple 
trend line. MAPE, therefore, performs a lot more robustly when gauging general 
performance, but fails in another sense. 
<!-- This following part is what my research has indicated, and it makes sense, 
but maybe it shouldn't be included --> Mathematically, the absolute value function 
isn't differentiable for all x (it's undefined at the origin). Therefore, given its 
differentiability, the MSPE is preferred for optimization problems in general as 
extrema can be found without iterative methods.

## Testing

We are (purely hypothetically) investigating racial bias in the COMPAS data with 
regards to recidivism. We are trying to predict two_year_recid (which is a 
classification setting), with race as our sensitive variable. Since this is a 
classification setting, the only utility metric we can use (out of the above listings) 
for this scenario is Test Accuracy (misclassification error). For each fairness metric, 
we will try to find the best balance between 
fairness and utility using some of the approaches implemented in the models from this 
package.

<!-- TODO: Note on general strat of deval = 1 and deval = 0 to find best balance by presenting 
the 2 extremes and noting the trends -->

We will consider decile_score and priors_count to be the proxies to our sensitive 
variable (race).[Based on results from **dsldConfounders**]{.column-margin}

We will look at the Random Forests model using **dsldQeFairRF**. This 
function uses the *explicitly deweighted features* strategy, so by 
adjusting the deweight values for proxies, we should be able to adjust fairness and 
utility.

<!-- TODO: Note on custom framework (S Corr) vs Fairness pkg functions -->

### Testing: S Correlation

<!-- TODO: Short intro (maybe mention custom framework/function) -->

We can use the **replicate** function to run several trials of our model 
to increase the reliability of our results:

```{r}
# set up base variables
data <- fairml::compas
yName <- 'two_year_recid'
sName <- 'race'
```

```{r}
############## S Correlation ##############
sCorrTest <- function(deval) {
  partition <- sample(1:nrow(data))[1:round(nrow(data) * 0.3)]
  # partition the data
  train <- data[-partition, ]
  test <- data[ partition, ]
  
  # deweight decile_score and priors_count
  deweightPars = list(decile_score=deval,priors_count=deval)
  
  sink(); sink(nullfile())
  model <- dsld::dsldQeFairRF(data=data, yName=yName, sName=sName, 
                              deweightPars=deweightPars)
  sink()
  
  out <- model$corrsens
  out["MeanCorr"] <- mean(out)
  out["MisclassRate"] <- model$testAcc
  out
}
# calculate the mean of 10 trials of no deweighting and deweighting 
# decile_score and priors_count at 0%
suppressWarnings({
  # Deweight Value = 1
  print(apply(replicate(10, sCorrTest(1)), 1, mean))
  # Deweight Value = 0
  print(apply(replicate(10, sCorrTest(0)), 1, mean))
})
```

This example demonstrates the tradeoff between utility and fairness: as we 
increase the deweight values, our utility also increases (since the 
misclassification error decreases). On the other hand, S Correlation increases, 
which indicates that fairness is *decreasing* since we are trying to increase 
fairness by reducing the impact of S proxies.

<!-- TODO: Note (callout box?) on spread of Misclassification Error, warning that 
the trend shown in Utility may not be an accurate trend -->

### Testing: Demographic Parity

<!-- TODO: Short intro (maybe mention use of Fairness pkg instead of custom 
framework/function) -->

```{r}
############ Demographic Parity #############
dem1 <- dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::dem_parity, 
             nReps = 10, deweightPars = list(decile_score=1, priors_count=1))
dem0 <- dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::dem_parity, 
             nReps = 10, deweightPars = list(decile_score=0, priors_count=0))
# Deweight Value = 1
print(dem1)
# Deweight Value = 0
print(dem0)
```

<!-- TODO: Summarize Results -->
<!-- Same trends as S Correlation -->
<!-- TODO: Note (callout box?) on the small sample sizes of Native American and Asian groups 
in this dataset, potentually causing unusual trends in these groups -->

### Testing: Equalized Odds

<!-- TODO: Short intro -->

```{r}
############## Equalized Odds ###############
dem1 <- dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::equal_odds, 
             nReps = 10, deweightPars = list(decile_score=1, priors_count=1))
dem0 <- dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::equal_odds, 
             nReps = 10, deweightPars = list(decile_score=0, priors_count=0))
# Deweight Value = 1
print(dem1)
# Deweight Value = 0
print(dem0)
```

<!-- TODO: Summarize Results -->
<!-- OPPOSITE trends compared to S Corr and Demo Parity due to possible 
unreliability in misclassification error -->




<!-- TODO: If ALL of the above todo's have been addressed, we can expand the testing/conceptual 
sections with any of the following options, in a rough top-down order of priority:
- More fairness metrics
- More models (besides qeFairRF)*
- * ^ This will likely require a new COMPAS example with a continuous Y name as opposed to
      the binary two_year_recid
- More utility metrics
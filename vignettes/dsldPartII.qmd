
# Part II:  Discovering/Mitigating Bias  in Machine Learning   

In modern applications of machine learning as a predictive modeling tool, it 
would be irresponsible to produce a model that biases one sensitive group over 
another. As such, it becomes imperative to find methods of uncovering
and reducing any such biases. 

## Goals  

There are two main aspects of fair machine learning:

* **Measuring unfairness**: A number of measures have been proposed.

* **Reducing unfairness**: For a given ML algorithm, how can we
  ameliorate its unfairness, yet still maintain an acceptable utility
  (predictive power) level?

In our earlier COMPAS example:  Is the risk assessment tool biased
against African-Americans?  And if so, how can we reduce that bias while
still maintaining good predictive ability?

## Comparison to Part I

First, recall our notation: [Actually, think of X as, at first,
consisting of all variables other than Y and S, but then selecting some
of X as O, with X then being all variables but Y, S and O.]{.column-margin}

* Y; our outcome variable, to be predicted

* S: our sensitive variable

* O: our proxy variables

* X: other variables to be used to predict Y

We wish to predict Y from X and O, omitting S, but with concern that we
may be indirectly using S via O.  Contrast this from our material in
Part I:

* In Part I, we fit models for predicting Y, but with the goal of using
such models to assess the effect of S on Y; we were not interested in
actually predicting Y.  We made central use of S, but wished to find
variables C that were correlated with both Y and S, so as to avoid
distorting our look at the impact of S on Y.  Any X variable unrelated
to Y was not of interest.

* Here in Part II, prediction of Y is our central goal.  We will omit S,
relying our prediction fully on X and partly on O.  We find the
variables O by investigating which variables are related to S; the
stronger the relation of an O variable to S, the less weight we will put
on that variable in predicting Y.

Below we will describe some common measures of unfairness.  But first,
how do we choose the O variables?

### Deciding proxies  

So, how does one choose those proxies O?

As with choosing confounders in Part I, analyst may simply choose the
proxies by using his/her domain expertise.  But a more formal approach
may involve correlation.  Here's an example, using the COMPAS data:

```{r}
data(compas1) 
cmp <- compas1[,-3]  # omit decile, as we are developing our own tool
dsldOHunting(cmp,'two_year_recid','race')
```

The function **dsldOHunting** calculates the correlations between S and
[It should be kept in mind that, as with any statistic, all utility and
fairness measures are subject to sampling variation.  Better precision
may be attained via the *bootstrap*; see @sec-boot.]{.column-margin}
all possible candidate O variables.  The output here  suggests possibly
using, say, the **age** and **priors_count** variables as proxies.

The function does not use the classic *Pearson product moment*
correlation here, opting instead for *Kendall's tau* correlation.  Both
are widely-used, and both take on values in [-1,], but while Pearson is
geared toward continuous numeric variables, Kendall is also usable for
[Tau is defined in terms of
*concordances* and *discordances*.  Say we look at height and weight,
and compare two people. If one of the people is both taller and heavier
than the other, that is a concordance.  If on the other hand, one is
shorter but heavier than the other, that is a discordance.  We consider
all possible pairs of people in our dataset, then sets tau to the
difference in concordance and discordance counts, divided by the number
of pairs.]{.column-margin} 
binary or ordinal integer-valued variables.  

## Measuring utility

Utility in ML classification algorithms in general,
and both utility and fairness in the fair ML classification realm,
often (but far from always) make use of quantitaties like False Positive
Rate (FPR).  So, let's start by defining these rates.

::: {.callout-note}
### The famous rates FPR etc.

These are conditional probabilities, but it's important not to confuse
the event with the condition.

Consider binary Y classification problems, where we label Y is either
*positive* (e.g. patient has the disease) or *negative* (e.g. patient does
not have the disease).

After we fit our ML tool to predict Y, we use it for prediction.
Consider a long period of time in which we do such predictions.  During 
that time, define the following counts:

* FP: Count of the number of times we predict Y to be positive and
actually it's negative.

* FN: Count of the number of times we predict Y to be negative and
actually it's positive.

* TP: Count of the number of times we predict Y to be positive and
actually it's positive.

* TN: Count of the number of times we predict Y to be negative and
actually it's negative.

Then some key rates are:
[It is standard to guess Y = 1 if the probability of that event is at
least 0.5.  But other thresholds can be used, with TPR and FPR varying
as we vary the threshold. The *ROC curve* is the resulting graph of 
TPR vs. FPR; see **qeROC** in the **qeML** package.]{.column-margin}

* FPR: FP / (FP + TN) = P(guess Y positive | Y actually is negative)

* TPR: TP / (TP + FN) = P(guess Y positive | Y actually is positive)

* FNR: FN / (TP + FN) = P(guess Y negative | Y actually is positive)

* TNR: TN / (FP + TN) = P(guess Y negative | Y actuually is negative)

So for instance, FPR is the proportion of time we guess positive, *among
those times* in which Y is actually negative.  Two other common terms:

* *recall*:  same as TPR

* *sensitivity*:  same as TPR

* *precision*: P(Y is actually positive | we guess Y is positive)=
(TP + FN) / (TP + FP)

:::

A simple but quite common measure of utility in binary classification
problems is the overall misclassification probability [The *F1-score*
has recently been especially popular in the ML research world.  It is
defined as the harmonic mean of precision and recall, and is thought to
be especially useful in applications in which one class or the other is
very rare.]{.column-margin} of misclassification; **qeML** predictive
functions report this in the **testAcc** component of the functions'
return object.  There are many, many other measures.

For whatever reason, ML research has tended to focus on that binary Y
case, and there are no analogous acronyms for numeric Y.  Accuracy of
the latter is handled simply as Mean Squared Prediction Error (MSPE),
the average squared difference between predicted and actual Y value, or
Mean Absolute Prediction Error (MAPE).  


## Measuring unfairness

Many unfairness criteria have been proposed.  We preseent a few of them
in this section.  [See the [Xiang and
Raji](https://arxiv.org/pdf/1912.00761.pdf) for an excellent analysis of
the legal implications of various fairness measures.]{.column-margin} It
should be kept in mind that, just as there is no single ML algorithm
that predicts the best in all applications, one's choice of fairness
measure also will depend on the given application.

### S-Correlation

A direct way to measure where Y and S are still related in spite of
physically omitting the latter is to compute the correlation between
predicted Y, to be denoted $\hat{Y}$ and S.  As noted earlier, we use
Kendall's Tau correlation here.

[Here and below, to keep things simple, we will not use a holdout
set.]{.column-margin} For instance, let's consider our mortgage example,
say with k-Nearest Neighbors as our ML prediction tool:

```{r}
z <- qeKNN(cmp,'two_year_recid',holdout=NULL,yesYVal='Yes') 
# look at the fitted model's probability of recidivism,
# i.e. regression estimates
probs <- z$regests
# the variable 'race' is an R factor, need a numeric
black <- ifelse(cmp$race=='African-American',1,0)
cor(black,probs,method='kendall')
```

That's a pretty substantial correlation, definitely a cause for concern
that our ML analysis here is unfair.  Of course, it's not the algorithm
itself's fault, but we must find a way to mitigate the problem.

[Prediction of income might be of interest in, say, a marketing context.
Actually, the field of marketing has been the subject of much concern in
fair ML; e.g. see
[FWA](https://thefwa.com/cases/a-marketers-guide-to-machine-learning-fairness).]{.column-margin}
An advantage of the S-Correlation measure is that it can also be used in
non-classification problems, say predicting wage income, and take age as
our sensitive variable S:

```{r}
z <- qeKNN(svcensus,'wageinc',holdout=NULL)
cor(z$regests,svcensus$age,method='kendall') 
```

So, again, our ML tool seems to be biased, this time in terms of age.

### Demographic Parity

The criterion for demographic parity is that the same proportion of each
sub-group within the sensitive feature is classified at equal rates for
each of the possible outcomes 

For example, let's consider the COMPAS dataset again. Demographic Parity
would require 

P(predict recidivate | Black) = P(predict recidivate | white),

i.e. that the quanitty

(TP+FP) / (TP+FP+TN+FN)

has the same value for each race.

Below is an example using the COMPAS data:

```{r}
z <- qeKNN(cmp,'two_year_recid',holdout=NULL,yesYVal='Yes')
# determine which rows were for Black applicants, which not
BlackRows <- which(cmp$race == 'African-American')
NonblackRows <- setdiff(1:nrow(cmp),BlackRows)  # all the others
# regests, the output of qeKNN, is the vector of fitted probabilities
# (recall that for the Y = 0,1 case, the mean reduces to the probability
# of a 1)
BlackProbs <- z$regests[BlackRows]
NonblackProbs <- z$regests[NonblackRows]
# if a probability is > 0.5, we will guess Y = 1, otherwise guess Y = 0;
# conveniently, that's same as rounding to the nearest integer
BlackYhats <- round(BlackProbs)
NonblackYhats <- round(NonblackProbs)
# again, recall that the mean of a bunch of 0s and 1s is the proportion
# of 1s, i.e. the probability of a 1
mean(BlackYhats)
mean(NonblackYhats)
```

That's quite a difference!  Overall, our ML model predicts about 51% of
Black defendants to recidivate,j versus than 27% for non-Blacks.

However, such a criterion is generally considered too coarse, since it doesn't
account for possible differences in qualifications between the two groups.
In other words, one must take confounders into account, as we did in Part I.

### Equalized Odds

This criterion takes a retrospective view, asking in the case of COMPAS:

> Among those who recidivate, what proportion of them had been predicted
> to do so?  And, does that proportion vary by race?

If the answer to that second question is No, we say our prediction tool
satisfies the Equalized Odds criterion.

So, Equalized Odds requires the quantity

TP / (TP+FN)

to be the same for each sensitive group.

### The **fairness** package

This package calculates and graphically displays a wide variety of
fairness criteria.  For instance, let's use it to evaluate the Equalized
Odds criterion in the above COMPAS example.

```{r}
library(fairness)
equal_odds(cmp,'two_year_recid','race',probs=z$regests)
```
Taking African-Americans as the base, we see that the Equalized Odds
criterion was not met, even approximately.  Nor was Demographic Parity.

## Remedies

Having established that ML prediction models can be biased against
certain sensitive groups, what remedies are available?  The **dsld**
package includes a few of these, presented in this section.  Of course,
all of them recognize a basic principle:

::: {.callout-note}
### The Fairness-Utility Tradefoff

Of course, nothing comes for free: the inherent tradeoff of increasing
fairness is reduced utility (reduced predictive power over the dataset).
Thus, a means of balancing this tradeoff between fairness and utility
becomes essential in any future implementations of machine learning.

:::

Any algorithm for ameliorating unfairness will thus include one or more
parameters that one can use to achieve a desired level of compromise between
fairness and utility.  The parameters essentially allow us to "dial" the
weight that our proxies will play in predicting Y; lighter weight means
more fairness but poorer utility, and vice versa.

Our context will be:

* Due to legal requirements or simply a desire for fairness, we will
  omit S from all analyses, other than for fairness assessment of our
  derived prediction tool.

* But we are concerned about the impact of proxies, and have chosen a
  set of variables O to play this role.

* We have chosen fairness and utility measures with which we will choose
  a desired point in the Fairness-Utility Tradefoff.

Many, many fair ML algorithms have been proposed, most of which are
technically complex.  The ones we present here have been chosen (a) for
their technical simplicity and (b) availability as R packages.
We will begin with the simplest algorithms (which have been developed by
one of the authors of this book).

Let's take as a running example the COMPAS data, predicting recidivism,
with age and prior convictions count as proxies.  To illustrate
prediction, we will predict the first case in the dataset:

```{r}
newx <- cmp[1,-(7:8)]  # just X and O, not Y and S
```

## **dsldQeFairRF**

This function fits an RF model, but with deweighting of the proxies.

```{r}
z <- dsldQeFairRF(cmp,'two_year_recid','race',
   list(age=0.2,priors_count=0.1),yesYVal='Yes')
z$corrs
predict(z,newx)
```

Recall that in RFs, each tree will use a different random ordering of
the X and O variables.  At any given node in a tree, a variable is
chosen at random to set up a possible split point.  But here we are
specifying that the age and priors count variables be used only 20% and
10% as often as other variables, in order to reduce their impact.

Yet the S-Correlation valuea are still substantial.  We need to give
the O variables even smaller weights or possibly include additional
variables in our O set.

## **dsldQeFairKNN**

Remember, the common theme here is reducing the role played in
prediction by the proxies O.  How might this be done with k-NN?

An easy answer is to weight the distance metric.  Ordinarily, if we move
3.2 meters to the right and then 1.1 meters forward, the distance
between our new point and our original one is

$\sqrt{3.2^2+1.1^2} = 3.38$

That puts equal weight in the left-right direction and the forward-back
direction, which makes sense for geometric distance.  But in data
prediction, we can use different weights.  In prediction wage income in
the Census data, say, we can place large weight on age and occupation,
and less weight on education.  The above call would change to:

```{r,eval=FALSE}
z <- dsldQeFairKNN(cmp,'two_year_recid','race',
   list(age=0.2,priors_count=0.1),yesYVal='Yes')
z$corrs
predict(z,newx)
```

## Remedies based on "shrunken" linear models

One of the most striking advances in modern statistics was the discovery
that classical estimators tend to be  "too big," and that one may
improve accuracy by "shrinking" them.  The details involve matrix
algebra, but here is the intuition:

Consider the example in @sec-compas.  The vector of estimated regression
coefficients was 

(12.46,-0.94,0.39,0.16,...)

We might (crudely) shrink this vector by multiplying by a factor of,
say, 0.5, yielding

(6.23,-0.47,0.20,0.08,...)

The actual shrinkage mechanisms used in the regression context are much
more complex than simply multiplying every component of the vector by
the same constant, but this is the basic principle.

Why might this odd action be helpful?  Shrinking introduces a bias, but
reduces variance (roughly speaking, smaller variables vary less).  If
outliers (extreme values, not errors) are common in the data, these
result in large estimator variance, possibly so much that it overwhelms
our increase in bias.

Some readers may have heard of *ridge regression* and the *LASSO*, both
of which impose shrinkage.  For our fair ML context, though we wish to
perform our shrinkage focusing on *only some* of the estimated regression
coefficients, specifically those corresponding to our proxies.  To be
sure, it should be noted that non-proxy coefficients are affected too.

Note that these estimators also have the potential ancillary benefit
obtained from shrinkage in general, i.e. better utility.


### Nonconvex Optimization with Fairness Constraints
As introduced by Komiyama et al. in 2018, this approach to fair machine 
learning defines a framework that **(1)** allows users to set a level of 
fairness to provably optimize accuracy for, **(2)** allows for numeric 
sensitive features, and **(3)** accomodates multiple sensitive features 
concurrently.

The authors of this approach primarily focused on two methods for gauging 
fairness: **demographic parity** and **s-correlation**. While we'll go more 
into detail later about these metrics, it's important to note that the latter 
metric motivates a part of their approach. Briefly put, s-correlation measures 
the relationship between the sensitive feature (often referred as $S$) and the 
predicted outcomes (often referred as $Y$).

Komiyama et al.'s approach works as such:

The first step in their process is segmenting out the proxies (features that 
are influenced by the sensitive feature; occupation, for example, is often a 
proxy for gender) and any other information in the dataset that has strong 
ties to this sensitive feature. What we can effectively do is rewrite $X$, the 
full dataset, as a combination of parts explainable by the sensitive feature 
(proxies, etc.) and the residuals that the sensitive feature cannot explain. 
How this is done is not entirely relevant to understanding the algorithm, but 
Komiyama essentially predicts $X$ from $S$ to find the residuals $U$. With 
a small proof, this can be determined to have 0 covariance (that is, similar 
behavior) with respect to the sensitive feature[Note: theoretically speaking, 
this is only proved to be as the number of samples in the dataset approaches 
infinity. In all practical applications, however, the covariance is so small 
it's practically zero; a negligible difference.]{.column-margin}.

What does this all mean? We've essentially managed to extract the data from 
$X$ that is completely distinct from the sensitive feature. So why not use 
this to predict? In most cases where fair machine learning is necessary, the 
response variable $Y$ normally has a strong relationship with $S$. By removing 
essentially all the information $S$ provides from the dataset, its effectively 
stripped of the majority of its predictive power. The practical implementation 
of these residuals, therefore, is to combine it with the $S$ data in a limited 
capacity. In other words, what's being done is splitting $X$ into its fair and 
unfair datapoints,[Note: by datapoints, it's not necessarily referring to 
distinct entries, but rather a decomposition of the same datapoint into a part 
contributed by the sensitive feature and a part contributed by the other 
features]{.column-margin} and then reintroducing the unfair portion to a 
user-defined level of fairness.

Ultimately, all users of Komiyama's approach need to understand is that it can 
provably optimize for a given level of fairness. The research Komiyama et al. 
have done have motivated further improvements on this class of 
fairness-constrained modeling with machine learning, some which will be 
introduced below.

::: {.callout-note}
Komiyama et al.'s functions are implemented under the fairML package as 
`nclm`, a linear-regressive model. In dsld, this is implemented as `dsldNclm`.
:::



## Testing

We are (purely hypothetically) investigating racial bias in the **COMPAS** data with 
regards to recidivism. We are trying to predict two_year_recid (which is a 
classification setting), with race as our sensitive variable. Since this is a 
classification setting, the only utility metric we can use (out of the above listings) 
for this scenario is Test Accuracy (misclassification error). For each fairness metric, 
we will try to find the best balance between fairness and utility using an approach 
implemented in the models from this package.

[Based on results from **dsldConfounders**]{.column-margin}We will consider decile_score 
and priors_count to be the proxies to our sensitive variable (race).

We will look at the Random Forests model using **dsldQeFairRF**. This 
function uses the *explicitly deweighted features* strategy, so by 
adjusting the deweight values for proxies, we should be able to adjust fairness and 
utility.

That approach used in this model to minimize fairness is *Explicitly Deweighted Features*, 
so we will present the two extreme cases (deweighting and not deweighting) and determine 
the fairness and utility for each based on the selected metrics. The trends in fairness 
vs utility can help inform the user's decision of where the best balance is (at the end of 
the day, there is no definitive balance between fairness and utility).

### Testing: S Correlation

**DSLD** has a **dsldFairTest** function that allows us to invoke a test function
on different models. For **dsldQeFairRF**, we can simply call the *corrsens* attribute
to get the S Correlations of the model with and without the deweighted features.

```{r, eval = FALSE}
# set up base variables
data <- fairml::compas
yName <- 'two_year_recid'
sName <- 'race'

############## S Correlation ##############
# The dsldQeFairRF model has a corrsens attribute that contains the 
# S Correlations of all the levels
scorr <- function (model) model$corrsens

# No deweights
dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, scorr, nReps = 20)
# Results
                          Metric Misclass Error
race.African-American 0.30380005      0.2164294
race.Caucasian        0.20623475            NaN
race.Hispanic         0.27411083            NaN
race.Other            0.20974439            NaN
race.Asian            0.19859936            NaN
race.Native American  0.04276491            NaN

# Deweight decile_score and priors_count
dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, scorr, nReps = 20, 
            deweightPars = list(decile_score=0, priors_count=0))
# Results
                          Metric Misclass Error
race.African-American 0.21539335      0.2234055
race.Caucasian        0.15130625            NaN
race.Hispanic         0.19563113            NaN
race.Other            0.13279856            NaN
race.Asian            0.16259914            NaN
race.Native American  0.01459168            NaN
```

Recall that the closer the Misclassification Error is to 0, the higher the utility.
The closer the S Correlation is to 0, the higher the fairness.

This example demonstrates the tradeoff between utility and fairness: when we 
deweight the Race proxies, our utility also **decreases** (since the 
misclassification error increases). On the other hand, S Correlation decreases, 
which indicates that fairness is **increasing** since we are trying to increase 
fairness by reducing the impact of S proxies.

### Testing: Demographic Parity

The **dsldFairTest** function really shines with the **fairness** package, which
has several functions for several fairness metrics, including demographic
parity. Its important to note that the demographic parity we described earlier is 
labeled *prop_parity* under the fairness package.

```{r, eval = FALSE}
# No deweights
dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::prop_parity, nReps = 20)
# Results
                 Proportion Proportional Parity Group size Misclass Error
African-American  0.5154766           1.0000000     884.20      0.2146071
Caucasian         0.3565180           0.6917514     613.95            NaN
Hispanic          0.3132897           0.6082632     149.60            NaN
Other             0.3070574           0.5951850      94.95            NaN
Asian             0.2225316           0.4324312       8.95            NaN
Native American   0.3816667           0.7379000       4.35            NaN

# Deweight decile_score and priors_count
dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::prop_parity, nReps = 20, 
            deweightPars = list(decile_score=0, priors_count=0))
# Results            
                 Proportion Proportional Parity Group size Misclass Error
African-American  0.5030514           1.0000000     874.70      0.2218964
Caucasian         0.3800740           0.7556834     624.20            NaN
Hispanic          0.3328865           0.6624944     149.85            NaN
Other             0.2912062           0.5796095      94.10            NaN
Asian             0.2225991           0.4391436       9.10            NaN
Native American   0.3657143           0.7320974       4.05            NaN
```

For fairness, we are concerned with the Proportional Parity column--the closer its 
values are to 1, the higher the fairness.

We see a similar set of results to our S Correlation testing: in the transition from 
deweight values = 1 to deweight values = 0, we see that the utility is **decreasing** 
since the misclassification error is decreasing. For our demographic parity results, 
the distance from 1 is smaller at the deweight 
value of 0, so deweighting proxies is **increasing** our fairness according to 
demographic parity.

[The way the fairness package calculates the proportional parity column is by
finding a factor that sets the proportion of the first level (African-Americans
in this case) to 1.0, and scales the rest of the levels by that factor.]{.column-margin}

::: {.callout-note}
##### Randomness in Misclassification Error
You might have noticed that the results for Native-American and Asian groups seem 
to contradict the overall trends we are analyzing. This could be due to the small 
sample sizes in this dataset for Native-American and Asian sub-groups (14 and 28 
respectively, compared to 2941 for African-American). Due to the small sample size, 
we will be ignoring the results for these sub-groups in our analyses.
:::

### Testing: Equalized Odds

Similar to our approach for measuring demographic parity, we will make use of the 
*equal_odds* label with the **fairness** package to determine the equalized odds.

```{r, eval = FALSE}
# No deweights
dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::equal_odds, nReps = 20)
# Results
                 Sensitivity Equalized odds Group size Misclass Error
African-American   0.7655988      1.0000000     876.75      0.2140661
Caucasian          0.6917817      0.9040189     618.20            NaN
Hispanic           0.6500930      0.8495973     151.80            NaN
Other              0.6981959      0.9116227      96.00            NaN
Asian              0.8888889      1.1648472       9.25            NaN
Native American    0.6491228      0.8461428       4.00            NaN

# Deweight decile_score and priors_count
dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::equal_odds, nReps = 20, 
                  deweightPars = list(decile_score=0, priors_count=0))
# Results
                 Sensitivity Equalized odds Group size Misclass Error
African-American   0.7444010      1.0000000     882.70      0.2251708
Caucasian          0.6882518      0.9252327     612.80            NaN
Hispanic           0.6654964      0.8942501     152.80            NaN
Other              0.6960300      0.9345101      94.85            NaN
Asian              0.8333333      1.1208604       8.85            NaN
Native American    0.5601852      0.7530386       4.00            NaN
```

For fairness, we are concerned with the Equalized Odds column--the closer its 
values are to 1, the higher the fairness.

Similar to demographic parity, we can see a trend where the fairness is 
**increased** at a deweight value of 0, while our utility **decreases** as 
missclassification error increases.

Overall, the different fairness metrics show us that deweighting the proxies 
to the sensitive variable will help to **increase fairness at the cost of utility**.


<!-- TODO: If ALL of the above todo's have been addressed, we can expand the testing/conceptual 
sections with any of the following options, in a rough top-down order of priority:
- More fairness metrics
- More models (besides qeFairRF)*
- * ^ This will likely require a new COMPAS example with a continuous Y name as opposed to
      the binary two_year_recid
- More utility metrics
-->

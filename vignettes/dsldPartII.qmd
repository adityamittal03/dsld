
# Part II:  Fairness in Machine Learning

In modern applications of machine learning as a predictive modeling tool, it 
would be irresponsible to produce a model that biases one sensitive group over 
another. As such, it becomes imperative to find methods of uncovering
and reducing any such biases. 

## Goals  

There are two main aspects of fair machine learning:

* **Measuring unfairness**: A number of measures have been proposed.

* **Reducing unfairness**: For a given ML algorithm, how can we
  ameliorate its unfairness, yet still maintain an acceptable utility
  level?

In our earlier COMPAS example:  Is the risk assessment tool biased
against African-Americans?  And if so, how can we reduce that bias while
still maintaining good predictive ability?

Of course, nothing comes for free: the inherent tradeoff of increasing
fairness is reduced utility (reduced predictive power over the dataset).
Thus, a means of balancing this tradeoff between fairness and utility
becomes essential in any future implementations of machine learning.

## Comparison to Part I

First, recall our notation: [Actually, think of X as, at first,
consisting of all variables other than Y and S, but then selecting some
of X as O, with X then being all variables but Y, S and O.]{.column-margin}

* Y; our outcome variable, to be predicted

* S: our sensitive variable

* O: our proxy variables

* X: other variables to be used to predict Y

We wish to predict Y from X and O, omitting S, but with concern that we
may be indirectly using S via O.  Contrast this from our material in
Part I:

* In Part I, we fit models for predicting Y, but with the goal of using
such models to assess the effect of S on Y; we were not interested in
actually predicting Y.  We made central use of S, but wished to find
variables C that were correlated with both Y and S, so as to avoid
distorting our look at the impact of S on Y.  Any X variable unrelated
to Y was not of interest.

* Here in Part II, prediction of Y is our central goal.  We will omit S,
relying our prediction fully on X and partly on O.  We find the
variables O by investigating which variables are related to S; the
stronger the relation of an O variable to S, the less weight we will put
on that variable in predicting Y.

Below we will describe some common measures of unfairness.  But first,
how do we choose the O variables?

### Deciding proxies  

So, how does one choose those proxies O?

As with choosing confounders in Part I, analyst may simply choose the
proxies by using his/her domain expertise.  But a more formal approach
may involve correlation.  Here's an example, using the COMPAS data:

```{r}
data(compas1) 
cmp <- compas1[,-3]  # omit decile, as we are developing our own tool
dsldOHunting(cmp,'two_year_recid','race')
```

The function **dsldOHunting** calculates the correlations between S and
all possible candidate O variables.  The output here  suggests possibly
using, say, the **age** and **priors_count** variables as proxies.

The function does not use the classic *Pearson product moment*1 [Tau is
defined in terms of *concordances* and *discordances*.  Say we look at
height and weight, and compare two people. If one of the people is both
taller and heavier than the other, that is a concordance.  If on the
other hand, one is shorter but heavier than the other, that is a
discordance.  We consider all possible pairs of people in our dataset,
then sets tau to the difference in concordance and discordance counts,
divided by the number of pairs.]{.column-margin} correlation here,
opting instead for *Kendall's tau* correlation.  Both are widely-used,
and both take on values in [-1,], but while Pearson is geared toward
continuous numeric variables, Kendall is also usable for binary or
ordinal integer-valued variables.

## Measuring utility

Utility in ML classification algorithms in general,
and both utility and fairness in the fair ML classification realm,
often (but far from always) make use of quantitaties like False Positive
Rate (FPR).  So, let's start by defining these rates.

::: {.callout-note}
### The famous rates FPR etc.

These are conditional probabilities, but it's important not to confuse
the event with the condition.

Consider binary Y classification problems, where we label Y is either
*positive* (e.g. patient has the disease) or *negative* (e.g. patient does
not have the disease).

After we fit our ML tool to predict Y, we use it for prediction.
Consider a long period of time in which we do such predictions.  During 
that time, define the following counts:

* FP: Count of the number of times we predict Y to be positive and
actually it's negative.

* FN: Count of the number of times we predict Y to be negative and
actually it's positive.

* TP: Count of the number of times we predict Y to be positive and
actually it's positive.

* TN: Count of the number of times we predict Y to be negative and
actually it's negative.

Then some key rates are:

* FPR: FP / (FP + TN) = P(guess Y positive | Y actually is negative)

* TPR: TP / (TP + FN) = P(guess Y positive | Y actually is positive)

* FNR: FN / (TP + FN) = P(guess Y negative | Y actually is positive)

* TNR: TN / (FP + TN) = P(guess Y negative | Y actuually is negative)

So for instance, FPR is the proportion of time we guess positive, *among
those times* in which Y is actually negative.  Two other common terms:

* *recall*:  same as TPR

* *sensitivity*:  same as TPR

* *precision*: P(Y is actually positive | we guess Y is positive)=
(TP + FN) / (TP + FP)

:::

For whatever reason, ML research has tended to focus on that binary Y
case, and there are no analogous acronyms for numeric Y.  Accuracy of
the latter is handled simply as Mean Squared Prediction Error (MSPE),
the average squared difference between predicted and actual Y value.


## Measuring unfairness

### S-Correlation

A direct way to measure where Y and S are still related in spite of
physically omitting the latter is to compute the correlation between
predicted Y, to be denoted $\hat{Y}$ and S.  As noted earlier, we use
Kendall's Tau correlation here.

[Here and below, to keep things simple, we will not use a holdout
set.]{.column-margin} For instance, let's consider our mortgage example,
say with k-Nearest Neighbors as our ML prediction tool:

```{r}
z <- qeKNN(cmp,'two_year_recid',holdout=NULL,yesYVal='Yes') 
# look at the fitted model's probability of recidivism,
# i.e. regression estimates
probs <- z$regests
# the variable 'race' is an R factor, need a numeric
black <- ifelse(cmp$race=='African-American',1,0)
cor(black,probs,method='kendall')
```

That's a pretty substantial correlation, definitely a cause for concern
that our ML analysis here is unfair.  Of course, it's not the algorithm
itself's fault, but we must find a way to mitigate the problem.

[Prediction of income might be of interest in, say, a marketing context.
Actually, the field of marketing has been the subject of much concern in
fair ML; e.g. see
[FWA](https://thefwa.com/cases/a-marketers-guide-to-machine-learning-fairness).]{.column-margin}
An advantage of the S-Correlation measure is that it can also be used in
non-classification problems, say predicting wage income, and take age as
our sensitive variable S:

```{r}
z <- qeKNN(svcensus,'wageinc',holdout=NULL)
cor(z$regests,svcensus$age,method='kendall') 
```

So, again, our ML tool seems to be biased, this time in terms of age.

### Demographic Parity

The criterion for demographic parity is that the same proportion of each
sub-group within the sensitive feature is classified at equal rates for
each of the possible outcomes 

For example, let's consider the COMPAS dataset again. Demographic Parity
would require 

P(predict recidivate | Black) = P(predict recidivate | white),

i.e. that the quanitty

(TP+FP) / (TP+FP+TN+FN)

has the same value for each race.

Below is an example using the COMPAS data:

```{r}
z <- qeKNN(cmp,'two_year_recid',holdout=NULL,yesYVal='Yes')
# determine which rows were for Black applicants, which not
BlackRows <- which(cmp$race == 'African-American')
NonblackRows <- setdiff(1:nrow(cmp),BlackRows)  # all the others
# regests, the output of qeKNN, is the vector of fitted probabilities
# (recall that for the Y = 0,1 case, the mean reduces to the probability
# of a 1)
BlackProbs <- z$regests[BlackRows]
NonblackProbs <- z$regests[NonblackRows]
# if a probability is > 0.5, we will guess Y = 1, otherwise guess Y = 0;
# conveniently, that's same as rounding to the nearest integer
BlackYhats <- round(BlackProbs)
NonblackYhats <- round(NonblackProbs)
# again, recall that the mean of a bunch of 0s and 1s is the proportion
# of 1s, i.e. the probability of a 1
mean(BlackYhats)
mean(NonblackYhats)
```

That's quite a difference!  Overall, our ML model predicts about 51% of
Black defendants to recidivate,j versus than 27% for non-Blacks.

However, such a criterion is generally considered too coarse, since it doesn't
account for possible differences in qualifications between the two groups.
In other words, one must take confounders into account, as we did in Part I.

### Equalized Odds

This criterion takes a retrospective view, asking in the case of COMPAS:

> Among those who recidivate, what proportion of them had been predicted
> to do so?  And, does that proportion vary by race?

If the answer to that second question is No, we say our prediction tool
satisfies the Equalized Odds criterion.

So, Equalized Odds requires the quantity

TP / (TP+FN)

to be the same for each sensitive group.

### The **fairness** package

This package calculates and graphically displays a wide variety of
fairness criteria.  For instance, let's use it to evaluate the Equalized
Odds criterion in the above COMPAS example.

```{r}
library(fairness)
equal_odds(cmp,'two_year_recid','race',probs=z$regests)
```

Taking African-Americans as the base, we see that the Equalized Odds
criterion was not met, even approximately.  Nor was Demographic Parity.

## Approaches Deployed in DSLD
While there exists a plethora of methods for ensuring a fair machine learning 
model, the primary approaches utilized in the **dsld** package rely on fairness 
constraints; each metric has its strengths and weaknesses. As such, it will be 
up to users to decide first which *metric* of fairness makes the most sense 
within their domain, and then based on the performances we record below, choose 
an appropriate model that best balances that measurement of fairness with the 
utility they would like. Again, it should be stressed that how you measure 
fairness and utility will determine the approach taken in balancing the two 
with regards to the model itself.

So, what are fairness constraints? In simple terms, we can treat Machine 
Learning as an optimization problem. We want to find a set of weights that 
somewhow minimizes the amount of error, or cost as it's commonly referred to as, 
that the model produces during testing. This cost function that's used to gauge 
how well/poorly the model is performing provides the feedback necessary to 
make adjustments to the weights as needed.

Fairness constraints come into play by enforcing an additional parameter to 
consider with this cost function during model training. Instead of simply 
saying "minimize the RMSE (Root Mean Square Error)" or whatever utility metric 
chosen (MAPE, MSPE, etc.), this class of fair ML models *also* includes a 
fairness metric to calculate cost. In effect, this produces a final set of 
weights that optimizes not for the best performance [as with most base ML 
models], but for the best **balance** of performance between utility and a 
given definition of fairness.

More simply put, an additional fairness parameter is introduced into the 
optimization problem, forcing whichever optimization algorithm is picked to 
weigh a tradeoff between utility **and** fairness when calculating coefficients 
or weights. This has the effect of balancing the model to be both fair and 
useful to users, but there is a nuance underneath this.

As we will explain in more detail further in this book, the definition of fair 
and fairness with respect to ML algorithms will vary from context to context 
and research paper to research paper. While we provide a plethora of different 
metrics that can be used to gauge fairness, there is no **single correct** 
**metric**; each will have their own use-case. It's up to the reader to decide 
which constraint/metric will optimize the model for their needs.


### Nonconvex Optimization with Fairness Constraints
As introduced by Komiyama et al. in 2018, this approach to fair machine 
learning defines a framework that **(1)** allows users to set a level of 
fairness to provably optimize accuracy for, **(2)** allows for numeric 
sensitive features, and **(3)** accomodates multiple sensitive features 
concurrently.

The authors of this approach primarily focused on two methods for gauging 
fairness: **demographic parity** and **s-correlation**. While we'll go more 
into detail later about these metrics, it's important to note that the latter 
metric motivates a part of their approach. Briefly put, s-correlation measures 
the relationship between the sensitive feature (often referred as $S$) and the 
predicted outcomes (often referred as $Y$).

Komiyama et al.'s approach works as such:

The first step in their process is segmenting out the proxies (features that 
are influenced by the sensitive feature; occupation, for example, is often a 
proxy for gender) and any other information in the dataset that has strong 
ties to this sensitive feature. What we can effectively do is rewrite $X$, the 
full dataset, as a combination of parts explainable by the sensitive feature 
(proxies, etc.) and the residuals that the sensitive feature cannot explain. 
How this is done is not entirely relevant to understanding the algorithm, but 
Komiyama essentially predicts $X$ from $S$ to find the residuals $U$. With 
a small proof, this can be determined to have 0 covariance (that is, similar 
behavior) with respect to the sensitive feature[Note: theoretically speaking, 
this is only proved to be as the number of samples in the dataset approaches 
infinity. In all practical applications, however, the covariance is so small 
it's practically zero; a negligible difference.]{.column-margin}.

What does this all mean? We've essentially managed to extract the data from 
$X$ that is completely distinct from the sensitive feature. So why not use 
this to predict? In most cases where fair machine learning is necessary, the 
response variable $Y$ normally has a strong relationship with $S$. By removing 
essentially all the information $S$ provides from the dataset, its effectively 
stripped of the majority of its predictive power. The practical implementation 
of these residuals, therefore, is to combine it with the $S$ data in a limited 
capacity. In other words, what's being done is splitting $X$ into its fair and 
unfair datapoints,[Note: by datapoints, it's not necessarily referring to 
distinct entries, but rather a decomposition of the same datapoint into a part 
contributed by the sensitive feature and a part contributed by the other 
features]{.column-margin} and then reintroducing the unfair portion to a 
user-defined level of fairness.

Ultimately, all users of Komiyama's approach need to understand is that it can 
provably optimize for a given level of fairness. The research Komiyama et al. 
have done have motivated further improvements on this class of 
fairness-constrained modeling with machine learning, some which will be 
introduced below.

::: {.callout-note}
Komiyama et al.'s functions are implemented under the fairML package as 
`nclm`, a linear-regressive model. In dsld, this is implemented as `dsldNclm`.
:::


### Flexible Fairness Classification
As introduced by Zafar et al. in 2019, this approach to fair machine learning 
defines a framework that **(1)** accomodates multiple definitions of fairness 
simultaneously, **(2)** reduces unfairness with respect to a variable amount of 
sensitive features, and **(3)** is applicable to multiple ML algorithms.

The authors of this approach specifically focused on the following key fairness 
metrics in development:

1. **Disparate Treatment**: if a change in a sensitive feature's value results 
in a substantial change in the predicted outcome. For example, if changing the 
gender results in someone being predicted as a violent crime risk then the 
algorithm would be guilty of disparate treatment.
2. **Disparate Impact**: if the overall result with respect to different 
sensitive sub-groups is different. We also reference this metric as *demographic 
parity*. For example, if Race was the sensitive feature and the white subgroup 
would be given preferential treatment with respect to credit scores (i.e. higher 
scores) than other subgroups, then the algorithm would be guilty of disparate 
impact (in violation of demographic parity).
3. **Disparate Mistreatment**: if the error rate in predictions is substantially 
different with respect to different sensitive sub-groups. For example, if a 
model predicts with higher accuracy if asian applicants will be accepted for a 
loan or not, then we can conclude the model is guilty of disparate mistreatment.

The contraint employed by Zafar et al. in their approach relies on binding the 
covariance of the sensitive features to the predicted outcomes. In other words, 
they have designed a model that constrains its behavior based on if the 
predictions it makes is mimicking the behavior of the sensitive feature. By 
doing so, they can effectively balance the amount of information the sensitive 
attribute provides to the model by limiting the impact it *can* have on the 
model's predictions.

It's worth noting here that this approach has also enabled Zafar et al. to 
optimize with respect to a minimum accuracy threshold, essentially allowing for 
maximal fairness while meeting the minimum benchmark requirements. This is 
especially useful for implementations of fair machine learning in the context 
of business.

::: {.callout-note}
While we are yet to apply this approach in the **dsld** package, users are 
encouraged to explore the fairML package if they deem this framework best 
suites their needs.

These are implemented under the functions `zlm`, `zlm.orig`, `zlrm`, and 
`zlrm.orig`. This is for both linear & logistic regression, although the paper 
defines approaches for SVMs as well.
:::


### Penalty Driven Fair Ridge Regression
First introduced by Scutari et al. in their 2021 paper. Building on the prior 
research of Komiyama and Zafar, they introduce a ridge-regression analog to 
Komiyama's non-convex optimization approach that **(1)** allows for 
applications beyond just linear regression, **(2)** provides a closed-form 
solution that optimizes for a given fairness constraint, **(3)** produces an 
intuitive set of coefficients for more effective interpretation, and **(4)** 
enables users to choose from a selection of fairness metrics to optimize for.

The approach Scutari et al. took in this paper follows much of the same 
sequence for the initial steps. They first decompose the training data $X$ 
into its sensitive $S$ and residual $U$ parts, then trains on them separately. 

The catch here is the residuals can be trained entirely without penalty, thus 
allowing for the most optimal set of weights to be generated for the 
residuals. Recall that this contrasts with regards to Komiyama et al.'s 
approach which produced coefficients for the residuals and sensitive features 
in one step, in effect making the coefficients for the residuals change with 
respect to the fairness constraint. Intuitively, this makes sense when 
considering that the residuals by definition do not have any relationship with 
the sensitive features. As such, why would the weights we use for the 
residuals have any dependence on how fair the constraint is set to be? After 
all, they have no bearing on unfairness.

With this in mind, Scutari's model then trains the with increasing levels of 
penalty until the desired level of fairness is arrived upon.[Note: Scutari 
also derived a closed-form version of this iterative penalty-finding approach, 
but it fails in cases where sensitive features aren't mutually independent]{.column-margin}

In their own testing, Scutari's approach has outperformed the analagous models 
from the aforementioned approaches. It is, however, important to note that 
there do exist positives and negatives to each approach.

::: {.callout-note}
Derived from the fair ML package (which Scutari himself had developed), this 
approach is deployed in the `frrm` and `fgrrm` models. Within DSLD itself, we 
have also included easy-to-deploy `dsldFrrm` and `dsldFgrrm` wrappers that 
leverage these models.
:::


### Explicitly Deweighted Features
<!-- TODO: this will be for the Matloff paper -->



### The Limited Scope of Fair Machine Learning
As it may have become clear when looking at these approaches, a certain class 
of machine learning model has yet to make an appearance: neural-networks. Why 
is that the case? A large majority of fair ML research has avoided diving into 
these "black-box" algorithms for one key reason: explainability.

The term "black-box" is commonplace for a reason: the interpretability of the 
weights a neural net has trained on is [for the large part] low. It's hard for 
machine learning researchers to evaluate a deep learning neural network, let 
alone ensure fairness in its predictions.

It's because of this, in addition to the computational resources and amount of 
data required, that research has primarily focused on simpler models.

As neural networks become more prevalant, however, the need for ensuring bias 
in their training data doesn't persist into their predictions will be of key 
importance. Research is already being done into large language models (LLMs) 
[[Quite technical paper recently published by Li et al.](https://arxiv.org/pdf/2308.10149.pdf)]{.column-margin}
and how to engineer and otherwise augment data on a large scale to ensure 
responses generated don't bias any one sensitive group.


### Understanding the Approaches
A recurring theme (or lack thereof) throughout fair machine learning is no one 
size fits all. Whether its choosing how to measure fairness or how to prevent 
it, everything is dependent on the specific context with which one operates 
in. This can be everything from the dataset you use (does it have historical 
bias, etc.), the country you are in (i.e. the European GDPR prevents the 
processing of race and other sensitive features), or the organization you work 
for (business might favor accuracy over fairness, while government 
organizations may favor the opposite).

The reason we have included whole sections devoted to explaining the 
approaches taken to reducing and measuring unfairness is to enable you to make 
these decisions on your own. Demystifying the inner-workings of these models 
can help you not only optimize the performance of your model, but also 
communicate your findings to others.

There is no *one* metric, *one* model, *one* approach. Any solutions marketed 
as such should be treated with a healthy level of skepticism. We'll repeat 
this frequently throughout the book, but it's a valuable lesson that is best 
understood sooner rather than later.


## Measurements of Fairness & Utility
Different algorithms and approaches attempt to reduce bias as indicated through 
different metrics. In the following subsection, our aim is to explain what each 
of these metrics tell users and which algorithms perform best with respect to 
each one.



### Utility Metrics
Utility metrics are used to inform the user of how accurate and/or reliable 
a model is with regards to its performance. This (usually quantitative) 
information can be used to further modify/optimize a model for its usecase, or 
to help the user make a choice in comparing different models for selection.

We can choose what utility metric to use based on the problem we are trying to 
solve. [Reminder: classification settings are prediction applications where Y is 
categorical, while regression settings are prediction applications where Y 
is continuous and/or ordinal]{.column-margin}
For classification settings, we may want to determine how well a model can 
classify instances into different classes. For regression settings, we may want 
to instead look at how different our predicted values are to the actual values.

Usually, a utility metric has its intended usecase in one of the above settings, 
not necessarily both.


#### Test Accuracies (Misclassification Error)

For classification settings, one simple method to measure the utility of a model 
is to measure how well it can predict something via test accuracy. Normally, 
test accuracy can be defined as follows:

$$
Accuracy = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}
$$

However, when using the **dsld** package for classification settings, the 
**TestAcc** attribute refers to the *error rate*, or how *inaccurate* our 
prediction is.
[For example, if our Y was gender (from **svcensus**) and our model 
produced a **TestAcc** 
between 0.6 and 0.7, our prediction was incorrect around 60-70% of the 
time.]{.column-margin} 
Accuracy and error rate are complements of each other, so 
$Error Rate = 1 - Accuracy$.

For a two-class (binary) classification scenario (i.e. negative and positive, as 
in diagnoses), we can make use of True-False Positives and Negatives to redefine 
our accuracy and error rate formulas as follows:
[See @classification-terms for definitions]{.column-margin}

$$
Accuracy = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$
$$
Error Rate = \frac{\text{FP}+\text{FN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$

The advantage of test accuracy as a utility metric is its simplicity and 
intuitiveness compared to other metrics. However, test accuracy can be limited as 
a utility metric when used in scenarios involving imbalanced classes (i.e. 
substantially more instances of one class over another). In this scenario, a high 
accuracy score may be misleading if it only signifies that the majority class had 
a high prediction accuracy, while the minority class had a low accuracy. Depending 
on the scenario, a low minority class prediction accuracy could indicate a poor 
model--test accuracy alone would not inform the user of this potential failure. 
For this reason, test accuracy is less straightforward with scenarios involving 
several classes, where imbalance is more likely.

Another issue lies with decision boundaries/thresholds, which are used to assign 
instances to a particular class based on its value (i.e. if the threshold is 0.5, 
a value of 0.6 is assigned to Y = 1, while 0.4 is assigned to Y = 0)--some **dsld** 
functions allow for the user to enter a threshold of their choosing. In cases where 
instances fall close to this threshold (i.e. 0.5001), these instances could be 
misclassified, thus affecting the Test Accuracy--more on this in the below section 
on ROC curves.

If our problem is a regression setting as opposed to a classification setting, it 
may fare better to use a different metric (see the below section on MSPE & MAPE).

<!-- uncommenting out ROC, may have to cut some content though -->
#### ROC Curves

[![ROC Curve](partII/rocCurve.png)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic){width=50%}
An ROC curve (receiver operating characteristic curve) is a graph that plots
the model's behavior at various classification thresholds. The graph aids in 
visually identifying optimal thresholds by avoiding confusion matrices.

The graph plots the true positive rate (sensitivity) on the y-axis and the 
false positive rate (selectivity) on the x axis. For each threshold, its true 
positive rate and false positive rate are calculated.

A model that perfectly classifies each sample is plotted at (0, 1). A point at
(1,1), (.75, .75), (.5, .5) etc. means that the proportion of correctly 
classified samples is equal to the the proportion of incorrectly classified
samples. A point at (1, 0) incorrectly classifies each sample.

Say, we are trying to classify a group of people as infected or not infected
to prevent an outbreak. So, initially we set our threshold to be "Lets consider
everyone to be infected." Then our threshold correctly identified a person who 
is infected as infected. But it also incorrectly calculated all non-infected 
peopl as infected. So, this threshold is plotted at (1, 1). 

If we change our threshold to "a person with a high body temperature is 
considered infected" then our point will move on the graph. This may correctly 
identify most people who are infected, but it may also misidentify a few people
who are not infected as infected, and vice-versa. So we can arbitrarily say 
that this threshold is plotted at (.1, .8).

We can keep changing our threshold level and we will get different points on
the graph. It is up to the user to determine what true positive rate is optimal  
for the model and make a risk assessment on who is free to socialize and who
is required to quarantine. The ROC curve, therefore, functions as a diagnostic 
tool to help users choose which threshold optimizes predictive performance.


#### F1-Score

One utility metric trending in recent times, especially popular with the ML 
research world, is F1-Score. The F1-Score is defined as the harmonic mean of 
precision and recall. In more simple terms, the F1-Score essentially balances 
the performance of a classifier model with respect to precision--a ratio of how 
many of the positive predictions are correct $\frac{TP}{TP + FP}$--and recall--
aka TPR (true positive rate), aka the ratio of correctly predicted positive 
outcomes, given by $\frac{TP}{TP + FN}$.

The full formula for F1-Score is given therefore by 
$F_1 = \frac{2}{\text{precision}^{-1} + \text{recall}^{-1}}$

To unpack this, let's first consider harmonic mean. Harmonic mean is simply 
another form of averaging a set of numbers. While people are most familiar with 
the arithmetic mean, $\frac{\sum_{i=0}^{n}x_i}{n}$, the harmonic mean offers 
some behaviors that make it more attractive in machine learning.

::: {.callout-note}
##### Harmonic Means
For anyone curious, the formula for harmonic mean is 
$\frac{n}{\sum_{i=0}^{n}\frac{1}{x_i}}$. It essentially takes reciprocal of the 
arithmetic mean of the reciprocals of the set. This is not relevant, however, 
to understanding F1 and its uses, so don't worry too much about the formula if 
you don't understand it.
:::

1. It's weighting on the smaller of the two (precision or recall) effectively 
stops researchers from adjusting hyperparameters to boost performance of either 
precision or recall without accounting for the other.

2. Mathematically, the F1-Score will always be less than the arithmetic 
mean except for when precision and recall are requivalent

3. The inherent tradeoff between precision and recall also means models will 
be forced to make an intelligent decisions. Take, for example, a dataset 
with binary outcomes where one of the possible outcomes far outnumbers the 
other. A model that blindly predicts this outcome would perform relatively 
well under test accuracy, but against F1-Score this model would likely perform 
poorly.

F1-Score essentially restrains models from biasing one side of this precision-
recall tradeoff, ensuring a more performant model in the face of deployment.

Of course, there is no "one-size fits all", so ensure that this metric fits the 
context of a model before using it. F1 also doesn't have a direct application 
multiclass predictors (although macro-F1-Score exists, but that comes with its 
own set of issues), so strictly this is a binary-classifier metric.


#### MSPE & MAPE
<!-- Mean Squared Prediction Error & Mean Absolute Prediction Error -->
The Mean Squared Prediction Error (MSPE) and Mean Absolute Prediction Error 
(MAPE) are both metrics used for quantifying the accuracy of a predictive 
model. The two parallel each other in many ways as they are both an average of 
the difference in predicted value and the actual value, except one (as the 
names suggest) squares the difference and the other takes its absolute value.

The Mean Squared Prediction Error can be defined as:
$MSPE(L) = \frac{1}{n} * \sum{[(L(X_{i}) - y(X_{i}))^{2}]}$, 
where $L$ is a predictive model and $y(X_{i})$ is the actual value at the 
specified point in space $X_{i}$.

The Mean Absolute Prediction Error can be defined as:
$MSPE(L) = \frac{1}{n} * \sum{|L(X_{i}) - y(X_{i})|}$, 
where $L$ is a predictive model and $y(X_{i})$ is the actual value at the 
specified point in space $X_{i}$.

They both measure roughly the same thing (deviation of the prediction from the 
actual value), but their behavior varies depending on the type of data being 
dealt with. For one, the squared error terms means the MSPE is more sensitive 
to outliers. In other words, outliers may artificially inflate the MSPE, 
especially for linear models that tend to over-generalize behavior as a simple 
trend line. MAPE, therefore, performs a lot more robustly when gauging general 
performance, but fails in another sense.Mathematically, the absolute value 
function isn't differentiable for all x (it's undefined at the origin). 
Therefore, given its differentiability, the MSPE is preferred for optimization 
problems in general as extrema can be found without iterative methods.


## Testing

We are (purely hypothetically) investigating racial bias in the **COMPAS** data with 
regards to recidivism. We are trying to predict two_year_recid (which is a 
classification setting), with race as our sensitive variable. Since this is a 
classification setting, the only utility metric we can use (out of the above listings) 
for this scenario is Test Accuracy (misclassification error). For each fairness metric, 
we will try to find the best balance between fairness and utility using an approach 
implemented in the models from this package.

[Based on results from **dsldConfounders**]{.column-margin}We will consider decile_score 
and priors_count to be the proxies to our sensitive variable (race).

We will look at the Random Forests model using **dsldQeFairRF**. This 
function uses the *explicitly deweighted features* strategy, so by 
adjusting the deweight values for proxies, we should be able to adjust fairness and 
utility.

That approach used in this model to minimize fairness is *Explicitly Deweighted Features*, 
so we will present the two extreme cases (deweighting and not deweighting) and determine 
the fairness and utility for each based on the selected metrics. The trends in fairness 
vs utility can help inform the user's decision of where the best balance is (at the end of 
the day, there is no definitive balance between fairness and utility).

### Testing: S Correlation

**DSLD** has a **dsldFairTest** function that allows us to invoke a test function
on different models. For **dsldQeFairRF**, we can simply call the *corrsens* attribute
to get the S Correlations of the model with and without the deweighted features.

```{r, eval = FALSE}
# set up base variables
data <- fairml::compas
yName <- 'two_year_recid'
sName <- 'race'

############## S Correlation ##############
# The dsldQeFairRF model has a corrsens attribute that contains the 
# S Correlations of all the levels
scorr <- function (model) model$corrsens

# No deweights
dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, scorr, nReps = 20)
# Results
                          Metric Misclass Error
race.African-American 0.30380005      0.2164294
race.Caucasian        0.20623475            NaN
race.Hispanic         0.27411083            NaN
race.Other            0.20974439            NaN
race.Asian            0.19859936            NaN
race.Native American  0.04276491            NaN

# Deweight decile_score and priors_count
dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, scorr, nReps = 20, 
            deweightPars = list(decile_score=0, priors_count=0))
# Results
                          Metric Misclass Error
race.African-American 0.21539335      0.2234055
race.Caucasian        0.15130625            NaN
race.Hispanic         0.19563113            NaN
race.Other            0.13279856            NaN
race.Asian            0.16259914            NaN
race.Native American  0.01459168            NaN
```

Recall that the closer the Misclassification Error is to 0, the higher the utility.
The closer the S Correlation is to 0, the higher the fairness.

This example demonstrates the tradeoff between utility and fairness: when we 
deweight the Race proxies, our utility also **decreases** (since the 
misclassification error increases). On the other hand, S Correlation decreases, 
which indicates that fairness is **increasing** since we are trying to increase 
fairness by reducing the impact of S proxies.

### Testing: Demographic Parity

The **dsldFairTest** function really shines with the **fairness** package, which
has several functions for several fairness metrics, including demographic
parity. Its important to note that the demographic parity we described earlier is 
labeled *prop_parity* under the fairness package.

```{r, eval = FALSE}
# No deweights
dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::prop_parity, nReps = 20)
# Results
                 Proportion Proportional Parity Group size Misclass Error
African-American  0.5154766           1.0000000     884.20      0.2146071
Caucasian         0.3565180           0.6917514     613.95            NaN
Hispanic          0.3132897           0.6082632     149.60            NaN
Other             0.3070574           0.5951850      94.95            NaN
Asian             0.2225316           0.4324312       8.95            NaN
Native American   0.3816667           0.7379000       4.35            NaN

# Deweight decile_score and priors_count
dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::prop_parity, nReps = 20, 
            deweightPars = list(decile_score=0, priors_count=0))
# Results            
                 Proportion Proportional Parity Group size Misclass Error
African-American  0.5030514           1.0000000     874.70      0.2218964
Caucasian         0.3800740           0.7556834     624.20            NaN
Hispanic          0.3328865           0.6624944     149.85            NaN
Other             0.2912062           0.5796095      94.10            NaN
Asian             0.2225991           0.4391436       9.10            NaN
Native American   0.3657143           0.7320974       4.05            NaN
```

For fairness, we are concerned with the Proportional Parity column--the closer its 
values are to 1, the higher the fairness.

We see a similar set of results to our S Correlation testing: in the transition from 
deweight values = 1 to deweight values = 0, we see that the utility is **decreasing** 
since the misclassification error is decreasing. For our demographic parity results, 
the distance from 1 is smaller at the deweight 
value of 0, so deweighting proxies is **increasing** our fairness according to 
demographic parity.

[The way the fairness package calculates the proportional parity column is by
finding a factor that sets the proportion of the first level (African-Americans
in this case) to 1.0, and scales the rest of the levels by that factor.]{.column-margin}

::: {.callout-note}
##### Randomness in Misclassification Error
You might have noticed that the results for Native-American and Asian groups seem 
to contradict the overall trends we are analyzing. This could be due to the small 
sample sizes in this dataset for Native-American and Asian sub-groups (14 and 28 
respectively, compared to 2941 for African-American). Due to the small sample size, 
we will be ignoring the results for these sub-groups in our analyses.
:::

### Testing: Equalized Odds

Similar to our approach for measuring demographic parity, we will make use of the 
*equal_odds* label with the **fairness** package to determine the equalized odds.

```{r, eval = FALSE}
# No deweights
dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::equal_odds, nReps = 20)
# Results
                 Sensitivity Equalized odds Group size Misclass Error
African-American   0.7655988      1.0000000     876.75      0.2140661
Caucasian          0.6917817      0.9040189     618.20            NaN
Hispanic           0.6500930      0.8495973     151.80            NaN
Other              0.6981959      0.9116227      96.00            NaN
Asian              0.8888889      1.1648472       9.25            NaN
Native American    0.6491228      0.8461428       4.00            NaN

# Deweight decile_score and priors_count
dsld::dsldFairTest(data, yName, sName, dsld::dsldQeFairRF, fairness::equal_odds, nReps = 20, 
                  deweightPars = list(decile_score=0, priors_count=0))
# Results
                 Sensitivity Equalized odds Group size Misclass Error
African-American   0.7444010      1.0000000     882.70      0.2251708
Caucasian          0.6882518      0.9252327     612.80            NaN
Hispanic           0.6654964      0.8942501     152.80            NaN
Other              0.6960300      0.9345101      94.85            NaN
Asian              0.8333333      1.1208604       8.85            NaN
Native American    0.5601852      0.7530386       4.00            NaN
```

For fairness, we are concerned with the Equalized Odds column--the closer its 
values are to 1, the higher the fairness.

Similar to demographic parity, we can see a trend where the fairness is 
**increased** at a deweight value of 0, while our utility **decreases** as 
missclassification error increases.

Overall, the different fairness metrics show us that deweighting the proxies 
to the sensitive variable will help to **increase fairness at the cost of utility**.


<!-- TODO: If ALL of the above todo's have been addressed, we can expand the testing/conceptual 
sections with any of the following options, in a rough top-down order of priority:
- More fairness metrics
- More models (besides qeFairRF)*
- * ^ This will likely require a new COMPAS example with a continuous Y name as opposed to
      the binary two_year_recid
- More utility metrics
-->

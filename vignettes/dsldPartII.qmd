
# Part II:  Fairness in Machine Learning

## Motivation
<!-- Don't know how accurate this is, please feel free to make edits -->
In modern applications of machine learning as a predictive modeling tool, it 
would be irresponsible to produce a model that biases one sensitive group over 
another. As such, it becomes imperative to find methods of reducing any such 
biases that a model might hold when it comes to modeling and predicting on 
sensitive datasets.

Of course, nothing comes for free: the inherent tradeoff of increasing fairness 
is reduced utility (predictive power over the dataset). Thus, a means of 
balancing this tradeoff between fairness and utility becomes essential in any 
future implementations of machine learning.

The end result of this modified modeling process is a tool that allows users to:
    1. Compare a fair model against one with no fairness implemented; the 
       effective result is a means of uncovering if biases exist within the 
       dataset itself, but also how much fairness is gained for utility lost.
    2. Reduce the impact of any inherent discrimination within the data (of 
       course only to the level that is specified).


## Example
<!-- introduce running example + context... this example should most likely 
come from COMPAS -->
Let's take a look back at the COMPAS dataset from before to best compare the 
range of approaches to measuring and ensuring fairness in machine learning 
model deployment. [more background here...]


## Approaches Deployed in DSLD
<!-- go over all the approaches we use in the fairML & EDFair wrappers -->
<!-- compare their performance against each other across all metrics 
previously described -->
While there exists a plethora of methods for ensuring a fair machine learning 
model, the primary approaches utilized in the `dsld` package are:
    1. Engineering the data itself before modeling through deweighting
    2. Minimizing the impact of sensitive features in the model itself

Each of these methods have their strengths and weaknesses. It will be up to 
users to decide first which *metric* of fairness makes the most sense within 
their domain, and then based on the performances we record below, choose an 
appropriate model that best balances that measurement of fairness with the 
utility they would like. Again, it should be stressed that how you measure 
fairness and utility will determine the approach taken in balancing the two 
with regards to the model itself.


### Explicitly Deweighted Features



### Fairness Constraints
<!-- accuracy of this information may need to be double-checked -->
Another major approach in assuring a fair machine learning model relies on 
enforcing an additional constraint while training the model. Base-level ML 
algorithms rely on minimizing a certain error (i.e. RMSE (Root Mean Square 
Error), etc.). With this class of fair ML models, a constraint is placed on the 
training itself, ensuring the model optimizes not just for utility, but also 
a given definition of fairness.

More simply put, an additional fairness parameter is introduced into the 
optimization problem, forcing whichever optimization algorithm is picked to 
weigh a tradeoff between utility **and** fairness when calculating coefficients 
or weights. This has the effect of balancing the model to be both fair and 
useful to users, but there is a nuance underneath this.

As we will explain in more detail further in this book, the definition of fair 
and fairness with respect to ML algorithms will vary from context to context 
and research paper to research paper. While we provide a plethora of different 
metrics that can be used to gauge fairness, there is no **single correct** 
**metric**; each will have their own use-case. It's up to the reader to decide 
which constraint/metric will optimize the model for their needs.


### Ridge Penalty
<!-- this will be for the Scutari paper -->
description...


#### Nonconvex Optimization with Fairness Constraints
<!-- this will be for the Komiyama paper -->
description...


#### Other Constraints (paper three, we'll update title later)
<!-- this will be for the Zafar paper -->
description...


## Measurements of Fairness & Utility
Different algorithms and approaches attempt to reduce bias as indicated through 
different metrics. In the following subsection, our aim is to explain what each 
of these metrics tell users and which algorithms perform best with respect to 
each one.


### Fairness Metrics
<!-- compare all the fairness metrics against one another... -->
reiterate points about measuring fairness...


#### S-Correlation
<!-- correlation between S-feature and output -->
In order to ensure an ML model isn't discriminating, it may be a meaningful 
measure of fairness to see how much a sensitive feature like race, gender, or 
age relates to the predictions the model is making. A way to measure this sort 
of association is called S-correlation. Measuring how strongly S (sensitive 
feature) and Y (response variable) are correlated allows us to understand the 
how much the model has trained to discriminate. An added benefit of 
S-Correlation is a warning that the data itself may be biased if there exists 
a strong correlation between S and Y.

The function dsldTakeALookAround calculates the S-Correlation through a
non-parametric test, known as the Kendall rank correlation. The strength of 
associativity is measured by the pattern of accordance/discordance 
(i.e: agreement/disagreement) of pairs of observations. A benefit of using
Kendall's rank correlation is that it can be used with ordinal or continuous
data. It is also viable with smaller sets of data compared to other methods.
For more information on how Kendall rank correlation is calculated, click
[here](https://arxiv.org/abs/2206.04019).


#### Demographic Parity
Demographic parity is a fairness metric that measures if a classifier model's 
predictions are dependent on a sensitive attribute. Demographic parity 
essentially compares if the same proportion of each sub-group within the 
sensitive feature is classified at equal rates for each of the possible 
outcomes.

For example, let's consider the COMPAS dataset again. If the proportion of 
African-Americans who are predicted to recidivate is different than the 
proportion of Asian-Americans, then we can consider this model to fail to be 
demographically par. Of course, exactness is something to be decided by the 
user--what constitutes a signficant difference will vary on the data, context, 
etc.

<!-- the followng paragraphs' accuracy will need to be double-checked -->
The measure may seem a bit restrictive or aggressive at first, but keep in mind 
its main use-cases. Typically, demographic parity performs well as a measure 
for fairness in contexts surrounding historical biases skewing the dataset 
itself. In order to prevent a self-reinforcing cycle (such as in the case of 
COMPAS predicting African-Americans to recidivate more --> kept in jail longer 
--> have a more difficult time adjusting to society post-jail --> more likely 
to commit crime), we'll try to minimize the bias of the model used to predict 
on that dataset as a compensatory measure, even if it comes at the risk of 
losing even more utility.

That last point is crucial: the restrictiveness of demographic parity as a 
fairness metric will mean a lower-reward tradeoff with utility--more will need 
to be sacrificed in performance to achieve fairness. Of course, if this metric 
is used in the first place, the seriousness of the context the model will be 
deployed in motivates the use of something as restrictive as demographic 
parity.


### Utility Metrics
<!-- compare all the utility metrics against one another... -->
Utility metrics are used to inform the user of how accurate and/or reliable 
a model is with regards to its performance. This (usually quantitative) 
information can be used to further modify/optimize a model for its usecase, or 
to help the user make a choice in comparing different models for selection.

We can choose what utility metric to use based on the problem we are trying to 
solve. [Reminder: classification settings are prediction applications where Y is 
categorical, while regression settings are prediction applications where Y 
is continuous and/or ordinal]{.column-margin}
For classification settings, we may want to determine how well a model can 
classify instances into different classes. For regression settings, we may want 
to instead look at how different our predicted values are to the actual values.

Usually, a utility metric has its intended usecase in one of the above settings, 
not necessarily both.

#### Test Accuracies (Misclassification Error)
<!-- accuracy results on test sets -->
For classification settings, one simple method to measure the utility of a model 
is to measure how well it can predict something via test accuracy. Normally, 
test accuracy can be defined as follows:
$$
Accuracy = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}
$$

However, when using the **dsld** package for classification settings, the 
**TestAcc** attribute refers to the *error rate*, or how *inaccurate* our 
prediction is.
[For example, if our Y was gender (from **svcensus**) and our model 
produced a **TestAcc** 
between 0.6 and 0.7, our prediction was incorrect around 60-70% of the 
time.]{.column-margin} 
Accuracy and error rate are complements of each other, so 
$Error Rate = 1 - Accuracy$.

For a two-class (binary) classification scenario (i.e. negative and positive, as 
in diagnoses), we can make use of more descriptive terms:

* True Positives (TP): Number of instances from the positive class predicted 
correctly (i.e. as positive)

* True Negatives (TN): Number of instances from the negative class predicted 
correctly (i.e. as negative)

* False Positives (FP): Type I Error - Number of instances from negative class 
predicted incorrectly (i.e. as positive)

* False Negatives (FN): Type II Error - Number of instances from positive class 
predicted incorrectly (i.e. as negative)

Thus, we can redefine our accuracy and error rate formulas (for binary 
classification) as follows:
$$
Accuracy = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$
$$
Error Rate = \frac{\text{FP}+\text{FN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}}
$$

The advantage of test accuracy as a utility metric is its simplicity and 
intuitiveness compared to other metrics. However, test accuracy can be limited as 
a utility metric when used in scenarios involving imbalanced classes (i.e. 
substantially more instances of one class over another). In this scenario, a high 
accuracy score may be misleading if it only signifies that the majority class had 
a high prediction accuracy, while the minority class had a low accuracy. Depending 
on the scenario, a low minority class prediction accuracy could indicate a poor 
model--test accuracy alone would not inform the user of this potential failure. 
For this reason, test accuracy is less straightforward with scenarios involving 
several classes, where imbalance is more likely.

Another issue lies with decision boundaries/thresholds, which are used to assign 
instances to a particular class based on its value (i.e. if the threshold is 0.5, 
a value of 0.6 is assigned to Y = 1, while 0.4 is assigned to Y = 0)--some **dsld** 
functions allow for the user to enter a threshold of their choosing. In cases where 
instances fall close to this threshold (i.e. 0.5001), these instances could be 
misclassified, thus affecting the Test Accuracy--more on this in the below section 
on ROC curves.

If our problem is a regression setting as opposed to a classification setting, it 
may fare better to use a different metric (see the below section on MSPE & MAPE).

#### ROC Curves
[![ROC Curve](partII/rocCurve.png)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic){width=50%}
An ROC curve (receiver operating characteristic curve) is a graph that plots
the model's behavior at various classification thresholds. The graph aids in 
visually identifying optimal thresholds by avoiding confusion matrices.

The graph plots the true positive rate (sensitivity) on the y-axis and the 
false positive rate (selectivity) on the x axis. For each threshold, its true 
positive rate and false positive rate are calculated.

A model that perfectly classifies each sample is plotted at (0, 1). A point at
(1,1), (.75, .75), (.5, .5) etc. means that the proportion of correctly 
classified samples is equal to the the proportion of incorrectly classified
samples. A point at (1, 0) incorrectly classifies each sample.

Say, we are trying to classify a group of people as infected or not infected
to prevent an outbreak. So, initially we set our threshold to be "Lets consider
everyone to be infected." Then our threshold correctly identified a person who 
is infected as infected. But it also incorrectly calculated all non-infected 
peopl as infected. So, this threshold is plotted at (1, 1). 

If we change our threshold to "a person with a high body temperature is 
considered infected" then our point will move on the graph. This may correctly 
identify most people who are infected, but it may also misidentify a few people
who are not infected as infected, and vice-versa. So we can arbitrarily say 
that this threshold is plotted at (.1, .8).

We can keep changing our threshold level and we will get different points on
the graph. It is up to the user to determine what true positive rate he wants 
for the model and make a risk assessment on who is free to socialize and who
is required to quarantine. The ROC curve is a diagnostic tool to help the user
choose which threshold he wants to use.


#### MSPE & MAPE
<!-- Mean Squared Prediction Error & Mean Absolute Prediction Error -->
The Mean Squared Prediction Error (MSPE) and Mean Absolute Prediction Error 
(MAPE) are both metrics used for quantifying the accuracy of a predictive 
model. The two parallel each other in many ways as they are both an average of 
the difference in predicted value and the actual value, except one (as the 
names suggest) squares the difference and the other takes its absolute value.

The Mean Squared Prediction Error can be defined as:
$MSPE(L) = \frac{1}{n} * \sum{[(L(X_{i}) - y(X_{i}))^{2}]}$, 
where $L$ is a predictive model and $y(X_{i})$ is the actual value at the 
specified point in space $X_{i}$.

The Mean Absolute Prediction Error can be defined as:
$MSPE(L) = \frac{1}{n} * \sum{|L(X_{i}) - y(X_{i})|}$, 
where $L$ is a predictive model and $y(X_{i})$ is the actual value at the 
specified point in space $X_{i}$.

They both measure roughly the same thing (deviation of the prediction from the 
actual value), but their behavior varies depending on the type of data being 
dealt with. For one, the squared error terms means the MSPE is more sensitive 
to outliers. In other words, outliers may artificially inflate the MSPE, 
especially for linear models that tend to over-generalize behavior as a simple 
trend line. MAPE, therefore, performs a lot more robustly when gauging general 
performance, but fails in another sense. 
<!-- This following part is what my research has indicated, and it makes sense, 
but maybe it shouldn't be included --> Mathematically, the absolute value function 
isn't differentiable for all x (it's undefined at the origin). Therefore, given its 
differentiability, the MSPE is preferred for optimization problems in general as 
extrema can be found without iterative methods.


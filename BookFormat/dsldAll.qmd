
---
title: "Data Science Looks at Discrimination"
subtitle: "A toolkit for investigating bias in race, gender, age and so on"
author: 
  - Norman Matloff, Project Lead
  - Taha Abdullah
  - Arjun Ashok
  - Brandon Estrada
  - Shubhada Martha
  - Aditya Mittal 
  - Billy Ouattara 
  - Jonathan Tran
date: "9/13/2023"
crossref:
   labels: roman i
   subref-labels: roman i
format: 
  pdf:
     toc: true
     number-sections: true
---

```{r}
#| include: false
library(dsld)
library(qeML)
```

{{< pagebreak >}}

# Overview

```{r,echo=FALSE}  
data(svcensus)
dsldDensityByS(svcensus,'wageinc',
   'educ','plot',fill=TRUE)
```

Discrimination is a key social issue in the US and in a number of other
countries.  There is lots of available data with which one might
investigate possible discrimination.  But how might such investigations
be conducted?

Our **dsld** package provides both graphical and analytical tools
for this purpose.  We see it as widely applicable; here are just a few
use cases:

* Quantitative analysis in instruction and research in social science.

* Corporate HR analysis and research.

* Litigation involving discrimination and related issues.

* Concerned citizenry. 

This document provides a tutorial regarding applicable methodology, as
well as introduction to use of the package.

## Prerequisite background

In addition to having rudimentary skill in R, the user should have a
very basic [Python wrappers are included for most
functions.]{.column-margin} knowledge of statistical inference--mean,
variance, confidence intervals and tests, and histograms.  A "bare
bones" refresher, with emphasis on intuition, is given in Appendix A.

## The dsld package

The **dsld** package, which this tutorial uses for examples, has two
aims:

* To enable exploratory analysis of possible discrimination effects
through various graphical and tabular functions.

* To enable formal statistical analysis of such effects via addition of
a number of group-comparison operations to general R functions
such as **lm()** and **glm()**, thereby facilitating comparisons across
races, genders and so on.

# Introduction and Motivating Examples

To set the stage, consider the following:

## UC Berkeley discrimination claims 

![UCB](SatherGate.jpg){width=15%}


UC Berkeley was accused of discriminating against female applicants
for graduate school, and indeed the overall acceptance rate for women
was lower than that for men.  This seemed odd, given Berkeley's liberal
reputation.

However, upon breaking the data down [This data is included in R, as the
built-in dataset **UCBAdmissions**.]{.column-margin} according to the
program students were applying to, it was found that in every
department, the female acceptance rate *within that department* was
either higher than the male rate or of similar level.  The problem:
women were applying to more selective programs, causing their overall
rate to below that of men.

## US Census data {#sec-census}

![US Census](USCensus.png){width=25%}

The **svcensus** dataset is a subset of US census data from back in 2000,
focusing on six engineering occupations.  [Included here in the **dsld**
package.]{.column-margin} The question at hand is whether there is a
gender pay gap.  Again, the overall pay for men is higher, by about 25%.
But what if we break things down by occupation?  Though it does turn out
that some occupations pay more than others, and that men and women are
not distributed evenly among the occupations, there still is a gender
pay gap, of about 16%.

## Commonality

In both examples, we have an outcome variable Y of interest--acceptance
rate and wage income--and a sensitive variable S, which was gender in
both examples.  But in both cases, were concerned that merely comparing
mean Y for each gender was an oversimplication, due to a possible
*confounder* C--department in the first example, occupation in the
second.  Failure to take confounders (there can be more than one, and 
usually are so) into account can lead to spurious "relations" between S
and Y.  

::: {.callout-note}
### Confounder Adjustment Settings

So, in general, we wish to investigate the impact of a sensitive
variable S on an outcome variable Y, but *accounting for confounders* C.
Let's call them "confounder adjustment" settings.

:::

Now contrast the above examples with a different kind:

## COMPAS recidivism data

COMPAS is a commercial machine learning software tool for aiding judges
to predict recidivism by those convicted of crimes.  A 2016 [**Pro
Publica**
article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
investigated, finding  the tool to be racially biased; African-American
defendants tended to be given harsher ratings--i.e. higher estimated
probabilities of recidivism--than similarly situated white defendants.

Northpointe, the firm that developed COMPAS, [disagrees with the *Pro
Publica*
analysis](https://www.equivant.com/response-to-propublica-demonstrating-accuracy-equity-and-predictive-parity/),
and we are not supporting either side here.  But if the COMPAS tool were
in fact biased, how could the analysis be fixed?  

**A key point is that any remedy must not only avoid using race directly,
but must also minimize the impact of variables O that are separate from
race but still correlated with it, known as *proxies*.**  If, say,
educational attainment is correlated with race, its inclusion in our
analysis will mean that race is still playing a role in our analysis
after all.  

::: {.callout-note}
### Fair ML Settings

Thus our goal is to predict the outcome variable Y, without 
using the sensitive variable S, while making only
limited use of the proxy variables O.

:::

## Summary:  the two kinds of discrimination analysis covered here

This COMPAS example falls in the category of *fairness in machine learning ML*.

Note the difference between accounting for confounders on the one hand,
and fair ML on the other.  Here is a side-by-side comparison:

  aspect      confounder adjustment   fair ML
  ----------- ----------------------- ------------------------------------
  goal        estimate an effect      predict an outcome
  harm        comes from society      comes from an algorithm
  side info   adjust for confounders  limit impact of proxies 

Part I, on confounder adjustment, focuses on discrimination examples but
is applicable to confounder adjustment applications in general.  Part
II, on fair ML, is more specific to discrimination settings.

## Summary of symbols

We'll use X to denote the rest of the variables, i.e. those that are 
related to Y but are not S, C or O.  The general terminology is that Y
is variously termed the *outcome variable*, *target variable* or
*dependent variable*; the X, C, S and O variables are known collectively
as *covariates*, *features* or *independent variables*.

  example      Y            C                S        O
  ---------    ---------    -------          ---      ---
  UCB admits   acceptance   program          gender   \-
  Census       wage         e.g. occupation  gender   \-
  COMPAS       sentence     \-               race     e.g. education


## Format of this tutorial

We treat the topics in this order:

* adjusting for confounders

* fair ML

Within each of the above topics, we cover:

* graphical and tabular exploration

* formal quantitative analysis

In each case, we present explanations of the relevant concepts, so that
this is a general tutorial on methodology for analysis of
discrimination, and show the details of using our **dsld** package to
make use of that methodology.

So, let's get started.  One key point first, though:

::: {.callout-note}
### Notes on modeling, the role of the software, etc.

This book makes use of the **dsld** software, but is definitely nota
user manual for that package.  Instead, it is a guide to the statistical
principles, with the software playing a supporting role.

Any statistical model is approximate.  [The pioneering statistician
George Box famously said, "All models are wrong, but some are
useful."]{.column-margin}  And virtually any relation of interest in
practice is nonzero.  Modern statistical thinking shies away from
p-values, and asks instead whether A has an effect on B that is
substantial enough to be of interest.

A related point is that, unlike some readers may have experienced in
some statistics courses, real-world statistical analysis is not
conducted in a formulaic, "Step 1, Step 2,..." manner.  Instead,
decisions on say, which model to use, must be made by you, the analyst,
based on your overall assessment of the available information.  The
software cannot make your decisions for you.  


:::


# Part I:  Adjustment for Confounders   

How do we adjust for confounders?  The most common approach involves
[There will also be the question of *which* possible confounders to use.
]{.column-margin} linear models, with which we express the mean Y for
given values of the X, C and S variables in a linear form.  

::: {.callout-note}
### Important term: the *regression function*
The relation of mean Y to the X, C and S variables is formally called
the *regression function* of Y on those variables. Our first model
below, which expreses mean income as a function of age and gender,
will assume this relation as linear, but the term *regression function*
is general.

Indeed, one commonality between statistics and machine learning (ML)
methods is that both types of analysis typically involve estimation of
the regression function, even though they differ in the uses to which
they put such estimates:  In statistics, the goal can be either effect
estimation (e.g. the impact of gender on wages) or prediction, while the
latter is almost always the goal of ML.

:::

## Linear model example: a simple gender wage gap analysis   

Consider the **svcensus** data example in @sec-census above,
investigating a possible gender pay gap.  So Y is wage and S is gender.
We might treat age as a confounder C, reasoning as follows.  Older
workers tend to have more experience and thus higher wages, and if there
is an age differential in our data, say with female workers tending to
be older, this may mask a gender pay gap.  

So, let's take the set of confounders C to consist of age, and for
simplicity in this introductory example, not include any other
confounders, such as occupation, and not include any other
variables X.

### Initial analysis

Our linear model would thus be

> mean W = $\beta$~0~ + $\beta$~1~ A + $\beta$~2~ M

where W is
wage, A is age and M is an indicator variable, with M = 1 for men and M
= 0 for women.  The parameters $\beta$~i~ are estimated by fitting the
model to the data:
[The column svcensus$gender is an R factor. Our function **dsldLinear**
calls R's **lm**, which replaces that column by a dummy variable
**gendermale**, our M above.  If a factor has f levels, i.e. represents
f categories, R will create f-1 dummies.]{.column-margin} 

```{r}
svcensus1 <- 
   svcensus[,c(1,4,6)]  # age, wage, gender
z <- dsldLinear(svcensus1,'wageinc','gender')
coef(z)  # print the estimated coefficients b_i 
```

Let's use b~i~ to denote our estimated $\beta$~i~.  So for instance b~1~
= 489.5728 is our estimate of the unknown population parameter $\beta_1$.  
[The b~i~ are computed using *least squares*, which find the
b~i~ that minimize the sum of the square of the differences between the
observed Y and fitted Y.]{.column-margin} 

### Interpretation of $\beta$~2~

Lots in the output to discuss, which we will gradually cover below.  For
now, note that the estimate b~2~ turns out to be about $13,000, which is
the (estimated) wage gap, if any.  Here's why: [Always keep in mind that
statistical quantities are only estimated, since we work only with
sample data from some population, real or conceptual.  Hence the need
for standard errors, confidence intervals and so on.]{.column-margin} 

Under the model, the mean wage for, say, 36-year-old men is

> $\beta$~0~ + 36 $\beta$~1~ + 1 $\beta$~2~ 

while for women of that age it is

> $\beta$~0~ + 36 $\beta$~1~ 

The difference is $\beta$~2~.  But if we look at, for instance, people of age
43, the mean wages for men and women are

> $\beta$~0~ + 43 $\beta$~1~ + 1 $\beta$~2~ 

and

> $\beta$~0~ + 43 $\beta$~1~ 

and the difference *is still* $\beta$~2~.  

::: {.callout-note}
### "The" effect of gender
Thus we can speak of $\beta$~2~ as *the* gender wage gap, at any age.
According to the model, younger men earn an estimated $13,000 more than
younger women, with the *same-sized* gap between older men and older
women.
:::

The above approach to dealing with confounders is a very common one.
But it raises questions, such as:

* What are the assumptions underlying that model?  And how might we [In
  addition, the data here are, as is commonly the case, *observational*,
  as opposed to being the result of a *randomized clinical trial*; there
  may be serious issues, due to unobserved confounders.  Such problems
  might be solvable via an advanced (and rather controversial)
  methodology known as *causal inference*.  Unfortunately, details are
  beyond our scope in this tutorial, but we will explain some basic
  concepts in @sec-causal.]{.column-margin} check whether they are
  (approximately) valid?

* We chose only one C variable here, age.  We might also include
occupation, as noted earlier.  In some datasets, might have dozens of
possible confounders.  How do we choose which ones to use in our model?
And for that matter, why not use them all?

* The above model, in which the gender wage gap was uniform across all
wages, may not be adequate.  How can we determine this, and what
alternative models might we use?

### Statistical inference

The full output of **dsldLinear()** goes to the heart of discrimination
analysis, enabling statistical inferences on differences in levels of
the sensitive variable S.  Let's take a look, continuing from the above
code: 

```{r}
summary(z)
```

The first half of this output is from **lm()**, which is called by
**dsldLinear()**.  The second half is the "value added" material 
from **dsld**.

So, an approximate 95% confidence interval for the gender wage gap is
[Since the estimated gender gap here is simply b~2~, the CI could of
course have also been obtained directly from the **lm** half of the
output.  But with an S having more than two levels, e.g. race, the
**dsld** enhancement is quite valuable.]{.column-margin}

> 13098.2091 &plusmn; 1.96 x 790.4451

or (11548.94,14647.48).

### With-interactions model

As discussed above, in our model

> mean W = $\beta$~0~ + $\beta$~1~ A + $\beta$~2~ M

we identified b~2~ as *the* difference in mean wage between men and
women, regardless of age, so that for instance:

> According to the model, younger men earn about $13,000 more than younger
> women, with the same-sized gap between older men and older women.

But that may not be true.  On the contrary, gender discrimination and
[Interaction between two types of discrimination is called
*intersectionality* by some analysts.]{.column-margin}
age discrimination may interact.  It may be, for instance, that the
gender gap is small at younger ages but much larger for older people.

Technically, the with-interactions model adds a product term:

> mean W = $\beta$~0~ + $\beta$~1~ A + $\beta$~2~ M + $\beta$~3~ AM 

So for example, the gender pay gap for people of age 36 is

($\beta$~0~ + $\beta$~1~ 36 + $\beta$~2~ 1 + $\beta$~3~ 36) -
($\beta$~0~ + $\beta$~1~ 36 ) =

$\beta$~2~ 1 + $\beta$~3~ 36

And at age 43, the gap is

$\beta$~2~ 1 + $\beta$~3~ 43

So, this model does indeed allow for interaction between age and gender.

However, this added-product-term is a bit abstract, and it is easier
(and approximately equivalent) to simply fit two linear models, one for
men and one for women.

::: {.callout-note}
The **dsldLinear** function includes an argument **interactions**. The
default value is FALSE, but if TRUE, it fits separate linear models for
each level of S.  An additional argument 'newData' is now
needed, through which the user specifies a data frame consisting of
one or more (X,C) values at which to compare the effect of S.
:::

```{r}
newData <- data.frame(age=c(36,43))
z <- dsldLinear(svcensus1,'wageinc','gender',interactions=T,
   newData)
summary(z)
```

So the gender pay gap is estimated to be -13156.88 at age 36, -13039.27
[The classical approach to choosing between the no-interaction and
with-interaction models is of course to test the hypothesis $H_0:
\beta_3=0$.  As noted earlier, modern practice discourages such
approaches, which can be misleading.]{.column-margin}
at age 43, only about $100.  The estimated gap between ages 36 and 53,
not shown, is larger, close to $300, but it seems there is not much
interaction here.  The no-interactions model should be adequate after
all.

### Linearity and other assumptions

As noted, linear models are ubiquitous in observational data analysis.
Open any professional journal in medicine, sociology, economics and so
on, and you'll see many applications of this methodology.  But how would one
check that most basic assumption, the linearity of the mean Y for given
X, C and S values?

::: {.callout-note}
### Assumptions--not just a formality

Assumptions *matter*.  They are never perfectly satisfied, but failure
to be even approximately valid can mean deciding that there is no
discrimination when it actually is there, or vice versa.  It can mean
bad medication being declared by the government as good, or vice versa.
In litigation, if a key expert witness is exposed by opposing counsel as
not having checked the assumptions in his/her analysis, the side for
which the witness was testifying will likely lose the case on the spot.

:::

Typically, linearity is checked graphically.  A common approach involves
plotting the *residuals*, which are the differences between the fitted
line and the Y values.  Here, though, we use another graphical approach,
via a **dsld** function that may be more informative. 

Returning to our earlier setting with just S = gender for our example,
we run

```{r}
dsldConditDisparity(svcensus1,'wageinc','gender',
   'age','age > 0',yLim=c(0,150000)) 
```

The function plots a smoothed graph of Y against a user-specified C
[The function has a 'conditions' argument; we have none here, so we just used a
trival one, 'age > 0']{.column-margin} variable, once for each level of
S.  So, the call here says, "Plot wage income against age, for each
gender."  

The model has mean Y being a linear of function of age, so we  should
expect to see approximate straight lines here.  Yet the relation
certainly looks nonlinear, possibly reflecting age discrimination
against both very young and very old workers.  We are already
investigating one kind of discrimination here, gender, so again for
simplicity let's just keep age as a confounder.  

### Updated model

But we must do something about the substantial nonlinearity [Adding a
squared term does not make our model nonlinear, as it is still linear in
the $\beta$~i~; if we, say, double each of those, the entire expression is
doubled, the definition of linearity. The model is nonlinear in age but
linear in the $\beta$~i~.]{.column-margin} we've discovered,
and one possible remedy is to add an age^2^ term be added to the
equation:

mean W = $\beta$~0~ + $\beta$~1~ A + $\beta$~2~ A^2^ + $\beta$~3~ M

Let's fit the updated model:

```{r}
svcensus1$age2 <- svcensus1$age^2
z <- dsldLinear(svcensus1,'wageinc','gender')
coef(z)  # print the estimated coefficients b_i 
```

So we see that the original wage gap figure of about $13,000 was
incorrect, underestimating it by about 15%.

We see in this example that misspecifying a linear model can have a
major impact on its accuracy.  As usual, though, we will try to keep things
simple, and thus use only the original linear model in our subsequent
examples below.

### Other assumptions

Other than linearity, the standard errors reported by **lm()** also [It
is also assumed that wage income has a normal/gaussian distribution at
each level, but the Central Limit Theorem's implications for the sums
created by **lm()** are in fact approximately normal.]{.column-margin}
assume that variance of wage income is approximately constant across
ages and genders.  Lack of this property has some effect on the accuracy
of reported standard errors, but this can be adjusted via the so-called
*sandwich* operation, an option in **dsldLinear()**.

## S may consist of more than one factor 

In introducing this example, we noted the need to start simple.  Let's
move away from that a bit.

In the **svcensus** data, both age and gender are potential areas of
discrimination.  We can treat both as such by combining these two R
factor variables into one "super R factor," as follows.

We'll need to discretize age, and since federal law on age
discrimination uses age 40 as the definition of "older," let's use that
as an example:

```{r}
svc <- svcensus
age <- svc$age
age <- ifelse(age >= 40,'40+','under40')
age <- as.factor(age)
head(age)
svc$age <- age
```

Now let's make our "super factor," using a function from **qeML**:  

```{r}
AgeGend <- cartesianFactor('svc',c('age','gender')) 
svc$AgeGend <- AgeGend
head(svc)
```

We have only three education codes here, with 14 and 16 representing a
Master's degree and 16 a PhD, and 'zzzOther' denoting all others.  Since
this dataset consists of Silicon Valley programmers and engineers, the
vast majority of "others" have a Bachelor's degree.

We no longer need the original age and gender columns, so we'll delete
them and then try some analysis:

```{r}
svc$age <- NULL
svc$gender <- NULL
w <- dsldLinear(svc,'wageinc','AgeGend')
summary(w)
```

Ah, this is a more nuanced probe than the ones above in which we simply
[This is an example of *Simpson's Paradox*, in which an overall average
effect might be large than the terms that make up the average.]{.column-margin}
used age as a confounder.  The male-female differences at both the older
and younger age levels, about $9800 and $7800, are both substantial, but
smaller than the $13,100 overall figure we obtained earlier.  

Note too the impact of age within genders.  Older women made about $8200
more than younger women, but for men the figure was rather larger, about
$10,200.

On the other hand, this analysis is probably too coarse with respect to
age, as it does not reveal the negative impact of age well beyond 40.
It may be worth trying a finer discretization of age, say, 35-, 
35-55 and 55+.

## The Logistic model 

The *logistic* model is an example of a *generalized linear model*,
whose name stems from it having a linear component in the formula, as
will be seen below.

### General form of the model

Just as linear models are the most commonly used for numeric Y, in the
binary-Y case the go-to standard is the logistic model.  To introduce
it, let's first consider a very simple prediction problem, in which Y is
gender, say 1 for male, 0 for female, and X is simply income, using the
**svcensus** data.

::: {.callout-note}
### Probability is a special case of a mean
Note that mean Y is now the probability that Y = 1.  That's because the
average of a bunch of 1s and 0s is the proportion of 1s.  In the data
1,0,1,1 for instance, the mean is (1+0+1+1) / 4 = 3/4, and indeed 3/4 of
those numbers are 1s.
:::

Now consider a simple setting in which we are predicting Y = gender from
a single variable X = income (no C here).  Suppose that within each
gender, X has a normal (Gaussian) distribution, the familiar
"bell-shaped" curve, with the same standard deviation for each gender.
Then it turns out that one can show mathematically that

probability male =
$$
\frac
{1}{1 + e^{-(\beta_0 + \beta_1 income)}}
$$

That formula follows the form of the *logistic function*,
$f(t) = 1 / [1 + e^{-t}]$, which has the shape of an S-curve:

```{r}
curve(1/(1+exp(-x)),-4,4)
```

So at least the model retains a linear component, with much the same
interpretability.  For instance, if $\beta_2 > 0$, then the usual
monotonicity relation holds, i.e. the higher the income, the greater
the probability that the person is male.

The estimates b~i~ of the population values $\beta_i$ are obtained via a
method generalizing the least-squares method used in the linear case.

[There is a similar situation for the linear case.  If Y and the
predictor variables have a multivariate normal distribution, one can
show that mean Y as a function of the features is linear in the
features, etc.]{.column-margin} In the case of multiple predictor
variables, the logistic form can again be motivated by considering
within-group distributions:

Say we predict gender from age and wage income.  If the latter two
variables have a *bivariate normal* distribution (two-dimensional
histogram has a 3-D bell shape) with the same variance matrices within
each gender, it turns out that we again get a logistic form:

probability male =
$$
\frac
{1}{1 + e^{-(\beta_0 + \beta_1 age + \beta_2 income)}}
$$

Now, what about that assumption of the normal distributions and so on?
Just as many regression functions for numeric Y in practice are roughly
linear, in predicting binary Y the S-curve model is often roughly valid.
Moreover, the logistic model has two desirable properties for predicting
binary Y:

* Its value is between 0 and 1, appropriate for modeling a probability.

* As noted earlier, the expression $1 / [1 + e^{-t}]$ is increasing in
  t, which we wish to model when our predictors have monotonic relations
  with Y.

The point is that the logistic (popularly referred to as "logit") is
often a good model for binary-Y settings in general.

### We no longer have a no-interactions case

In our earlier linear model, predicting wage income from age and
gender, 

> mean W = $\beta$~0~ + $\beta$~1~ A + $\beta$~2~ M

recall that here $\beta$~2~ has the nice interpretation of there being a
uniform gender gap, independent of age.   Similar statements hold for
$\beta_1$; a 1-year increase in age, for instance, on average
is associated with a $\beta_1$ increase in mean income, identically for
either gender.

Geometrically, if we were to plot separate male and female regression
lines against age, the male and female r lines would be parallel.  That's
not possible in the logit case, as logit curves cannot be parallel:

```{r}
curve(1/(1+exp(-x)),-4,4)
curve(1/(1+exp(-2*x)),-4,4,add=TRUE)
```

The curves are no longer parallel; they even cross.  They typically
don't cross in applied settings, but they are always nonparallel.

The implication of this is the same as in the linear case.  We cannot
speak of "the" impact of S on Y, as it will be the same at different
levels of the X and C variables.  So there is no direct analog of the
no-interactions case for linear models, in which we could speak of
$\beta_2$ is being "the" gender pay gap, frre.

Some books motivate the logistic approach as the *log-odds* ratio,
meaning in this example that the logarithm of the ratio (probability
male) / (probability female) is linear in age and income.  

> log [probability male / probability female] =
> $\beta$~0~ + $\beta$~1~ A + $\beta$~2~ W

So here we do have a formulation in which the impact of wage is the same
[Note that a log-odds measure can take on any value betwen $-\infty$ and
$\infty$.]{.column-margin}
for any age level, albeit on a much less-interpretable scale.  

### Example: mortgage data

This dataset and its documentation are included in the **SortedEffects**
[We have done small modifications, to create R factors for some
columns.]{.column-margin} package.  The issue here is whether racism
played a role in mortgage denials in the Boston area.  As this is a
binary outcome, we might consider a logit model.   

Let's try a no-interactions model:

```{r}
data(mortgageSE)
z <- dsldLogit(mortgageSE,'deny','black',yesYVal='1') 
summary(z)
```

So, being Black resulted in an average increase in log-odds of about
0.6409.  A 95% confidence interval is 0.6409 $\pm$ 1.96 x 0.1836 =
(0.2810,1.0008).  By comparison, being self-employed, for instance, has
a similar estimated coefficient of about 0.6355.


## Machine Learning Approaches

We referred to the quantities $\beta_i$ above as "parameters."  The
linear and logistic models are thus called *parametric* models.  In each
case, the regression function is modeled as linear, "S curve-shaped" and
so on, that can be described with just a few quantities.

But these models make assumptions, such as assuming the regression
function is approximately linear.  It would be nice to conduct
analyses that don't rely on such assumptions.

*Machine learning* (ML) algorithms typically do not assume the regression
function has any particular form.  They are thus "safer," though on the
other hand they are less interpretable than, say, that $\beta_2$
quantity in our no-interactions linear model above.  Nor are standard
errors available for regression function values at given points.

ML is mainly concerned with prediction, while we here are interested in
estimation of effect sizes.  However, ML algorithms either directly or
indirectly do their prediction by estimating the regression function, as
we do here, so they can be quite useful.

### The k-Nearest Neighbor algorithm

We'll focus here on the k-Nearest Neighbor (k-NN) algorithm, as it is
the simplest to explain.  Say we are predicting people's weights,
knowing only their heights.  We have training data, on which we know
both the height and weight of each person.  Faced with a new case of a
person 70.2 inches tall but with unknown weight, we find the people in
our training set whose heights are close to 70.2 inches, and average
their weights.  That average is our predicted weight for the new person,
as well as our estimate for the value of the regression function at
height = 70.2.

How is "close to" defined?  Should we look at the closest 5 people, the
closest 50, or what?  That number is k, the number of nearest neighbors.
[The number k here is called *hyperparameter* of the
algorithms.]{.column-margin} It's chosen by the analyst, just like the
analyst chooses the number of bins in plotting a histogram.  Typically k
is chosen by trying several different values of k, then using the one
that best predicts in the holdout set.  Once we've settled on k, we
might use the full dataset, no holdout.

```{R}
set.seed(9999)
w <- dsldML('svcensus',quote(wageinc),'gender',
   qeMLftnName='qeKNN',opts=list(k=50))  
print(w)  
```

The first three arguments are as usual.  The fourth states that the ML
[The use of R's **quote** function here is due to a nuance
in the way R handles nested quotes.]{.column-margin}
algorithm we wish to use is kNN; the **qeML** package also includes
random forests, boosting and neural networks.  The **opts** argument
states which non-default values we wish to use for arguments to the ML
function; the default k actually is 25.

The comparison cases are by default a random sample of 5 rows of the
training set.  In order to obtain consistent results each time this book
is processed by Quarto, we've called R's **set.seed** function.

It's important to keep in mind that the output here is not simply for 5
persons; each row represents a subpopulation.  Thus for instance in that
first row of output, we estimate among *all* Silicon Valley techies of
age 37.4, with a Bachelor's degree or less, in occupation 101, and
having worked 52 weeks, women's average wage is 48,914 and for men it's
61,156.

As before, we see a substantial gender wage gap, but now we can feel
more confident about it, since this analysis is unencumbered by
assumptions on the form of the regression function.  On the other hand,
there is no easy way to obtain standard errors. 

### The random forests algorithm

In the above example, in which we are predicting wage income, a random
forests (RF) analysis would generate a large number of *decision trees*.
As explained below, each tree would give us a predicted income, and our
final prediction would simply the average of all those predicted values.

So, what is a decision tree?  It's just a flow chart.  Consider this
example involving vertebral disease.  Y is categorical, with values NO
(normal), SL and DH.  To predict a new case, we look at the X variables
(which have the nondescript names V1 through V6) one at a time, and
branch through the tree accordingly.

For example, if the new case to be predicted has V6 of at least 16, we 
branch right, and immediately decide to predict that this patient's
disease status is SL.  If on the other hand, V6 is less than 16, we look
at V4.  If it's under 28, we immediately predict status DH, and so on.

![A decision tree](RpartVert.png){width=100%}

The order in which variables are considered in building a tree is random
for each tree.  So, while V6 is considered first in the tree displayed
here, in some other tree it may be, say, V2.  The split points, e.g. 16
in the root node in the picture, are through formulas that depend on the
data in complex (though not terribly deep) ways.

There is no "universally best" predictive algorithm, with performance of
a given algorithm being dependent on the given dataset.  So, instead of
doing a kNN analysis, we could try RF.

```{R}
set.seed(9999)
w <- dsldML('svcensus',quote(wageinc),'gender',
   qeMLftnName='qeRF')  
print(w)  
```

The results here are rather different from what we obtained with kNN.   

### Other ML methods

The most famous class of ML these days is of course *neural networks*,
especially the refined version *deep learning*.  These have been found
highly useful in image recognition and language processing.

The ML people refer to the types of data we see in this book, where each
row represents, say, one person, as *tabular* data.  Their go-to
method for such data is *gradient boosting* (GB), more specifically XGBoost,
accessible in **qeML** via **qeXGBoost**.  GB is again a tree-based
method, but it works by starting with a primitive tree and repeatedly
refining it.


## Example:  The Law School Admissions dataset {#law-school}

This is the dataset **law.school.admissions**, included in **dsld** via
[Possible the 'age' variable is birth year.  The data are from 1991, so
an 'age' value of 69 would mean born in 1969, now age
22.]{.column-margin} **fairml**.  It's quite well-known in the ML world,
but its full provenance is unclear.  For instance, the age variable
skews far to middle-aged and older, seemingly not consistent with the
data's description on Kaggle.  

Thus it should be kept in mind that this is just
an illustration of methodology, and firm conclusions about the legal
education process should not be drawn.  The data concern students who
were admitted to law school, so in spite of the title, it's not about
the admissions process itself.

The main variables of interest here are:

* **decile1**, **decile2**:  Student's standing after Year 1 and Year 3
of law school.

* **fam_inc**: Apparently the income level of the family in which the
law students was raised in.

* **lsat**: Score on the LSAT, a major law school admissions test.

* **ugpa**:  Undergraduate GPA.

* **gender**:  Gender.

* **race1**:  Primary racial group.

* **cluster**:  Level of prestige of the law school.

* **bar**:  After law school, did this person pass the bar examination?

Here's what the data looks like:

```{r}
data(law.school.admissions)
head(law.school.admissions)
names(law.school.admissions)
lsa <- law.school.admissions
```

### Is the LSAT fair?

There has been concern that the LSAT and other similar tests are biased
against Black and Latino students, and might otherwise have racial
issues.  Let's investigate, using **dsld**.


```{r}
z <- dsldLinear(lsa,'lsat','race1')
summary(z)
```

[Note the retrospective view--using later events to "predict" the past.
This is valid, but may seem odd at first.]{.column-margin} 
There are very concerning racial differences here.  Two very similar
people&mdash;who attended the same quality law school, with the same
undergraduate grades, the same law school grades, even having the same
bar passage status&mdash;will have LSAT scores differing on average by
almost 6 points if one person is Black and the other is white.  

Again, one must be very cautious in drawing conclusions as to causes,
not only because of the questionable quality of the dataset but also
because *hidden confounders* may be at work here.  For instance, though
we have data on undergraduate GPA, we don't know the quality of the
undergraduate institution.  But the results here raise serious concerns.


### Is the bar exam fair?

And what about passage of the bar exam?  Does race play a role?

```{r}
comparisonPts <- lsa[c(2,22,222,2222),-c(8,11)]
w <- dsldLogit(lsa,'bar','race1',comparisonPts,yesYVal= 'TRUE')
summary(w)   
```

That's a lot of output.  Let's take a closer look.

```{r}
u <- summary(w)
u$'Sensitive Factor Level Comparisons'[,c(1,3,4)] 
```

Black and white students having the same traits appear to have passed
the bar exam at the same rates.   

We might also do a little check of the appropriateness of the logistic
model for this data.  One rough approach might be to use 
**dsldConditDisparity()***, as we did in the linear case:

```{r}
# Y needs to be numeric, in this case 0,1
lsab <- lsa
lsab$bar <- ifelse(lsab$bar=='TRUE',1,0)
dsldConditDisparity(lsab,'bar','race1','lsat','lsab$age > 0')
```

Looks pretty good.  But we can go further, using k-NN analysis, as it
makes no assumptions about the form of the regression function:


```{R}
w <- dsldML('lsa',quote(bar),'race1',qeMLftnName='qeKNN',
   opts=list(k=50,yesYVal='TRUE'))  
print(w)  
```

The pattern here seems to be a bit uneven, but again, Black and white
test takers seem to be on par with each other.

## Case study: problems with significance testing

In 2016, the American Statistical Association released its first-ever
position paper, to warn of the problems of significance testing and
"p-values."  Though the issues had been well known for years, it was
"significant" that the ASA finally took a stand.  Let's use the law
school data to illustrate.

There is concern that the LSAT and other similar tests may be
heavily influenced by family income, thus unfair, especially to 
underrepresented minorities.  To investigate this, let's
consider the b~i~, the estimated coefficients in our linear model
for the LSAT above.

In particular, look at the coefficient for family income, 0.3009.  The
p-value is essentially 0, which in an academic research journal would
classically be heralded with much fanfare, termed "very highly
significant," with a 3-star insignia.  Indeed, the latter is seen in the
output above.  (This comes from R, not **dlsd**.) But actually, the
impact of family income is not very large.  Here's why:

Family income in this dataset is measured by quintiles.  So
[Mathematically, testing for a 0 effect is equivalent to checking
whether the CI contains 0.  But this is missing the point of the CI,
which is to (a) give us an idea of the effect *size*, and (b) to
indicate how accurate our estimate is of that size.  Aspect (a) is given
by the location of the center of the interval, while (b) is seen from
the CI's radius.]{.column-margin} this estimated coefficient says that,
for example, if we compare people who grew up in the bottom 20% of
income with those who were raised in the next 20%, the mean LSAT score
rises by only about 1/3 of 1 point--on a test where scores are typically
in the 20s, 30s and 40s.  The 95% confidence interval (CI),
(0.2304,0.3714), again indicates that the effect size here is very
small.

So family income is not an important factor after all,
and the significance test was highly misleading.

Some who read this may object, "Sure, there sometimes may be a
difference between statistical significance and practical significance.
But I just want to check whether my model fits the data."  Actually,
it's the same problem.

For instance, suppose we are considering adding an interaction term
between race and undergraduate GPA to our above model.  For simplicity,
let's use R's linear model function, **lm()**, directly:

```{r}
w <- lm(lsat ~ .,lsa)  # predict lsat from all other variables
w1 <- lm(lsat ~ .+race1:ugpa,lsa)  # add interaction 
summary(w1)
typx <- lsa[1,-5]  # set up an example case
predict(w,typx)  # estimated regression function value
predict(w1,typx)  # estimated regression function value
```

Indeed, the Black and white interaction terms are "very highly
significant."  But that does mean we should use the more complex model?

Recall that many ML algorithms, including standard linear regression,
use the estimated regression function value as its predicted value.  So
we see here that adding in term interaction term changed the estimated
value of the regession function by only about 0.02 out of a 40.23
baseline.  So, we may well prefer the simpler, no-interaction model.

Again, we must not take small p-values literally.

::: {.callout-note}
### The basic problem with significance testing

The central issue in the above examples, and essentially in any other
testing situation, is that *the test is not answering the question of
interest to us.*  

We wish to know whether family income plays a substantial role in the
LSAT, not whether they is any relation at all, no matter how meaningless
Similarly, we wish to know whether the interaction between race and GPA
is substantial enough to include it in our model, not whether there is
any interaction at all, no matter how tiny.

The question at hand in research studies is rarely, if ever, whether a
quantity is 0.000... to infinitely many decimal places.  Indeed, in most
cases our measuring instrument is not this accurate in the first place;
there will always be systemic bias or missingness, unobserved variables
and so on.

Thus in almost all cases, significance tests don't address the issue of
interest, which is whether some population quantity is substantial
enough to be considered important.  Analysts should not be misled by
words like "significant." [Modern statistical
practice](https://tinyurl.com/2s7x6h2v) places reduced value, or in the
view of many, no value at all, on significance testing.

:::

## Example: COMPAS dataset {#sec-compas}

Recall that COMPAS is a tool developed to help judges decide on
sentences in criminal trials, which it does by predicting whether the
defendant is likely to recidivate.  Northpointe, the creator of the
tool, disagrees.

Let's investigate this, by predicting Y = the COMPAS score, with S =
race and C = the remaining variables.  Does S have much effect on Y?

```{r}
data(compas1)
w <- dsldLinear(compas1,'decile_score','race')
summary(w)

```

A 95% confidence interval for the mean difference between Black and
Caucasian risk scores, holding age, juvenile felony count and so on
constant, is 0.4680 $\pm$ 1.96 x 0.0623 = (0.3459,0.5902).  Since the
risk of recidivism is given in deciles, the value is 1,2,...,10, we see
that the tool does indeed appear to be biased against Black defendants.
Though the effect is rather small, around 0.6 of a decile point or less,  
and as usual, one must keep in mind the possibility of unseen confounders,
it is still a matter of serious concern.


## Deciding on a set of confounders C  

One may have specific confounders in mind for a particular analysis, but
it is often unclear as to which to use, or for that matter, why not use
them all?  This section addresses those issues.

We will address that second question first: Why not use all our
variables, other than Y and S, as confounders?  We will see that if we
have a large number of variables, there is good reason select only a
subset of potential confounders.  

How might we do so?  We will first recommend some graphical and tabular
methods aimed at preliminary exploration toward this end, and then
present a **dsld** function that users may find useful for more
systematic selection of confounders.

### The problem 

The German credit dataset, included in the **fairml** package and thus
with **dsld**, consists of a total of 21 variables:

```{r}
data(german.credit)
names(german.credit)  
```

Excluding Y = credit risk and S = gender, that gives us 19 variables to
choose among for our confounders.  Which ones should we use?

Technically, almost any variable is a confounder.  The impact may quite
minuscule, but through a long chain of relations among many variables,
there will usually be at least some connection, though possibly very
faint.

Well, then, why not use them all?  That is what we've done in our
earlier examples.  But there are several issues to consider not using
the full set of variable,s i.e. every variable other than Y and S:

* It may result in overfitting, resulting in large standard errors.

* It is unwieldy, difficult to interpret.  Many treatments of these
  issues speak of a desire for a "parsimonious" model.

* There is a concern regarding duplication.  It may be that, say, some
  pair of confounders suffices, and adding further confounders does
  nothing to further clarify discrimination effects.

Concerning this last point:  We do not merely have 19
choices for confounders; we must choose from the set of all possible
groups of confounders: singletons, pairs, triplets and so on.  There are
$2^{19}$ possibilities here, about half a million.

Our specific purpose here is to find a reasonable set of  confounders.
In essence, that means find variables C such that C is substantially
correlated with both Y and S.  One way to approach that is that do
separate predictions of Y and S, and see which features turn out to
predict both well.  

### Exploratory example:  engineering wages

One useful tool here is the function **dsldFrequencyByS()**, which aims
to analyze categorical (not numeric) columns by printing out a list of
frequencies. Since education level in the **svcensus* data is
categorical, we can call the function as follows:

```{r}
library(dsld)
data(svcensus)
dsldFrequencyByS(svcensus, "educ", "gender")
```

The similar frequency values between men and women suggests
not using education as a confounder.

On the other hand, we *do* see more interesting results if we look at 
*occupation* instead of *education*:

```{r}
library(dsld)
data(svcensus)
dsldFrequencyByS(svcensus, "occ", "gender")
```

Notice the difference between the proportion of males in occupation 106
[One might consider conducting formal statistical inference in this
comparison, but the relevance to the issue of our ultimate statistical
analysis of Y is unclear.  At any rate, here we are merely doing an
exploratory analysis anyway.]{.column-margin} vs females in that same
occupation--a difference of approximately 11%.  The difference in
frequencies here is much greater than with the 'educ' example, which
suggests using occupation as a confounder.  Since different occupations
correlate with different wages, it is possible that this difference in
occupations that men have compared with what women have could be
affecting the relationship between gender and wage. 

### Exploratory example: Law school admissions data

Suppose we are investigating the relationship between LSAT exams scores
and race, and wish to graphically explore the effects of the C, family
income, on the relationship between Y = LSAT score, and S = race.

We have to look at both the relationship between race and LSAT scores
and the relationship between race and family income. We may also want to
look at the effects of GPA as well. A quick way to view multiple
variables (and their relation to the sensitive variable) at once is to
plot them in a 3D scatter plot:

<!-- Placement pending -->
```{r, eval=FALSE}
data(law.school.admissions)
dsldScatterPlot3D(law.school.admissions, c('lsat','fam_inc','ugpa'), 'race1',
                  pointSize = 4)
```

![](graphTab/lawscatterplot3d2.png){width="90%"}

Looking at the fam_inc axis, the lowest quintile of family income is
[This is a **plotly** interactive graph.  To fully understand this
function, the reader should execute the function outside of Quarto,
trying features such as move, rotate, show annotation etc.]{.column-margin}
mostly populated by Black and Latino students, while the upper two
levels are almost entirely made up of white students. Interestingly on
the lsat axis, most of those with a lower score happen to be non-white,
across all income levels. On the other hand, the ugpa axis has a similar
trend to that of the lsat axis, albeit to a much weaker extent.

Once again, this analysis is merely exploratory, but the graph lends
some credence to claims that family income may confound the relationship
between race and LSAT score.  Note however, that these graphs do not do much to 
answer the question of whether, in this case, that relationship is substantial.
The formal analysis we did earlier indicates that it is not.

Returning to the question of whether race have a substantial impact on
results, we can look at the density plot of LSAT scores against
different races.

```{r, eval=FALSE}
# requires 'webshot' package
data(law.school.admissions)
dsldDensityByS(law.school.admissions, cName = "lsat", sName = "race1")
```

![](graphTab/lawdensitylsat.png){width="120%"}

Each curve represents the distribution of LSAT scores for each race.
From this, its easy to see the possibility of racial bias in the LSAT.
But again, this bias may be confounded.

Recall that the scatter plot also suggested some trend between family
income and race. We can investigate this possible relationship by
generating another density plot:

```{r, eval=FALSE}
data(law.school.admissions)
dsldDensityByS(law.school.admissions, cName = "fam_inc", sName = "race1")
```

![](graphTab/lawdensityfam.png){width="120%"}

White students have larger peaks at income levels 3, 4, and 5,
indicating that [Recall that the **fam_inc** variable is measured in
terms of quintiles, 1 through 5.  This is the reason behind the
seemingly-odd alignment of, say, the Black and Latino
curves.]{.column-margin} a larger proportion of white students are in
the higher income brackets than non-white students. Conversely, a larger
proportion of black and Latino students occupy the lower income bracket
levels.

[If we wanted to investigate other potential confounders, we can call 
**dsldConfounders()** with 'race1' as our sensitive variable.]{.column-margin}

This suggests that family income could be confounding the potential 
relationship between race and LSAT scores.  But, as noted, our earlier analysis indicated that the degree of this relationship is very small.

::: {.callout-note}
### Almost anything is a Confounder

Once one accounts for links of two variables, then links of links and so
on, virtually everything in practice is a confounder to *some* degree,
so it is up to the analyst to decide when a feature is confounding
enough to be considered for inclusion in a model.

:::

### Variable selection methodology

The above graphical and tabular approaches may suffice to determine
one's confounders in some applications.  We now present more formal,
systematic methods.

Many, many approaches to this *variable selection* or *feature
selection* [A nice overview of such methodology, and implementations in
R and Python, are given in [Parr *et
al*](https://explained.ai/rf-importance).]{.column-margin} problem have
been proposed over the years.  The **qeML** package includes five of
them (see the documentation), and there are myriad others.

One widely-used method is *permutation*, which we will use here.  To
gauge the importance of a variable V in the context of using some ML
algorithm, we do two runs through the data, first with the original dataset,
and then with the V column changed through a random shuffling of the
values in that column.  In that second run, our predictive accuracy
should be compromised, and the importance measure is taken to be the
proportional increase in error rate.  The larger the increase, the
more important we deem the variable.

### The dsldCHunting() function

This function serves as an aid to selecting confounders, using the 
following approach.  Denote our dataset by **d**.

* Two columnwise subsets of **d** are formed, **dNoS** and **doNoY**,
  deleting the respective column.

* We fit a random forests model to **dNoS**, predicting Y, resulting in
  variable importance measure **vY**.

* We fit a random forests model to **dNoY**, predicting S, resulting in
  variable importance measure **vS**.

* We sort each of the two 'v' measures, from largest to smallest.

* For each i, we take the intersection of the top-i 'v' measures in each set.

By taking the intersection, we are focusing on variables that are
correlated with both Y and S, as desired.

### Example:  Boston mortgage data

Here is an example, using the mortgage data.

```{r}
data(mortgageSE)
dsldCHunting(mortgageSE,'deny','black')
```

The importance measures are printed out (the ranking is what matters),
followed by the "top-i" sets in common.

As usual, there are no magic formulas to use here.  The analyst is given
a choice.  One might use just two confounders, 'p_irat' and 'loan_val';
for a set of three, the software suggests 'p_irat', 'loan_val' and 'hse_inc',
and so on.

### Example:  Employer bias test

In the paper "Are Emily and Greg More Employable Than Lakisha and Jamal?
A Field Experiment on Labor Market Discrimination" (*Am. Econ. Rev.,
Sept. 2004), researcher "test" employers by sending CVs with
"white-sounding" and "Black-sounding" names, checking for bias.  The
resulting data is **lak* in **dsld**.

```{r}
data(lak)
dim(lak)
names(lak)
```

We'll take Y to be **call**, which records whether the "applicant"
received a call back in response to submitting the CV.  S will be race,
which now leaves us with 63 - 2 = 61 potential confounders!

Actually, it's even worse.  Many rows of the dataset contain missing
values (coded NA in R).  The version here uses only intact rows, of
which there are 447.  (The original data had 4870 rows.)  A rough rule
of thumb commonly cited in statistics is that in regression estimation,
the number of features should be less than the square of the number of
data points; by this or almost any other measure, 61 predictors is well
beyond the capacity of 4447 data points.  And, as noted, that number of
variables is unwieldy.  Let's see what we can do to reduce that.

```{r}
dsldCHunting(lak,'call','race',25)
```

We are seeking variables that are correlated both with Y and S, yet
there appear to be many that correlate with just one of these.  But
still, in the end we see a few that meet our goals.


{{< pagebreak >}}

# Part II:  Discovering/Mitigating Bias  in Machine Learning   

In modern applications of machine learning as a predictive modeling tool, it 
would be irresponsible to produce a model that biases one sensitive group over 
another. As such, it becomes imperative to find methods of uncovering
and reducing any such biases. 

## Goals  

There are two main aspects of fair machine learning:

* **Measuring unfairness**: A number of measures have been proposed.

* **Reducing unfairness**: For a given ML algorithm, how can we
  ameliorate its unfairness, yet still maintain an acceptable utility
  (predictive power) level?

In our earlier COMPAS example:  Is the risk assessment tool biased
against African-Americans?  And if so, how can we reduce that bias while
still maintaining good predictive ability?

## Comparison to Part I

First, recall our notation: [Actually, think of X as, at first,
consisting of all variables other than Y and S, but then selecting some
of X as O, with X then being all variables but Y, S and O.]{.column-margin}

* Y; our outcome variable, to be predicted

* S: our sensitive variable

* O: our proxy variables

* X: other variables to be used to predict Y

We wish to predict Y from X and O, omitting S, but with concern that we
may be indirectly using S via O.  Contrast this from our material in
Part I:

* In Part I, we fit models for predicting Y, but with the goal of using
such models to assess the effect of S on Y; we were not interested in
actually predicting Y.  We made central use of S, but wished to find
variables C that were correlated with both Y and S, so as to avoid
distorting our look at the impact of S on Y.  Any X variable unrelated
to Y was not of interest.

* Here in Part II, prediction of Y is our central goal.  We will omit S,
relying our prediction fully on X and partly on O.  We find the
variables O by investigating which variables are related to S; the
stronger the relation of an O variable to S, the less weight we will put
on that variable in predicting Y.

Below we will describe some common measures of unfairness.  But first,
how do we choose the O variables?

### Deciding proxies  

So, how does one choose those proxies O?

As with choosing confounders in Part I, analyst may simply choose the
proxies by using his/her domain expertise.  But a more formal approach
may involve correlation.  Here's an example, using the COMPAS data:

```{r}
data(compas1) 
cmp <- compas1[,-3]  # omit decile, as we are developing our own tool
dsldOHunting(cmp,'two_year_recid','race')
```

The function **dsldOHunting** calculates the correlations between S and
[It should be kept in mind that, as with any statistic, all utility and
fairness measures are subject to sampling variation.]{.column-margin}
all possible candidate O variables.  The output here  suggests possibly
using, say, the **age** and **priors_count** variables as proxies.

The function does not use the classic *Pearson product moment*
correlation here, opting instead for *Kendall's tau* correlation.  Both
are widely-used, and both take on values in [-1,], but while Pearson is
geared toward continuous numeric variables, Kendall is also usable for
[Tau is defined in terms of
*concordances* and *discordances*.  Say we look at height and weight,
and compare two people. If one of the people is both taller and heavier
than the other, that is a concordance.  If on the other hand, one is
shorter but heavier than the other, that is a discordance.  We consider
all possible pairs of people in our dataset, then sets tau to the
difference in concordance and discordance counts, divided by the number
of pairs.]{.column-margin} 
binary or ordinal integer-valued variables.  

## Measuring utility

Utility in ML classification algorithms in general,
and both utility and fairness in the fair ML classification realm,
often (but far from always) make use of quantitaties like False Positive
Rate (FPR).  So, let's start by defining these rates.

::: {.callout-note}
### The famous rates FPR etc.

These are conditional probabilities, but it's important not to confuse
the event with the condition.

Consider binary Y classification problems, where we label Y is either
*positive* (e.g. patient has the disease) or *negative* (e.g. patient does
not have the disease).

After we fit our ML tool to predict Y, we use it for prediction.
Consider a long period of time in which we do such predictions.  During 
that time, define the following counts:

* FP: Count of the number of times we predict Y to be positive and
actually it's negative.

* FN: Count of the number of times we predict Y to be negative and
actually it's positive.

* TP: Count of the number of times we predict Y to be positive and
actually it's positive.

* TN: Count of the number of times we predict Y to be negative and
actually it's negative.

Then some key rates are:
[It is standard to guess Y = 1 if the probability of that event is at
least 0.5.  But other thresholds can be used, with TPR and FPR varying
as we vary the threshold. The *ROC curve* is the resulting graph of 
TPR vs. FPR; see **qeROC** in the **qeML** package.]{.column-margin}

* FPR: FP / (FP + TN) = P(guess Y positive | Y actually is negative)

* TPR: TP / (TP + FN) = P(guess Y positive | Y actually is positive)

* FNR: FN / (TP + FN) = P(guess Y negative | Y actually is positive)

* TNR: TN / (FP + TN) = P(guess Y negative | Y actuually is negative)

So for instance, FPR is the proportion of time we guess positive, *among
those times* in which Y is actually negative.  Two other common terms:

* *recall*:  same as TPR

* *sensitivity*:  same as TPR

* *precision*: P(Y is actually positive | we guess Y is positive)=
(TP + FN) / (TP + FP)

:::

A simple but quite common measure of utility in binary classification
problems is the overall misclassification probability [The *F1-score*
has recently been especially popular in the ML research world.  It is
defined as the harmonic mean of precision and recall, and is thought to
be especially useful in applications in which one class or the other is
very rare.]{.column-margin} of misclassification; **qeML** predictive
functions report this in the **testAcc** component of the functions'
return object.  There are many, many other measures.

For whatever reason, ML research has tended to focus on that binary Y
case, and there are no analogous acronyms for numeric Y.  Accuracy of
the latter is handled simply as Mean Squared Prediction Error (MSPE),
the average squared difference between predicted and actual Y value, or
Mean Absolute Prediction Error (MAPE).  


## Measuring unfairness

Many unfairness criteria have been proposed.  We preseent a few of them
in this section.  [See the [Xiang and
Raji](https://arxiv.org/pdf/1912.00761.pdf) for an excellent analysis of
the legal implications of various fairness measures.]{.column-margin} It
should be kept in mind that, just as there is no single ML algorithm
that predicts the best in all applications, one's choice of fairness
measure also will depend on the given application.

### S-Correlation

A direct way to measure where Y and S are still related in spite of
physically omitting the latter is to compute the correlation between
predicted Y, to be denoted $\hat{Y}$ and S.  As noted earlier, we use
Kendall's Tau correlation here.

[Here and below, to keep things simple, we will not use a holdout
set.]{.column-margin} For instance, let's consider our mortgage example,
say with k-Nearest Neighbors as our ML prediction tool:

```{r}
z <- qeKNN(cmp,'two_year_recid',holdout=NULL,yesYVal='Yes') 
# look at the fitted model's probability of recidivism,
# i.e. regression estimates
probs <- z$regests
# the variable 'race' is an R factor, need a numeric
black <- ifelse(cmp$race=='African-American',1,0)
cor(black,probs,method='kendall')
```

That's a pretty substantial correlation, definitely a cause for concern
that our ML analysis here is unfair.  Of course, it's not the algorithm
itself's fault, but we must find a way to mitigate the problem.

[Prediction of income might be of interest in, say, a marketing context.
Actually, the field of marketing has been the subject of much concern in
fair ML; e.g. see
[FWA](https://thefwa.com/cases/a-marketers-guide-to-machine-learning-fairness).]{.column-margin}
An advantage of the S-Correlation measure is that it can also be used in
non-classification problems, say predicting wage income, and take age as
our sensitive variable S:

```{r}
z <- qeKNN(svcensus,'wageinc',holdout=NULL)
cor(z$regests,svcensus$age,method='kendall') 
```

So, again, our ML tool seems to be biased, this time in terms of age.

### Demographic Parity

The criterion for demographic parity is that the same proportion of each
sub-group within the sensitive feature is classified at equal rates for
each of the possible outcomes 

For example, let's consider the COMPAS dataset again. Demographic Parity
would require 

P(predict recidivate | Black) = P(predict recidivate | white),

i.e. that the quanitty

(TP+FP) / (TP+FP+TN+FN)

has the same value for each race.

Below is an example using the COMPAS data:

```{r}
z <- qeKNN(cmp,'two_year_recid',holdout=NULL,yesYVal='Yes')
# determine which rows were for Black applicants, which not
BlackRows <- which(cmp$race == 'African-American')
NonblackRows <- setdiff(1:nrow(cmp),BlackRows)  # all the others
# regests, the output of qeKNN, is the vector of fitted probabilities
# (recall that for the Y = 0,1 case, the mean reduces to the probability
# of a 1)
BlackProbs <- z$regests[BlackRows]
NonblackProbs <- z$regests[NonblackRows]
# if a probability is > 0.5, we will guess Y = 1, otherwise guess Y = 0;
# conveniently, that's same as rounding to the nearest integer
BlackYhats <- round(BlackProbs)
NonblackYhats <- round(NonblackProbs)
# again, recall that the mean of a bunch of 0s and 1s is the proportion
# of 1s, i.e. the probability of a 1
mean(BlackYhats)
mean(NonblackYhats)
```

That's quite a difference!  Overall, our ML model predicts about 51% of
Black defendants to recidivate,j versus than 27% for non-Blacks.

However, such a criterion is generally considered too coarse, since it doesn't
account for possible differences in qualifications between the two groups.
In other words, one must take confounders into account, as we did in Part I.

### Equalized Odds

This criterion takes a retrospective view, asking in the case of COMPAS:

> Among those who recidivate, what proportion of them had been predicted
> to do so?  And, does that proportion vary by race?

If the answer to that second question is No, we say our prediction tool
satisfies the Equalized Odds criterion.

So, Equalized Odds requires the quantity

TP / (TP+FN)

to be the same for each sensitive group.

### The **fairness** package

This package calculates and graphically displays a wide variety of
fairness criteria.  For instance, let's use it to evaluate the Equalized
Odds criterion in the above COMPAS example.

```{r}
library(fairness)
equal_odds(cmp,'two_year_recid','race',probs=z$regests)
```
Taking African-Americans as the base, we see that the Equalized Odds
criterion was not met, even approximately.  Nor was Demographic Parity.

## Remedies

Having established that ML prediction models can be biased against
certain sensitive groups, what remedies are available?  The **dsld**
package includes a few of these, presented in this section.  Of course,
all of them recognize a basic principle:

::: {.callout-note}
### The Fairness-Utility Tradefoff

Of course, nothing comes for free: the inherent tradeoff of increasing
fairness is reduced utility (reduced predictive power over the dataset).
Thus, a means of balancing this tradeoff between fairness and utility
becomes essential in any future implementations of machine learning.

:::

Any algorithm for ameliorating unfairness will thus include one or more
parameters that one can use to achieve a desired level of compromise between
fairness and utility.  The parameters essentially allow us to "dial" the
weight that our proxies will play in predicting Y; lighter weight means
more fairness but poorer utility, and vice versa.

Our context will be:

* Due to legal requirements or simply a desire for fairness, we will
  omit S from all analyses, other than for fairness assessment of our
  derived prediction tool.

* But we are concerned about the impact of proxies, and have chosen a
  set of variables O to play this role.

* We have chosen fairness and utility measures with which we will choose
  a desired point in the Fairness-Utility Tradefoff.

Many, many fair ML algorithms have been proposed, most of which are
technically complex.  The ones we present here have been chosen (a) for
their technical simplicity and (b) availability as R packages.
We will begin with the simplest algorithms (which have been developed by
one of the authors of this book).

Let's take as a running example the COMPAS data, predicting recidivism,
with age and prior convictions count as proxies.  To illustrate
prediction, we will predict the first case in the dataset:

```{r}
newx <- cmp[1,-(7:8)]  # just X and O, not Y and S
```

## **dsldQeFairRF**

This function fits an RF model, but with deweighting of the proxies.

```{r}
z <- dsldQeFairRF(cmp,'two_year_recid','race',
   list(age=0.2,priors_count=0.1),yesYVal='Yes')
z$corrs
predict(z,newx)
```

Recall that in RFs, each tree will use a different random ordering of
the X and O variables.  At any given node in a tree, a variable is
chosen at random to set up a possible split point.  But here we are
specifying that the age and priors count variables be used only 20% and
10% as often as other variables, in order to reduce their impact.

Yet the S-Correlation valuea are still substantial.  We need to give
the O variables even smaller weights or possibly include additional
variables in our O set.

## **dsldQeFairKNN**

Remember, the common theme here is reducing the role played in
prediction by the proxies O.  How might this be done with k-NN?

An easy answer is to weight the distance metric.  Ordinarily, if we move
3.2 meters to the right and then 1.1 meters forward, the distance
between our new point and our original one is

$\sqrt{3.2^2+1.1^2} = 3.38$

That puts equal weight in the left-right direction and the forward-back
direction, which makes sense for geometric distance.  But in data
prediction, we can use different weights.  In prediction wage income in
the Census data, say, we can place large weight on age and occupation,
and less weight on education.  The above call would change to:

```{r,eval=FALSE}
z <- dsldQeFairKNN(cmp,'two_year_recid','race',
   list(age=0.2,priors_count=0.1),yesYVal='Yes')
z$corrs
predict(z,newx)
```

## Remedies based on "shrunken" linear models

One of the most striking advances in modern statistics was the discovery
that classical estimators tend to be  "too big," and that one may
improve accuracy by "shrinking" them.  Here is the intuition:

Consider the example in @sec-compas.  The vector of estimated regression
coefficients was 

(12.46,-0.94,0.39,0.16,...)

We might (crudely) shrink this vector by multiplying by a factor of,
say, 0.5, yielding

(6.23,-0.47,0.20,0.08,...)

The actual shrinkage mechanisms used in the regression context are much
more complex than simply multiplying every component of the vector by
the same constant, but this is the basic principle.

Why might this odd action be helpful?  Shrinking introduces a bias, but
reduces variance (roughly speaking, smaller variables vary less).  If
outliers (extreme values, not errors) are common in the data, these
result in large estimator variance, possibly so much that shrinkage's
reduction in variance overwhelms our increase in bias.

Some readers may have heard of *ridge regression* and the *LASSO*, both
of which impose shrinkage.  For our fair ML context, though we wish to
perform our shrinkage focusing on *only some* of the estimated regression
coefficients, specifically those corresponding to our proxies.  To be
sure, it should be noted that non-proxy coefficients are affected too,
but the main effects will be on the proxies.

Note that these estimators also have the potential ancillary benefit
obtained from shrinkage in general, i.e. better utility.

### The **dsldFgrrm** function

This is a wrapper to a corresponding function in the **fairml** package,
based on [a
paper](https://link.springer.com/content/pdf/10.1007/s11222-022-10143-w.pdf)
by the authors of the package.

Let's see what it does with the COMPAS data:

```{r,eval=FALSE}
dsldFgrrm(cmp,'two_year_recid','race',unfairness=0.1,family='binomial')
dsldFgrrm(cmp,'two_year_recid','race',unfairness=0.01,family='binomial')
```

The **unfairness** argument is a number in (0,1].  The smaller the
value, the fairer the fit.  The first value, 0.1, gave an estimate
coefficient for Caucasian of -0.73, while using 0.01 reduced this to
-0.15.  (Note that the base for the dummy variables is apparently
African-American.]  But both values are small relative some of the other
coefficients.


{{< pagebreak >}}

# Author Bios

**Norman Matloff** is Professor Emeritus of Computer Science, and was
[![NM](NMHighDef.jpg){width=50%}]{.column-margin}
formerly a Professor of Statistics at that university.  His
participation in this project is informed in part by his experience
serving in a number of litigation cases involving discrimination.  His
work has been recognized in various forms, including the
university-wide Distinguished Teaching Award, Outstanding Adviser Award,
and Distinguished Public Service Award.  His book, *Statistical
Regression and Classification: from Linear Models to Machine Learning*
was the 2017 recipient of the Ziegal Award, given by the statistics
journal *Technometrics*.

**Taha Abdullah** is studying for a B.S. in Computer Science at
[![TA](Taha.jpg){width=35%}]{.column-margin}
University of California, Davis. He has a keen interest in pursuing a
career in Software Engineering.

**Aditya Mittal** is pursuing a B.S. in Statistics with a minor in Computer
[![AM](adityamittal.png){width=35%}]{.column-margin}
Science at University of California, Davis. During the summer, he is
employed as a Business Analyst Intern at Cisco and will be returning in
summer 2024 for another round. He is interested in pursuing a career in
machine learning/software engineering. Fun fact: He moved to the U.S.
in 2014 from Mumbai, India.

{{< pagebreak >}}

# Appendices {.appendix}

## Appendix A: Standard Errors$\textemdash$Statistical Inference in a Nutshell

Say I wish to find the mean weight $\mu$ of all students at the University of
California, Davis.  It would be infeasible to measure them all, so I
take a random sample of 100 students.  The mean weight of those 100
students, denoted $\bar{X}$, is an estimate of $\mu$.

I know that $\bar{X}$ will have some unknowm amount of error as an estimate of
$\mu$.  One measure of this is its *standard error* (SE).  What is this?

Remember, we took a random sample of students.  Different samples will
have different values of $\bar{X}$, i.e. $\bar{X}$ will have *sampling
variation*.  Then the SE of $\bar{X}$ is defined to be the standard
deviation of all possible $\bar{X}$ values, from all possible samples.

The smaller the SE is, the more confident we are that the $\bar{X}$ from
[Meaning of that 95% probability figure: Think of all possible 
samples. Each one has an $\bar{X}$ value, and an SE value. So, each
possible sample, there is a different confidence interval (CI). 
95% of those CIs will contain the true $\mu$.]{.column-margin}
our particular sample is pretty accurate.  Of course, we can't be
completely sure, but we can quantify it probabilistically:

> An approximate 95% *confidence interval* for $\mu$, based on
> $\bar{X}$, is $\bar{X} \pm 1.96 \times SE$.

The reader may have heard of the Central Limit Theorem, which says that
the sum of many random variables, itself a random variable, has an
approximately normal distribution.  This is the case for most classical
statistical estimators, such as estimated coefficients in linear and
generalized linear models.  Hence we can find confidence intervals as
above, taking the estimator plus and minus 1.96 times the SE of the
estimator.

## Appendix B: A Note on Causal Inference {#sec-causal}

An *observational study* is, in essence, one that is not planned. For
instance, say we are comparing an old and a new drug for hypertension.
Suppose that, unknown to us, the new medication does well on younger
patients but not on older ones.  Say the nature of the data collection
process results in disproportionately sampling older patients, but our
data itself does not include patient ages.  This could unfairly make the
new drug appear ineffective.

In a *randomized clinical trial* (RCTs), we would randomly assign
treatments to patients, so there would likely be no strong imbalance in
age distribution in the two drug groups.  So, even if our data still did
not record patient age, there would be no age bias in our analysis.

In many cases, RCTs are infeasible or impossible (we cannot "assign"
race).  *Causal inference* CI can be viewed both as a way to deal with
unseen variables in observational studies, and in its graphical forms,
as a descriptive alternative to traditional statistical relational
analysis.  Though CI takes on many forms, our discussion here will focus
on *directed acyclic graphs* (DAGs).  

### DAGS

In a DAG, an arrow from A to B signifies that A "causes" B.

For instance, a graphical description of a situation in which older
people (say who came of age before anti-smoking campaigns) might be more
likely to smoke, and for other reasons may be more likely to develop
cancer even if they are nonsmokers.  A graphical description would be

![](appendixB/ascdag.png){width=40%}

Here is an example the **bnlearn** package on our **svcensus** dataset:

``` r
   library(bnlearn)
   library(dsld)
   data(svcensus)
   svcensus$wageinc <- as.numeric(svcensus$wageinc)
   svcensus$wkswrkd <- as.numeric(svcensus$wkswrkd)
   svcdag <- iamb(svcensus)  # default argument values
   plot(svcdag)
```

![](svcensusDAG.png){width=100%}

There are a couple of undirected arcs, e.g. between gender and [Though
again we omit a detailed treatment of CI DAGs here, we note that that
field defines a confounder of two variables as a variable that is an
"ancestor" to both, education being a confounder for income and
occupation.]{.column-margin} occupation--found to be relations, but 
with indeterminate direction, using available data.

More interesting, though, is the lack of a causal arc from gender to
wage income.  This is in stark contrast to what **dsldLinear** suggests:

```{r}
data(svcensus)
w <- dsldLinear(svcensus,'wageinc','gender',interactions=FALSE)
coef(w)
```

Here, even correcting for age, education and so on, we still find a
substantial gender pay gap.  Gender does seem to matter, on its own.

It should be noted that if the **bnlearn** function **hc** is used
instead of **iamb** (not shown), there is indeed a link from 'gender' to
'wageinc', but apparently at the cost of other anomalies.  For example,
there is no arc, or path of arcs, from 'wkswrkd' to 'wageinc'.

CI DAGs are of course quite visually appealing, and many people find
causal concepts helpful in their thoughts processes regarding the data.
But as pointed out by the developers of such methodology, the graphs can
be very misleading.  Using DAGs effectively requires deep understanding
of the concepts.  A treatment of these concepts is well beyond the scope
of this course, but we urge users and consumers of CI to keep the
following points in mind:


* The definition of *cause* is of course central to proper use of CI
  DAGs.  It does *not* mean what one ordinarily thinks of as causal ("The
  sidewalk was icy, so the man slipped and fell"), and it can become quite
  complex; "wrapping one's head around" the definition can be a
  challenge even for experienced probabilistic modelers.

* Unseen variables, such as age in the above example, can be just as
harmful in CI contexts as in observational data.

* Causal analysis is just as much at the mercy of sample size as is
  classical statistical analysis, indeed even more so, since the causal
  version in effect estimates many more parameters.  Absence of a link
  may be due to insufficient data, and nonlinks at the population level
  may appear as "links" in our graph by random chance in small datasets.   

* Most important, DAGs have a highly concerning uniqueness problem:

::: {.callout-note}
### DAGS are not unique

* Most important, for any given dataset, there is typically no unique
  DAG: [CMU professor C. Shalizi has [bluntly
  noted](https://www.stat.cmu.edu/~cshalizi/uADA/16/lectures/25.pdf)
  that in cases of nonuniqueness, presenting just one DAG without
  mentioning the equivalent ones, "...[which] is often what people seem
  to do," might be viewed as "lying by omission."  Recognizing the
  nonuniqueness is "simply a matter of scientific
  honesty."]{.column-margin} DAG.  Typically many DAGs can fit the
  dataset equally well, and they can be in conflict with each other.
  One DAG might say that A causes B, while another, equally credible in
  terms of data fit, might have B causing A, while a third says A and B
  are independent.

  M. Scutari, author of **bnlearn**, shows an example in his Cambridge lectures:

  ![](TwoEquivDAGs.png){width=100%}

  The nonuniqueness is of course very concerning.  In recent years, there
  has been much handwringing in science about the Replication Crisis, in
  which scientists try to replicate the findings of an earlier published
  paper, and discover serious problems with the earlier results.
  Imagine the replication problems that can arise if authors of a paper
  present just one of many equally-credible DAGs, as is usually the
  case.
:::

DAGs can indeed lead to valuable insight, but can be misleading as well.
A healthy skepticism and a solid understanding of the concepts are
mandatory for avoiding misleading or invalid results. 

## Appendix C: Standard Errors via the Bootstrap {#sec-boot}

## Appendix D: Installing the Software

<!-- Introduction/Pkg installation -->

### Python Interface Example

As previously mentioned, python wrappers are included for most functions. 
As an example, let's use the python interface for dsldTakeALookAround 
(the python equivalent function is called **dsldPyTakeALookAround**)  
to view tabular information on how data features relate to Y and S. The 
python function allows users to enter arguments using Python data types 
and converts the resultant R dataframe into a Pandas dataframe.

To run any dsld python functions, the user must have rpy2 installed
[rpy2 can be installed using **pip**, the python package installer]{.column-margin} 
and be using Python version 3.10. The user also needs  

Additionally, different python functions require different libraries 
depending on what the function does -- Pandas
[These libraries can also (usually) be installed with **pip**]{.column-margin} 
is required for dsldPyTakeALookAround.

<!-- This part may change if installation method changes -- assuming that we 
are using devtools::install_github and have access to /dsld folder -->
The user should open the Python Shell Prompt by typing **python** from the 
package Python directory (/dsld/inst/Python) in the terminal/cmd prompt.
Then, to run the Python function, we can run the following commands in the 
python shell:

```{python, eval=FALSE}
from dsldTakeALook_Py_R import dsldPyTakeALookAround    
import rpy2.robjects as robjects   
robjects.r['data']('svcensus')    
data = robjects.r('svcensus')    
result = dsldPyTakeALookAround(data, 'wageinc', 'gender')   
print(result)
```

The result should be the same information from the R dataframe, but 
converted to a Pandas dataframe.

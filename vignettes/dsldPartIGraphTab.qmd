
## Deciding on a set of confounders C  

One may have specific confounders in mind for a particular analysis, but
it is often unclear as to which to use, or for that matter, why not use
them all?  This section addresses those issues.

We will address that second question first: Why not use all our
variables, other than Y and S, as confounders?  We will see that if we
have a large number of variables, there is good reason select only a
subset of potential confounders.  

How might we do so?  We will first recommend some graphical and tabular
methods aimed at preliminary exploration toward this end, and then
present a **dsld** function that users may find useful for more
systematic selection of confounders.

### The problem 

The German credit dataset, included in the **fairml** package and thus
with **dsld**, consists of a total of 21 variables:

```{r}
data(german.credit)
names(german.credit)  
```

Excluding Y = credit risk and S = gender, that gives us 19 variables to
choose among for our confounders.  Which ones should we use?

Technically, almost any variable is a confounder.  The impact may quite
minuscule, but through a long chain of relations among many variables,
there will usually be at least some connection, though possibly very
faint.

Well, then, why not use them all?  That is what we've done in our
earlier examples.  But there are several issues to consider not using
the full set of variable,s i.e. every variable other than Y and S:

* It may result in overfitting, resulting in large standard errors.

* It is unwieldy, difficult to interpret.  Many treatments of these
  issues speak of a desire for a "parsimonious" model.

* There is a concern regarding duplication.  It may be that, say, some
  pair of confounders suffices, and adding further confounders does
  nothing to further clarify discrimination effects.

Concerning this last point:  We do not merely have 19
choices for confounders; we must choose from the set of all possible
groups of confounders: singletons, pairs, triplets and so on.  There are
$2^{19}$ possibilities here, about half a million.

Our specific purpose here is to find a reasonable set of  confounders.
In essence, that means find variables C such that C is substantially
correlated with both Y and S.  One way to approach that is that do
separate predictions of Y and S, and see which features turn out to
predict both well.  

### Exploratory example:  engineering wages

One useful tool here is the function **dsldFrequencyByS()**, which aims
to analyze categorical (not numeric) columns by printing out a list of
frequencies. Since education level in the **svcensus* data is
categorical, we can call the function as follows:

```{r}
library(dsld)
data(svcensus)
dsldFrequencyByS(svcensus, "educ", "gender")
```

The similar frequency values between men and women suggests
not using education as a confounder.

On the other hand, we *do* see more interesting results if we look at 
*occupation* instead of *education*:

```{r}
library(dsld)
data(svcensus)
dsldFrequencyByS(svcensus, "occ", "gender")
```

Notice the difference between the proportion of males in occupation 106
[One might consider conducting formal statistical inference in this
comparison, but the relevance to the issue of our ultimate statistical
analysis of Y is unclear.  At any rate, here we are merely doing an
exploratory analysis anyway.]{.column-margin} vs females in that same
occupation--a difference of approximately 11%.  The difference in
frequencies here is much greater than with the 'educ' example, which
suggests using occupation as a confounder.  Since different occupations
correlate with different wages, it is possible that this difference in
occupations that men have compared with what women have could be
affecting the relationship between gender and wage. 

### Exploratory example: Law school admissions data

Suppose we are investigating the relationship between LSAT exams scores
and race, and wish to graphically explore the effects of the C, family
income, on the relationship between Y = LSAT score, and S = race.

We have to look at both the relationship between race and LSAT scores
and the relationship between race and family income. We may also want to
look at the effects of GPA as well. A quick way to view multiple
variables (and their relation to the sensitive variable) at once is to
plot them in a 3D scatter plot:

<!-- Placement pending -->
```{r, eval=FALSE}
data(law.school.admissions)
dsldScatterPlot3D(law.school.admissions, c('lsat','fam_inc','ugpa'), 'race1',
                  pointSize = 4)
```

![](graphTab/lawscatterplot3d2.png){width="90%"}

Looking at the fam_inc axis, the lowest quintile of family income is
[This is a **plotly** interactive graph.  To fully understand this
function, the reader should execute the function outside of Quarto,
trying features such as move, rotate, show annotation etc.]{.column-margin}
mostly populated by Black and Latino students, while the upper two
levels are almost entirely made up of white students. Interestingly on
the lsat axis, most of those with a lower score happen to be non-white,
across all income levels. On the other hand, the ugpa axis has a similar
trend to that of the lsat axis, albeit to a much weaker extent.

Once again, this analysis is merely exploratory, but the graph lends
some credence to claims that family income may confound the relationship
between race and LSAT score.  Note however, that these graphs do not do much to 
answer the question of whether, in this case, that relationship is substantial.
The formal analysis we did earlier indicates that it is not.

Returning to the question of whether race have a substantial impact on
results, we can look at the density plot of LSAT scores against
different races.

```{r, eval=FALSE}
# requires 'webshot' package
data(law.school.admissions)
dsldDensityByS(law.school.admissions, cName = "lsat", sName = "race1")
```

![](graphTab/lawdensitylsat.png){width="120%"}

Each curve represents the distribution of LSAT scores for each race.
From this, its easy to see the possibility of racial bias in the LSAT.
But again, this bias may be confounded.

Recall that the scatter plot also suggested some trend between family
income and race. We can investigate this possible relationship by
generating another density plot:

```{r, eval=FALSE}
data(law.school.admissions)
dsldDensityByS(law.school.admissions, cName = "fam_inc", sName = "race1")
```

![](graphTab/lawdensityfam.png){width="120%"}

White students have larger peaks at income levels 3, 4, and 5,
indicating that [Recall that the **fam_inc** variable is measured in
terms of quintiles, 1 through 5.  This is the reason behind the
seemingly-odd alignment of, say, the Black and Latino
curves.]{.column-margin} a larger proportion of white students are in
the higher income brackets than non-white students. Conversely, a larger
proportion of black and Latino students occupy the lower income bracket
levels.

[If we wanted to investigate other potential confounders, we can call 
**dsldConfounders()** with 'race1' as our sensitive variable.]{.column-margin}

This suggests that family income could be confounding the potential 
relationship between race and LSAT scores.  But, as noted, our earlier analysis indicated that the degree of this relationship is very small.

::: {.callout-note}
### Almost anything is a Confounder

Once one accounts for links of two variables, then links of links and so
on, virtually everything in practice is a confounder to *some* degree,
so it is up to the analyst to decide when a feature is confounding
enough to be considered for inclusion in a model.

:::

### Variable selection methodology

The above graphical and tabular approaches may suffice to determine
one's confounders in some applications.  We now present more formal,
systematic methods.

Many, many approaches to this *variable selection* or *feature
selection* [A nice overview of such methodology, and implementations in
R and Python, are given in [Parr *et
al*](https://explained.ai/rf-importance).]{.column-margin} problem have
been proposed over the years.  The **qeML** package includes five of
them (see the documentation), and there are myriad others.

One widely-used method is *permutation*, which we will use here.  To
gauge the importance of a variable V in the context of using some ML
algorithm, we do two runs through the data, first with the original dataset,
and then with the V column changed through a random shuffling of the
values in that column.  In that second run, our predictive accuracy
should be compromised, and the importance measure is taken to be the
proportional increase in error rate.  The larger the increase, the
more important we deem the variable.

### The dsldCHunting() function

This function serves as an aid to selecting confounders, using the 
following approach.  Denote our dataset by **d**.

* Two columnwise subsets of **d** are formed, **dNoS** and **doNoY**,
  deleting the respective column.

* We fit a random forests model to **dNoS**, predicting Y, resulting in
  variable importance measure **vY**.

* We fit a random forests model to **dNoY**, predicting S, resulting in
  variable importance measure **vS**.

* We sort each of the two 'v' measures, from largest to smallest.

* For each i, we take the intersection of the top-i 'v' measures in each set.

By taking the intersection, we are focusing on variables that are
correlated with both Y and S, as desired.

### Example:  Boston mortgage data

Here is an example, using the mortgage data.

```{r}
data(mortgageSE)
dsldCHunting(mortgageSE,'deny','black')
```

The importance measures are printed out (the ranking is what matters),
followed by the "top-i" sets in common.

As usual, there are no magic formulas to use here.  The analyst is given
a choice.  One might use just two confounders, 'p_irat' and 'loan_val';
for a set of three, the software suggests 'p_irat', 'loan_val' and 'hse_inc',
and so on.

### Example:  Employer bias test

In the paper "Are Emily and Greg More Employable Than Lakisha and Jamal?
A Field Experiment on Labor Market Discrimination" (*Am. Econ. Rev.,
Sept. 2004), researcher "test" employers by sending CVs with
"white-sounding" and "Black-sounding" names, checking for bias.  The
resulting data is **lak* in **dsld**.

```{r}
data(lak)
dim(lak)
names(lak)
```

We'll take Y to be **call**, which records whether the "applicant"
received a call back in response to submitting the CV.  S will be race,
which now leaves us with 63 - 2 = 61 potential confounders!

Actually, it's even worse.  Many rows of the dataset contain missing
values (coded NA in R).  The version here uses only intact rows, of
which there are 447.  (The original data had 4870 rows.)  A rough rule
of thumb commonly cited in statistics is that in regression estimation,
the number of features should be less than the square of the number of
data points; by this or almost any other measure, 61 predictors is well
beyond the capacity of 4447 data points.  And, as noted, that number of
variables is unwieldy.  Let's see what we can do to reduce that.

```{r}
dsldCHunting(lak,'call','race',25)
```

We are seeking variables that are correlated both with Y and S, yet
there appear to be many that correlate with just one of these.  But
still, in the end we see a few that meet our goals.

